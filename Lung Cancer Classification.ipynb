{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f367a1c",
   "metadata": {},
   "source": [
    "#### IMPORTING Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0404a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 18:57:07.207879: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-04 18:57:07.542961: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-04 18:57:07.544554: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-04 18:57:09.274829: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import models,layers,preprocessing,optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "import os \n",
    "import random\n",
    "import shutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9898cf8",
   "metadata": {},
   "source": [
    "#### Path Of The Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9ef231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir('/home/hassan-ahmed-khan/Ai Practice/datasets/lung cancer Detection')\n",
    "base_dir = r'/home/hassan-ahmed-khan/Ai Practice/datasets/lung cancer Detection'\n",
    "positive = os.path.join(base_dir,'positive')\n",
    "negative = os.path.join(base_dir,'nagative')\n",
    "train_dir = os.path.join(base_dir,'train')\n",
    "test_dir = os.path.join(base_dir,'test')\n",
    "validation_dir = os.path.join(base_dir,'validation')\n",
    "train_pos_dir = os.path.join(train_dir,'positive')\n",
    "train_neg_dir = os.path.join(train_dir,'negative')\n",
    "test_pos_dir = os.path.join(test_dir,'positive')\n",
    "test_neg_dir = os.path.join(test_dir,'negative')\n",
    "validation_pos_dir = os.path.join(validation_dir,'positive')\n",
    "validation_neg_dir = os.path.join(validation_dir,'negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de15eec",
   "metadata": {},
   "source": [
    "#### Making Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e86cd648",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(train_dir)\n",
    "os.mkdir(test_dir)\n",
    "os.mkdir(validation_dir)\n",
    "os.mkdir(train_pos_dir)\n",
    "os.mkdir(train_neg_dir)\n",
    "os.mkdir(test_pos_dir)\n",
    "os.mkdir(test_neg_dir)\n",
    "os.mkdir(validation_pos_dir)\n",
    "os.mkdir(validation_neg_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39a8e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(os.path.join(base_dir,'positive'))))\n",
    "print(len(os.listdir(os.path.join(base_dir,'nagative'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e35da9",
   "metadata": {},
   "source": [
    "#### Shuffle Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38fbe2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NORMAL2-IM-0139-0001.jpeg',\n",
       " 'NORMAL2-IM-0297-0001.jpeg',\n",
       " 'NORMAL2-IM-0023-0001.jpeg',\n",
       " 'NORMAL2-IM-0171-0001.jpeg',\n",
       " 'IM-0095-0001.jpeg',\n",
       " 'NORMAL2-IM-0364-0001.jpeg',\n",
       " 'NORMAL2-IM-0123-0001.jpeg',\n",
       " 'NORMAL2-IM-0288-0001.jpeg',\n",
       " 'IM-0037-0001.jpeg',\n",
       " 'IM-0101-0001.jpeg',\n",
       " 'NORMAL2-IM-0233-0001.jpeg',\n",
       " 'IM-0041-0001.jpeg',\n",
       " 'NORMAL2-IM-0321-0001.jpeg',\n",
       " 'NORMAL2-IM-0305-0001.jpeg',\n",
       " 'NORMAL2-IM-0376-0001.jpeg',\n",
       " 'IM-0021-0001.jpeg',\n",
       " 'IM-0025-0001.jpeg',\n",
       " 'NORMAL2-IM-0079-0001.jpeg',\n",
       " 'IM-0085-0001.jpeg',\n",
       " 'NORMAL2-IM-0279-0001.jpeg',\n",
       " 'NORMAL2-IM-0353-0001.jpeg',\n",
       " 'NORMAL2-IM-0035-0001.jpeg',\n",
       " 'NORMAL2-IM-0195-0001.jpeg',\n",
       " 'NORMAL2-IM-0272-0001.jpeg',\n",
       " 'IM-0050-0001.jpeg',\n",
       " 'NORMAL2-IM-0348-0001.jpeg',\n",
       " 'NORMAL2-IM-0368-0001.jpeg',\n",
       " 'NORMAL2-IM-0207-0001.jpeg',\n",
       " 'NORMAL2-IM-0357-0001.jpeg',\n",
       " 'IM-0079-0001.jpeg',\n",
       " 'IM-0105-0001.jpeg',\n",
       " 'NORMAL2-IM-0033-0001.jpeg',\n",
       " 'NORMAL2-IM-0332-0001.jpeg',\n",
       " 'NORMAL2-IM-0302-0001.jpeg',\n",
       " 'NORMAL2-IM-0374-0001-0002.jpeg',\n",
       " 'IM-0039-0001.jpeg',\n",
       " 'NORMAL2-IM-0105-0001.jpeg',\n",
       " 'IM-0073-0001.jpeg',\n",
       " 'NORMAL2-IM-0325-0001.jpeg',\n",
       " 'NORMAL2-IM-0221-0001.jpeg',\n",
       " 'IM-0093-0001.jpeg',\n",
       " 'NORMAL2-IM-0131-0001.jpeg',\n",
       " 'IM-0023-0001.jpeg',\n",
       " 'IM-0091-0001.jpeg',\n",
       " 'NORMAL2-IM-0219-0001.jpeg',\n",
       " 'NORMAL2-IM-0027-0001.jpeg',\n",
       " 'NORMAL2-IM-0150-0001.jpeg',\n",
       " 'NORMAL2-IM-0300-0001.jpeg',\n",
       " 'NORMAL2-IM-0206-0001.jpeg',\n",
       " 'NORMAL2-IM-0345-0001.jpeg',\n",
       " 'IM-0087-0001.jpeg',\n",
       " 'IM-0111-0001.jpeg',\n",
       " 'NORMAL2-IM-0283-0001.jpeg',\n",
       " 'NORMAL2-IM-0060-0001.jpeg',\n",
       " 'NORMAL2-IM-0278-0001.jpeg',\n",
       " 'NORMAL2-IM-0052-0001.jpeg',\n",
       " 'IM-0070-0001.jpeg',\n",
       " 'NORMAL2-IM-0309-0001.jpeg',\n",
       " 'NORMAL2-IM-0059-0001.jpeg',\n",
       " 'NORMAL2-IM-0029-0001.jpeg',\n",
       " 'NORMAL2-IM-0281-0001.jpeg',\n",
       " 'NORMAL2-IM-0041-0001.jpeg',\n",
       " 'NORMAL2-IM-0111-0001.jpeg',\n",
       " 'NORMAL2-IM-0341-0001.jpeg',\n",
       " 'NORMAL2-IM-0222-0001.jpeg',\n",
       " 'NORMAL2-IM-0267-0001.jpeg',\n",
       " 'IM-0017-0001.jpeg',\n",
       " 'NORMAL2-IM-0019-0001.jpeg',\n",
       " 'NORMAL2-IM-0374-0001.jpeg',\n",
       " 'IM-0019-0001.jpeg',\n",
       " 'NORMAL2-IM-0117-0001.jpeg',\n",
       " 'IM-0059-0001.jpeg',\n",
       " 'NORMAL2-IM-0359-0001.jpeg',\n",
       " 'NORMAL2-IM-0286-0001.jpeg',\n",
       " 'NORMAL2-IM-0210-0001.jpeg',\n",
       " 'IM-0005-0001.jpeg',\n",
       " 'IM-0089-0001.jpeg',\n",
       " 'IM-0086-0001.jpeg',\n",
       " 'NORMAL2-IM-0347-0001.jpeg',\n",
       " 'NORMAL2-IM-0282-0001.jpeg',\n",
       " 'NORMAL2-IM-0096-0001.jpeg',\n",
       " 'NORMAL2-IM-0339-0001.jpeg',\n",
       " 'NORMAL2-IM-0072-0001.jpeg',\n",
       " 'NORMAL2-IM-0274-0001.jpeg',\n",
       " 'IM-0110-0001.jpeg',\n",
       " 'NORMAL2-IM-0294-0001.jpeg',\n",
       " 'NORMAL2-IM-0330-0001.jpeg',\n",
       " 'NORMAL2-IM-0381-0001.jpeg',\n",
       " 'NORMAL2-IM-0112-0001.jpeg',\n",
       " 'IM-0006-0001.jpeg',\n",
       " 'IM-0103-0001.jpeg',\n",
       " 'NORMAL2-IM-0132-0001.jpeg',\n",
       " 'NORMAL2-IM-0369-0001.jpeg',\n",
       " 'IM-0027-0001.jpeg',\n",
       " 'NORMAL2-IM-0338-0001.jpeg',\n",
       " 'NORMAL2-IM-0307-0001.jpeg',\n",
       " 'NORMAL2-IM-0326-0001.jpeg',\n",
       " 'NORMAL2-IM-0251-0001.jpeg',\n",
       " 'NORMAL2-IM-0028-0001.jpeg',\n",
       " 'NORMAL2-IM-0336-0001.jpeg',\n",
       " 'NORMAL2-IM-0335-0001.jpeg',\n",
       " 'NORMAL2-IM-0173-0001-0002.jpeg',\n",
       " 'IM-0069-0001.jpeg',\n",
       " 'NORMAL2-IM-0277-0001.jpeg',\n",
       " 'NORMAL2-IM-0304-0001.jpeg',\n",
       " 'NORMAL2-IM-0129-0001.jpeg',\n",
       " 'NORMAL2-IM-0311-0001.jpeg',\n",
       " 'NORMAL2-IM-0319-0001.jpeg',\n",
       " 'IM-0022-0001.jpeg',\n",
       " 'IM-0097-0001.jpeg',\n",
       " 'NORMAL2-IM-0343-0001.jpeg',\n",
       " 'IM-0028-0001.jpeg',\n",
       " 'IM-0043-0001.jpeg',\n",
       " 'NORMAL2-IM-0346-0001.jpeg',\n",
       " 'NORMAL2-IM-0252-0001.jpeg',\n",
       " 'NORMAL2-IM-0337-0001.jpeg',\n",
       " 'NORMAL2-IM-0073-0001.jpeg',\n",
       " 'NORMAL2-IM-0246-0001-0001.jpeg',\n",
       " 'IM-0102-0001.jpeg',\n",
       " 'IM-0001-0001.jpeg',\n",
       " 'NORMAL2-IM-0354-0001.jpeg',\n",
       " 'IM-0007-0001.jpeg',\n",
       " 'NORMAL2-IM-0275-0001.jpeg',\n",
       " 'NORMAL2-IM-0110-0001.jpeg',\n",
       " 'NORMAL2-IM-0351-0001.jpeg',\n",
       " 'NORMAL2-IM-0372-0001.jpeg',\n",
       " 'IM-0061-0001.jpeg',\n",
       " 'NORMAL2-IM-0380-0001.jpeg',\n",
       " 'NORMAL2-IM-0360-0001.jpeg',\n",
       " 'IM-0077-0001.jpeg',\n",
       " 'NORMAL2-IM-0280-0001.jpeg',\n",
       " 'NORMAL2-IM-0328-0001.jpeg',\n",
       " 'NORMAL2-IM-0045-0001.jpeg',\n",
       " 'NORMAL2-IM-0146-0001.jpeg',\n",
       " 'NORMAL2-IM-0317-0001.jpeg',\n",
       " 'NORMAL2-IM-0256-0001.jpeg',\n",
       " 'NORMAL2-IM-0349-0001.jpeg',\n",
       " 'NORMAL2-IM-0217-0001.jpeg',\n",
       " 'NORMAL2-IM-0213-0001.jpeg',\n",
       " 'NORMAL2-IM-0340-0001.jpeg',\n",
       " 'NORMAL2-IM-0329-0001.jpeg',\n",
       " 'NORMAL2-IM-0241-0001.jpeg',\n",
       " 'NORMAL2-IM-0107-0001.jpeg',\n",
       " 'NORMAL2-IM-0145-0001.jpeg',\n",
       " 'NORMAL2-IM-0246-0001-0002.jpeg',\n",
       " 'NORMAL2-IM-0173-0001-0001.jpeg',\n",
       " 'NORMAL2-IM-0370-0001.jpeg',\n",
       " 'NORMAL2-IM-0378-0001.jpeg',\n",
       " 'NORMAL2-IM-0120-0001.jpeg',\n",
       " 'IM-0033-0001.jpeg',\n",
       " 'IM-0030-0001.jpeg',\n",
       " 'IM-0099-0001.jpeg',\n",
       " 'NORMAL2-IM-0287-0001.jpeg',\n",
       " 'NORMAL2-IM-0030-0001.jpeg',\n",
       " 'NORMAL2-IM-0292-0001.jpeg',\n",
       " 'NORMAL2-IM-0303-0001.jpeg',\n",
       " 'NORMAL2-IM-0327-0001.jpeg',\n",
       " 'IM-0071-0001.jpeg',\n",
       " 'NORMAL2-IM-0373-0001.jpeg',\n",
       " 'NORMAL2-IM-0273-0001.jpeg',\n",
       " 'IM-0067-0001.jpeg',\n",
       " 'IM-0033-0001-0002.jpeg',\n",
       " 'IM-0015-0001.jpeg',\n",
       " 'NORMAL2-IM-0271-0001.jpeg',\n",
       " 'IM-0065-0001.jpeg',\n",
       " 'NORMAL2-IM-0316-0001.jpeg',\n",
       " 'IM-0063-0001.jpeg',\n",
       " 'NORMAL2-IM-0362-0001.jpeg',\n",
       " 'NORMAL2-IM-0333-0001.jpeg',\n",
       " 'NORMAL2-IM-0232-0001.jpeg',\n",
       " 'NORMAL2-IM-0322-0001.jpeg',\n",
       " 'IM-0075-0001.jpeg',\n",
       " 'NORMAL2-IM-0374-0001-0001.jpeg',\n",
       " 'NORMAL2-IM-0249-0001.jpeg',\n",
       " 'NORMAL2-IM-0350-0001.jpeg',\n",
       " 'NORMAL2-IM-0066-0001.jpeg',\n",
       " 'IM-0016-0001.jpeg',\n",
       " 'IM-0109-0001.jpeg',\n",
       " 'NORMAL2-IM-0051-0001.jpeg',\n",
       " 'IM-0049-0001.jpeg',\n",
       " 'NORMAL2-IM-0259-0001.jpeg',\n",
       " 'IM-0009-0001.jpeg',\n",
       " 'IM-0036-0001.jpeg',\n",
       " 'NORMAL2-IM-0198-0001.jpeg',\n",
       " 'IM-0033-0001-0001.jpeg',\n",
       " 'NORMAL2-IM-0130-0001.jpeg',\n",
       " 'NORMAL2-IM-0012-0001.jpeg',\n",
       " 'IM-0003-0001.jpeg',\n",
       " 'NORMAL2-IM-0361-0001.jpeg',\n",
       " 'NORMAL2-IM-0331-0001.jpeg',\n",
       " 'IM-0013-0001.jpeg',\n",
       " 'NORMAL2-IM-0199-0001.jpeg',\n",
       " 'IM-0011-0001-0002.jpeg',\n",
       " 'IM-0081-0001.jpeg',\n",
       " 'IM-0031-0001.jpeg',\n",
       " 'NORMAL2-IM-0081-0001.jpeg',\n",
       " 'IM-0010-0001.jpeg',\n",
       " 'NORMAL2-IM-0102-0001.jpeg',\n",
       " 'NORMAL2-IM-0196-0001.jpeg',\n",
       " 'NORMAL2-IM-0201-0001.jpeg',\n",
       " 'IM-0011-0001.jpeg',\n",
       " 'NORMAL2-IM-0315-0001.jpeg',\n",
       " 'NORMAL2-IM-0313-0001.jpeg',\n",
       " 'NORMAL2-IM-0086-0001.jpeg',\n",
       " 'IM-0029-0001.jpeg',\n",
       " 'NORMAL2-IM-0238-0001.jpeg',\n",
       " 'NORMAL2-IM-0098-0001.jpeg',\n",
       " 'NORMAL2-IM-0013-0001.jpeg',\n",
       " 'NORMAL2-IM-0366-0001.jpeg',\n",
       " 'NORMAL2-IM-0312-0001.jpeg',\n",
       " 'NORMAL2-IM-0007-0001.jpeg',\n",
       " 'IM-0046-0001.jpeg',\n",
       " 'NORMAL2-IM-0237-0001.jpeg',\n",
       " 'NORMAL2-IM-0229-0001.jpeg',\n",
       " 'NORMAL2-IM-0092-0001.jpeg',\n",
       " 'IM-0011-0001-0001.jpeg',\n",
       " 'IM-0084-0001.jpeg',\n",
       " 'NORMAL2-IM-0058-0001.jpeg',\n",
       " 'IM-0083-0001.jpeg',\n",
       " 'IM-0035-0001.jpeg',\n",
       " 'NORMAL2-IM-0276-0001.jpeg',\n",
       " 'NORMAL2-IM-0301-0001.jpeg',\n",
       " 'IM-0107-0001.jpeg',\n",
       " 'NORMAL2-IM-0323-0001.jpeg',\n",
       " 'NORMAL2-IM-0285-0001.jpeg',\n",
       " 'NORMAL2-IM-0289-0001.jpeg',\n",
       " 'NORMAL2-IM-0135-0001.jpeg',\n",
       " 'NORMAL2-IM-0290-0001.jpeg',\n",
       " 'NORMAL2-IM-0310-0001.jpeg',\n",
       " 'IM-0045-0001.jpeg',\n",
       " 'NORMAL2-IM-0352-0001.jpeg',\n",
       " 'NORMAL2-IM-0095-0001.jpeg',\n",
       " 'NORMAL2-IM-0141-0001.jpeg',\n",
       " 'NORMAL2-IM-0246-0001.jpeg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = os.listdir(os.path.join(base_dir,'positive'))\n",
    "random.shuffle(a)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51c3122",
   "metadata": {},
   "source": [
    "#### Divide images into 3 parts Training,Test,Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8d1d8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_images = os.listdir(positive)\n",
    "random.shuffle(positive_images)\n",
    "negative_images = os.listdir(negative)\n",
    "random.shuffle(negative_images)\n",
    "train_pos_images = positive_images[:150]\n",
    "train_neg_images = negative_images[:150]\n",
    "validation_pos_images = positive_images[150:190]\n",
    "validation_neg_images = negative_images[150:230]\n",
    "test_pos_images = positive_images[190:]\n",
    "test_neg_images = negative_images[230:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac82a2",
   "metadata": {},
   "source": [
    "#### Copying each part into Their respective folded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9dba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in train_pos_images:\n",
    "    src = os.path.join(positive,img)\n",
    "    dst = os.path.join(train_pos_dir,img)\n",
    "    shutil.copy(src,dst)\n",
    "\n",
    "for img in train_neg_images:\n",
    "    src = os.path.join(negative,img)\n",
    "    dst = os.path.join(train_neg_dir,img)\n",
    "    shutil.copy(src,dst)\n",
    "\n",
    "for img in validation_pos_images:\n",
    "    src = os.path.join(positive,img)\n",
    "    dst = os.path.join(validation_pos_dir,img)\n",
    "    shutil.copy(src,dst)\n",
    "\n",
    "for img in validation_neg_images:\n",
    "    src = os.path.join(negative,img)\n",
    "    dst = os.path.join(validation_neg_dir,img)\n",
    "    shutil.copy(src,dst)\n",
    "\n",
    "for img in test_pos_images:\n",
    "    src = os.path.join(positive,img)\n",
    "    dst = os.path.join(test_pos_dir,img)\n",
    "    shutil.copy(src,dst)\n",
    "\n",
    "for img in test_neg_images:\n",
    "    src = os.path.join(negative,img)\n",
    "    dst = os.path.join(test_neg_dir,img)\n",
    "    shutil.copy(src,dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "754afb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print(len(test_pos_images))\n",
    "print(len(test_neg_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b5e67e",
   "metadata": {},
   "source": [
    "#### Making Generator By using inbuilt Tensorflow Function for applying data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b33f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1./255,brightness_range=(0.2,0.6),vertical_flip=True,horizontal_flip=True)\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1cb2d5",
   "metadata": {},
   "source": [
    "#### passing validaton images and train images into generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cabc85cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 images belonging to 2 classes.\n",
      "Found 120 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = train_data_gen.flow_from_directory(train_dir,target_size=(150,150),class_mode='binary',batch_size=32)\n",
    "val_gen = test_data_gen.flow_from_directory(validation_dir,target_size=(150,150),class_mode='binary',batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "338677df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 150, 150, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "for data,label in train_gen:\n",
    "    print(data.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11c540",
   "metadata": {},
   "source": [
    "#### Making Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ef92bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 148, 148, 64)      1792      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 148, 148, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 74, 74, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 350464)            0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                11214880  \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11216705 (42.79 MB)\n",
      "Trainable params: 11216705 (42.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64,(3,3),activation='relu',input_shape=(150,150,3)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "# model.add(layers.MaxPool2D((2,2)))\n",
    "# model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32,activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9837c1bc",
   "metadata": {},
   "source": [
    "#### Training or Fitting and Validating model by using training and validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dd1f6b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3085/2611730864.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_gen,steps_per_epoch=10,epochs=10,validation_data=val_gen,validation_steps=4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 14s 1s/step - loss: 3.7555 - accuracy: 0.4867 - val_loss: 1.2477 - val_accuracy: 0.3860\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.7516 - accuracy: 0.5833 - val_loss: 0.5657 - val_accuracy: 0.7456\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 19s 2s/step - loss: 0.7945 - accuracy: 0.5367 - val_loss: 0.6377 - val_accuracy: 0.6316\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 19s 2s/step - loss: 0.7015 - accuracy: 0.7300 - val_loss: 0.5422 - val_accuracy: 0.7632\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 19s 2s/step - loss: 0.5521 - accuracy: 0.8067 - val_loss: 0.5068 - val_accuracy: 0.7632\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.5141 - accuracy: 0.7967 - val_loss: 0.4550 - val_accuracy: 0.8333\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.5378 - accuracy: 0.7867 - val_loss: 0.4636 - val_accuracy: 0.8509\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.4541 - accuracy: 0.8300 - val_loss: 0.5300 - val_accuracy: 0.7193\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 20s 2s/step - loss: 0.4052 - accuracy: 0.8200 - val_loss: 0.4668 - val_accuracy: 0.8070\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.3905 - accuracy: 0.8467 - val_loss: 1.1394 - val_accuracy: 0.6491\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_gen,steps_per_epoch=10,epochs=10,validation_data=val_gen,validation_steps=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e79c97",
   "metadata": {},
   "source": [
    "#### passing Test Images Into Generator for evalution of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac85795f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 114 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3085/2205740475.py:2: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  test = model.evaluate_generator(val_gen)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_8/dense_16/Relu' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3085/2602469645.py\", line 2, in <module>\n      test = model.evaluate_generator(test_gen)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2852, in evaluate_generator\n      return self.evaluate(\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2200, in evaluate\n      logs = test_function_runner.run_step(\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 4000, in run_step\n      tmp_logs = self._function(dataset_or_iterator)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1972, in test_function\n      return step_function(self, iterator)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1956, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1944, in run_step\n      outputs = model.test_step(data)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1850, in test_step\n      y_pred = self(x, training=False)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/sequential.py\", line 405, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py\", line 255, in call\n      outputs = self.activation(outputs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/activations.py\", line 321, in relu\n      return backend.relu(\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/backend.py\", line 5397, in relu\n      x = tf.nn.relu(x)\nNode: 'sequential_8/dense_16/Relu'\nMatrix size-incompatible: In[0]: [32,16875], In[1]: [2048,32]\n\t [[{{node sequential_8/dense_16/Relu}}]] [Op:__inference_test_function_1955869]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m val_gen \u001b[38;5;241m=\u001b[39m test_data_gen\u001b[38;5;241m.\u001b[39mflow_from_directory(test_dir,target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m150\u001b[39m,\u001b[38;5;241m150\u001b[39m),class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m test \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate_generator(val_gen)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:2852\u001b[0m, in \u001b[0;36mModel.evaluate_generator\u001b[0;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   2844\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2845\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.evaluate_generator` is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2846\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2847\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `Model.evaluate`, which supports generators.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2848\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2849\u001b[0m )\n\u001b[1;32m   2850\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluate_generator\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m   2853\u001b[0m     generator,\n\u001b[1;32m   2854\u001b[0m     steps\u001b[38;5;241m=\u001b[39msteps,\n\u001b[1;32m   2855\u001b[0m     max_queue_size\u001b[38;5;241m=\u001b[39mmax_queue_size,\n\u001b[1;32m   2856\u001b[0m     workers\u001b[38;5;241m=\u001b[39mworkers,\n\u001b[1;32m   2857\u001b[0m     use_multiprocessing\u001b[38;5;241m=\u001b[39muse_multiprocessing,\n\u001b[1;32m   2858\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2859\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m   2860\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_8/dense_16/Relu' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_3085/2602469645.py\", line 2, in <module>\n      test = model.evaluate_generator(test_gen)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2852, in evaluate_generator\n      return self.evaluate(\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2200, in evaluate\n      logs = test_function_runner.run_step(\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 4000, in run_step\n      tmp_logs = self._function(dataset_or_iterator)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1972, in test_function\n      return step_function(self, iterator)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1956, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1944, in run_step\n      outputs = model.test_step(data)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1850, in test_step\n      y_pred = self(x, training=False)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/sequential.py\", line 405, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py\", line 255, in call\n      outputs = self.activation(outputs)\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/activations.py\", line 321, in relu\n      return backend.relu(\n    File \"/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/backend.py\", line 5397, in relu\n      x = tf.nn.relu(x)\nNode: 'sequential_8/dense_16/Relu'\nMatrix size-incompatible: In[0]: [32,16875], In[1]: [2048,32]\n\t [[{{node sequential_8/dense_16/Relu}}]] [Op:__inference_test_function_1955869]"
     ]
    }
   ],
   "source": [
    "val_gen = test_data_gen.flow_from_directory(test_dir,target_size=(150,150),class_mode='binary',batch_size=32)\n",
    "test = model.evaluate_generator(val_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553cb5fd",
   "metadata": {},
   "source": [
    "####  Printing Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88f014fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1394065618515015, 0.6491228342056274]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675bee8c",
   "metadata": {},
   "source": [
    "#### ======================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dad16ae",
   "metadata": {},
   "source": [
    "#### Making Model by using Feature Extraction fron pretrained convent (VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "839c626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 14714688 (56.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv = VGG16(include_top=False,input_shape=(150,150,3))\n",
    "conv.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d565c21",
   "metadata": {},
   "source": [
    "#### Featture Extraction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34eabea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(directory,sample_count,vertical_flip=False,horizontal_flip=False,zoom_range=0.0):\n",
    "    features = np.zeros(shape=(sample_count,4,4,512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    gen = ImageDataGenerator(rescale=1./255,vertical_flip=vertical_flip,horizontal_flip=horizontal_flip,zoom_range=zoom_range)\n",
    "    data_gen = gen.flow_from_directory(directory,target_size=(150,150),class_mode='binary',batch_size=30)\n",
    "    i=0\n",
    "    for img_batch,label_batch in data_gen:\n",
    "        prediction = conv.predict(img_batch)\n",
    "        features[i*30 : (i+1)*30] = prediction \n",
    "        labels[i*30:(i+1)*30] = label_batch\n",
    "        i+=1\n",
    "        if i*30 >= sample_count:\n",
    "            return features,labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c4b517",
   "metadata": {},
   "source": [
    "#### passing train images and validation images to Feature Extraction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0802dd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 images belonging to 2 classes.\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "Found 120 images belonging to 2 classes.\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n"
     ]
    }
   ],
   "source": [
    "train_features = feature_extraction(train_dir,300,vertical_flip=True,horizontal_flip=True,zoom_range=(0.2,0.5))\n",
    "validation_features = feature_extraction(validation_dir,120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb3f4c2",
   "metadata": {},
   "source": [
    "#### making model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d06563d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m,loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mRMSprop(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m),loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 17\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3403\u001b[0m, in \u001b[0;36mModel.summary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3372\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[1;32m   3373\u001b[0m \n\u001b[1;32m   3374\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3400\u001b[0m \u001b[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[1;32m   3401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m-> 3403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3404\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3407\u001b[0m     )\n\u001b[1;32m   3408\u001b[0m layer_utils\u001b[38;5;241m.\u001b[39mprint_summary(\n\u001b[1;32m   3409\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3410\u001b[0m     line_length\u001b[38;5;241m=\u001b[39mline_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3415\u001b[0m     layer_range\u001b[38;5;241m=\u001b[39mlayer_range,\n\u001b[1;32m   3416\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "# model2.add(layers.MaxPool2D((2,2)))\n",
    "# model2.add(layers.Flatten())\n",
    "# model2.add(layers.Dropout(0.5))\n",
    "# model2.add(layers.Dense(32,activation='relu'))\n",
    "# # model2.add(layers.Dropout(0.5))\n",
    "# model2.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32,activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=1e-5),loss='binary_crossentropy',metrics=['acc'])\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb5ae6",
   "metadata": {},
   "source": [
    "#### Training Or Fitting The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1a42b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 4, 4, 512)\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 1s 35ms/step - loss: 1.1816 - acc: 0.5000 - val_loss: 0.6140 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 1.1233 - acc: 0.5000 - val_loss: 0.6159 - val_acc: 0.6667\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 1.0569 - acc: 0.5000 - val_loss: 0.6203 - val_acc: 0.6667\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 1.0382 - acc: 0.5000 - val_loss: 0.6269 - val_acc: 0.6667\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.9630 - acc: 0.5033 - val_loss: 0.6358 - val_acc: 0.6667\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.9188 - acc: 0.4967 - val_loss: 0.6457 - val_acc: 0.6583\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8869 - acc: 0.4967 - val_loss: 0.6572 - val_acc: 0.6250\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.8594 - acc: 0.4933 - val_loss: 0.6705 - val_acc: 0.6000\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.7993 - acc: 0.5000 - val_loss: 0.6829 - val_acc: 0.5750\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8039 - acc: 0.4933 - val_loss: 0.6985 - val_acc: 0.4917\n"
     ]
    }
   ],
   "source": [
    "print((train_features[0]).shape)\n",
    "history = model.fit(train_features[0],train_features[1],epochs=10,batch_size=32,validation_data=validation_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e87ba",
   "metadata": {},
   "source": [
    "#### ======================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034376a",
   "metadata": {},
   "source": [
    "#### Making Model Architecture By using convent as an layer but freeze convent from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b0e7007",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.trainable = False\n",
    "model = models.Sequential()\n",
    "model.add(conv)\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32,activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=2e-5),loss='binary_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca55b2",
   "metadata": {},
   "source": [
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1297df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 2, 2, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                65568     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14780289 (56.38 MB)\n",
      "Trainable params: 65601 (256.25 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da949a",
   "metadata": {},
   "source": [
    "####  Training or Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "775a30e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 99s 10s/step - loss: 0.7366 - acc: 0.5333 - val_loss: 0.7434 - val_acc: 0.3860\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 105s 10s/step - loss: 0.7233 - acc: 0.5300 - val_loss: 0.7433 - val_acc: 0.3947\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 92s 10s/step - loss: 0.7099 - acc: 0.5467 - val_loss: 0.7429 - val_acc: 0.3860\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 86s 8s/step - loss: 0.7419 - acc: 0.4733 - val_loss: 0.7459 - val_acc: 0.3860\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 92s 9s/step - loss: 0.7117 - acc: 0.5000 - val_loss: 0.7477 - val_acc: 0.3860\n"
     ]
    }
   ],
   "source": [
    "# model.fit(train_gen,steps_per_epoch=10,epochs=5,validation_data=val_gen,validation_steps=5)\n",
    "history = model.fit(train_gen,epochs=5,steps_per_epoch=10,validation_data=val_gen,validation_steps=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a8933",
   "metadata": {},
   "source": [
    "#### =================================================Fine Tuning==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd30f63",
   "metadata": {},
   "source": [
    "#### Freeze convent From learning except few lower layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5935b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.trainable = True\n",
    "set_trainable = False\n",
    "for layer in conv.layers:\n",
    "    if layer.name == 'block5':\n",
    "        set_trainableinable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f48bb439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997695b",
   "metadata": {},
   "source": [
    "#### Making Architeture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36df599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(conv)\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32,activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=2e-5),loss='binary_crossentropy',metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca09b8a",
   "metadata": {},
   "source": [
    "#### Summary Of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "013c2914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
      "                                                                 \n",
      " max_pooling2d_39 (MaxPooli  (None, 2, 2, 512)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_39 (Flatten)        (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 32)                65568     \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14780289 (56.38 MB)\n",
      "Trainable params: 65601 (256.25 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8762b722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 77s 8s/step - loss: 0.7082 - acc: 0.5367 - val_loss: 0.6945 - val_acc: 0.4750\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 83s 9s/step - loss: 0.6836 - acc: 0.5733 - val_loss: 0.6925 - val_acc: 0.4917\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 86s 8s/step - loss: 0.7325 - acc: 0.5033 - val_loss: 0.6946 - val_acc: 0.4917\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 76s 8s/step - loss: 0.7119 - acc: 0.5067 - val_loss: 0.6883 - val_acc: 0.4917\n",
      "Epoch 5/10\n",
      " 1/10 [==>...........................] - ETA: 53s - loss: 0.7015 - acc: 0.5938"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_gen,epochs=10,steps_per_epoch=10,validation_data=val_gen,validation_steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60fcdf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_extraction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_features,train_labels \u001b[38;5;241m=\u001b[39m feature_extraction(train_dir,\u001b[38;5;241m300\u001b[39m,vertical_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,horizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,zoom_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.5\u001b[39m))\n\u001b[1;32m      2\u001b[0m validation_features,validation_labels \u001b[38;5;241m=\u001b[39m feature_extraction(validation_dir,\u001b[38;5;241m120\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_extraction' is not defined"
     ]
    }
   ],
   "source": [
    "train_features,train_labels = feature_extraction(train_dir,300,vertical_flip=True,horizontal_flip=True,zoom_range=(0.2,0.5))\n",
    "validation_features,validation_labels = feature_extraction(validation_dir,120)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c356f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(540,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (9,) (90,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m concatenated_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((a, b), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(concatenated_array\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m a[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m10\u001b[39m] \u001b[38;5;241m+\u001b[39m a[\u001b[38;5;241m10\u001b[39m:\u001b[38;5;241m100\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (9,) (90,) "
     ]
    }
   ],
   "source": [
    "a = np.array(train_labels)\n",
    "b = np.array(validation_labels)\n",
    "concatenated_array = np.concatenate((a, b), axis=0)\n",
    "print(concatenated_array.shape)\n",
    "a[1:10] + a[10:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0dfb9",
   "metadata": {},
   "source": [
    "####  ============================= Applying K-fold Technique ==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df6ef1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(directory,sample_count,vertical_flip=False,horizontal_flip=False,zoom_range=0.0):\n",
    "    features = np.zeros(shape=(sample_count,4,4,512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    gen = ImageDataGenerator(rescale=1./255,vertical_flip=vertical_flip,horizontal_flip=horizontal_flip,zoom_range=zoom_range)\n",
    "    data_gen = gen.flow_from_directory(directory,target_size=(150,150),class_mode='binary',batch_size=30)\n",
    "    i=0\n",
    "    for img_batch,label_batch in data_gen:\n",
    "        prediction = conv.predict(img_batch)\n",
    "        features[i*30 : (i+1)*30] = prediction \n",
    "        labels[i*30:(i+1)*30] = label_batch\n",
    "        i+=1\n",
    "        if i*30 >= sample_count:\n",
    "            return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddd92151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 512)\n"
     ]
    }
   ],
   "source": [
    "print((train_features[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509bd0ea",
   "metadata": {},
   "source": [
    "#### Applying Feature Extraction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "899b9439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 images belonging to 2 classes.\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "Found 120 images belonging to 2 classes.\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "1/1 [==============================] - 10s 10s/step\n",
      "(420, 4, 4, 512)\n"
     ]
    }
   ],
   "source": [
    "train_features,train_labels = feature_extraction(train_dir,300,vertical_flip=True,horizontal_flip=True,zoom_range=(0.2,0.5))\n",
    "validation_features,validation_labels = feature_extraction(validation_dir,120)\n",
    "train_features = np.concatenate((train_features,validation_features), axis=0)\n",
    "train_labels =np.concatenate((train_labels,validation_labels),axis=0)\n",
    "\n",
    "print((train_features.shape))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((np.reshape(train_labels,(420))))\n",
    "# print((validation_labels))\n",
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10fa2aa",
   "metadata": {},
   "source": [
    "####  Making build Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7e979b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "#     conv.trainable = False\n",
    "    model2 = models.Sequential()\n",
    "    model2.add(layers.MaxPool2D((2,2)))\n",
    "    model2.add(layers.Flatten())\n",
    "    model2.add(layers.Dropout(0.5))\n",
    "    model2.add(layers.Dense(32,activation='relu'))\n",
    "    # model2.add(layers.Dropout(0.5))\n",
    "    model2.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "    model2.compile(optimizer=optimizers.RMSprop(learning_rate=1e-5),loss='binary_crossentropy',metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e63bf4",
   "metadata": {},
   "source": [
    "#### applying K- Fold Technique By using above function In which Number of Folds is 5 also training the model and Number Of Ecpochs 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc006103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold #  0\n",
      "(336,)\n",
      "(336, 4, 4, 512)\n",
      "(84, 4, 4, 512)\n",
      "(84,)\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 1s 5ms/step - loss: 0.7323 - acc: 0.4911\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7204 - acc: 0.5208\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.7071 - acc: 0.5774\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.7235 - acc: 0.5179\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.7239 - acc: 0.4911\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.7168 - acc: 0.5149\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.6957 - acc: 0.5446\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.7241 - acc: 0.5327\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7019 - acc: 0.5536\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.7039 - acc: 0.5536\n",
      "fold #  1\n",
      "(336,)\n",
      "(336, 4, 4, 512)\n",
      "(84, 4, 4, 512)\n",
      "(84,)\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.7215 - acc: 0.5357\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.7114 - acc: 0.5417\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.7069 - acc: 0.5298\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.7106 - acc: 0.5060\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.7047 - acc: 0.5298\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.7203 - acc: 0.5238\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.7000 - acc: 0.5268\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.7007 - acc: 0.5238\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.7254 - acc: 0.5327\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6987 - acc: 0.5446\n",
      "fold #  2\n",
      "(336,)\n",
      "(336, 4, 4, 512)\n",
      "(84, 4, 4, 512)\n",
      "(84,)\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6769 - acc: 0.5625\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.6995 - acc: 0.5536\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6712 - acc: 0.6012\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6955 - acc: 0.5387\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6664 - acc: 0.5982\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7017 - acc: 0.5476\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7109 - acc: 0.5208\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6992 - acc: 0.5565\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6809 - acc: 0.5714\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6918 - acc: 0.5446\n",
      "fold #  3\n",
      "(336,)\n",
      "(336, 4, 4, 512)\n",
      "(84, 4, 4, 512)\n",
      "(84,)\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6851 - acc: 0.5357\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6977 - acc: 0.5268\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7095 - acc: 0.5119\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6892 - acc: 0.5387\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6846 - acc: 0.5893\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6969 - acc: 0.5268\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6924 - acc: 0.5536\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6895 - acc: 0.5595\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6945 - acc: 0.5357\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6867 - acc: 0.5565\n",
      "fold #  4\n",
      "(336,)\n",
      "(336, 4, 4, 512)\n",
      "(84, 4, 4, 512)\n",
      "(84,)\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6920 - acc: 0.5446\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.7076 - acc: 0.5268\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6944 - acc: 0.5417\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7051 - acc: 0.5089\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6800 - acc: 0.5357\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.6762 - acc: 0.5774\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.6905 - acc: 0.5625\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.6978 - acc: 0.5149\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.6811 - acc: 0.5744\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.6792 - acc: 0.5506\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folds = 5\n",
    "sample_per_fold = len(train_labels)//folds\n",
    "validation_score = []\n",
    "for fold in range(folds) :\n",
    "    print('fold # ', fold)\n",
    "    validatoin_data = train_features[fold * sample_per_fold : (fold+1)*sample_per_fold]\n",
    "    validation_label = train_labels[fold*sample_per_fold : (fold+1)*sample_per_fold]\n",
    "    train_data = np.concatenate([train_features[:fold * sample_per_fold],train_features[(fold + 1) * sample_per_fold:]],axis=0)  \n",
    "    train_label = np.concatenate([train_labels[:fold * sample_per_fold],train_labels[(fold + 1) * sample_per_fold:]],axis=0)\n",
    "    print(train_label.shape)\n",
    "    print(train_data.shape)\n",
    "    print(validatoin_data.shape)\n",
    "    print(validation_label.shape)\n",
    "    model = build_model()\n",
    "#     model.summary()\n",
    "#     history = model.fit(train_features,train_labels,batch_size=32,epochs=10)\n",
    "    history = model.fit(train_data, train_label,epochs=10, batch_size=32)\n",
    "    #     validation_score.append(history.history['val_acc'])\n",
    "    \n",
    "    \n",
    "    \n",
    "print(validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a4c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44305fd3",
   "metadata": {},
   "source": [
    "#### Applying K- Fold Method With number of epochs 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38266622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n",
      "processing fold # 4\n"
     ]
    }
   ],
   "source": [
    "folds=5\n",
    "num_val_samples = len(train_labels)//folds\n",
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = train_features[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_labels[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate([train_features[:i * num_val_samples],train_features[(i + 1) * num_val_samples:]],axis=0)\n",
    "    partial_train_targets = np.concatenate([train_labels[:i * num_val_samples],train_labels[(i + 1) * num_val_samples:]],axis=0)\n",
    "    model = build_model()\n",
    "    history = model.fit(train_features,train_labels,epochs=num_epochs, batch_size=32,validation_data=(val_data,val_targets),verbose=0)\n",
    "    val_history = history.history['val_acc']\n",
    "    validation_score.append(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d81b3",
   "metadata": {},
   "source": [
    "#### Checking Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95a4c59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8452380895614624,\n",
       "  0.8452380895614624,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8452380895614624,\n",
       "  0.8571428656578064,\n",
       "  0.8452380895614624,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8452380895614624,\n",
       "  0.8452380895614624,\n",
       "  0.8333333134651184,\n",
       "  0.8452380895614624,\n",
       "  0.8452380895614624,\n",
       "  0.8452380895614624,\n",
       "  0.8571428656578064,\n",
       "  0.8452380895614624,\n",
       "  0.8571428656578064,\n",
       "  0.8452380895614624,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8452380895614624,\n",
       "  0.8452380895614624,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8452380895614624,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8452380895614624,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8571428656578064,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8452380895614624,\n",
       "  0.8571428656578064,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8571428656578064,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8571428656578064,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8690476417541504,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8690476417541504,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8809523582458496,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8690476417541504,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8571428656578064,\n",
       "  0.8809523582458496,\n",
       "  0.8571428656578064,\n",
       "  0.8690476417541504,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496],\n",
       " [0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8690476417541504,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8809523582458496,\n",
       "  0.8809523582458496,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8809523582458496,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504,\n",
       "  0.8690476417541504],\n",
       " [0.8095238208770752,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8095238208770752,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8214285969734192,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184,\n",
       "  0.8333333134651184],\n",
       " [0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248,\n",
       "  0.9404761791229248],\n",
       " [0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656,\n",
       "  0.988095223903656]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f614a",
   "metadata": {},
   "source": [
    "#### Finding Mean Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6a01fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896200002193451"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "096325c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "639831c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 114 images belonging to 2 classes.\n",
      "(32, 150, 150, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_gen = test_data_gen.flow_from_directory(test_dir,target_size=(150,150),class_mode='binary',batch_size=32)\n",
    "for data,label in test_gen:\n",
    "    print(data.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f47c4",
   "metadata": {},
   "source": [
    "#### Evaluate Model on test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4cdf4853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 114 images belonging to 2 classes.\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4091 - acc: 0.8684\n"
     ]
    }
   ],
   "source": [
    "test_feature,test_label = feature_extraction(test_dir,114)\n",
    "test = model.evaluate(test_feature,test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62b9eb",
   "metadata": {},
   "source": [
    "#### Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "202ee414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.40908724069595337, 0.8684210777282715]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1d8e1",
   "metadata": {},
   "source": [
    "####  checking mean As per Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e201d0db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "(5, 500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8880952358245849"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(validation_score))\n",
    "print(len(validation_score[0]))\n",
    "for i in range(num_epochs):\n",
    "    a = []\n",
    "    for x in validation_score:\n",
    "#         print(x)\n",
    "        a.append(x[i])\n",
    "        print(i)\n",
    "#     print(a)\n",
    "    m = np.mean(a)\n",
    "    break\n",
    "# print(m)\n",
    "\n",
    "\n",
    "val = np.array(validation_score)\n",
    "print(val.shape)\n",
    "mean = np.mean(val,axis=0)\n",
    "mean.shape\n",
    "mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d2418b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_val_history = np.mean(val,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392299b",
   "metadata": {},
   "source": [
    "####  Taking mean As per Epochs and Visualize The result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "31d5bc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "average_val_history = [np.mean([x[i] for x in validation_score]) for i in range(num_epochs)]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_val_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0b80d",
   "metadata": {},
   "source": [
    "#### Saving And Loading The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8fdcc9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hassan-ahmed-khan/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('lungs_cancer_classification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4a98f184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lungs_cancer_classification.sav/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lungs_cancer_classification.sav/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('lungs_cancer_classification.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e7bd6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.load_model('lungs_cancer_classification.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "981a8282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " max_pooling2d_8 (MaxPoolin  (None, None, None, None   0         \n",
      " g2D)                        )                                   \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, None, None, None   0         \n",
      "                             )                                   \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, None)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 32)                65568     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65601 (256.25 KB)\n",
      "Trainable params: 65601 (256.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "473f943a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 512)\n",
      "1.0\n",
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.94766915], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_feature[0].shape)\n",
    "print(test_label[50])\n",
    "\n",
    "network.predict(test_feature)[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3e053176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4091 - acc: 0.8684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40908724069595337, 0.8684210777282715]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.evaluate(test_feature,test_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

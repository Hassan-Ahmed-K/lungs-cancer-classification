{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mbymiPzkCBlg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbymiPzkCBlg",
        "outputId": "a1c78b90-8986-4fea-dd9d-930b9bfd7ee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6WtAMp88cCN",
      "metadata": {
        "id": "e6WtAMp88cCN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"/content/lung cancer Detection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2blk9P-tB_NA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2blk9P-tB_NA",
        "outputId": "db68c271-f078-4417-99f6-aaa737ae8926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unrar is already the newest version (1:6.1.5-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "\n",
            "UNRAR 6.11 beta 1 freeware      Copyright (c) 1993-2022 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/drive/MyDrive/lung_cancer_dataset.rar\n",
            "\n",
            "Creating    /content/lung cancer Detection/nagative                   OK\n",
            "Extracting  /content/lung cancer Detection/nagative/person100_bacteria_475.jpeg     \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person100_bacteria_477.jpeg     \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person100_bacteria_478.jpeg     \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person100_bacteria_479.jpeg     \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person100_bacteria_480.jpeg     \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person100_bacteria_481.jpeg     \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person100_bacteria_482.jpeg     \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person101_bacteria_483.jpeg     \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person101_bacteria_484.jpeg     \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person101_bacteria_485.jpeg     \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person101_bacteria_486.jpeg     \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person102_bacteria_487.jpeg     \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person103_bacteria_488.jpeg     \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person103_bacteria_489.jpeg     \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person103_bacteria_490.jpeg     \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person104_bacteria_491.jpeg     \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person104_bacteria_492.jpeg     \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person108_bacteria_504.jpeg     \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person108_bacteria_506.jpeg     \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person108_bacteria_507.jpeg     \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person108_bacteria_511.jpeg     \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person109_bacteria_512.jpeg     \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person109_bacteria_513.jpeg     \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person109_bacteria_517.jpeg     \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person10_virus_35.jpeg     \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person11_virus_38.jpeg     \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person134_bacteria_644.jpeg     \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person135_bacteria_646.jpeg     \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person135_bacteria_647.jpeg     \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person136_bacteria_648.jpeg     \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person136_bacteria_649.jpeg     \b\b\b\b  2%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person136_bacteria_650.jpeg     \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person136_bacteria_652.jpeg     \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person136_bacteria_654.jpeg     \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person137_bacteria_655.jpeg     \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person138_bacteria_657.jpeg     \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person138_bacteria_658.jpeg     \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person138_bacteria_659.jpeg     \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person139_bacteria_661.jpeg     \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person139_bacteria_664.jpeg     \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person139_bacteria_665.jpeg     \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person139_bacteria_666.jpeg     \b\b\b\b  3%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person140_bacteria_667.jpeg     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person140_bacteria_668.jpeg     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person141_bacteria_670.jpeg     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person141_bacteria_676.jpeg     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person141_bacteria_677.jpeg     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person141_bacteria_678.jpeg     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person141_bacteria_681.jpeg     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person142_bacteria_682.jpeg     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person144_bacteria_690.jpeg     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person146_bacteria_700.jpeg     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person146_bacteria_703.jpeg     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person146_bacteria_704.jpeg     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person147_bacteria_705.jpeg     \b\b\b\b  4%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person147_bacteria_706.jpeg     \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person147_bacteria_707.jpeg     \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person14_virus_44.jpeg     \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person150_bacteria_717.jpeg     \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person151_bacteria_718.jpeg     \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person152_bacteria_720.jpeg     \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person152_bacteria_721.jpeg     \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person152_bacteria_722.jpeg     \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person152_bacteria_723.jpeg     \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person152_bacteria_724.jpeg     \b\b\b\b  5%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person153_bacteria_725.jpeg     \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person153_bacteria_726.jpeg     \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person154_bacteria_728.jpeg     \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person155_bacteria_729.jpeg     \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person155_bacteria_730.jpeg     \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person155_bacteria_731.jpeg     \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person157_bacteria_735.jpeg     \b\b\b\b  6%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person157_bacteria_739.jpeg     \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person157_bacteria_740.jpeg     \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person158_bacteria_742.jpeg     \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person158_bacteria_743.jpeg     \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person158_bacteria_744.jpeg     \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person158_bacteria_745.jpeg     \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person159_bacteria_746.jpeg     \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person159_bacteria_747.jpeg     \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person15_virus_46.jpeg     \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1608_virus_2786.jpeg     \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1610_virus_2793.jpeg     \b\b\b\b  7%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1612_virus_2797.jpeg     \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1612_virus_2798.jpeg     \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1613_virus_2799.jpeg     \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1614_virus_2800.jpeg     \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1615_virus_2801.jpeg     \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1616_virus_2802.jpeg     \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1618_virus_2805.jpeg     \b\b\b\b  8%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1619_virus_2806.jpeg     \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person161_bacteria_757.jpeg     \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person161_bacteria_759.jpeg     \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person161_bacteria_762.jpeg     \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1620_virus_2807.jpeg     \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1622_virus_2810.jpeg     \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1623_virus_2813.jpeg     \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1625_virus_2817.jpeg     \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1626_virus_2818.jpeg     \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1627_virus_2819.jpeg     \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1628_virus_2821.jpeg     \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1628_virus_2822.jpeg     \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1629_virus_2823.jpeg     \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1631_virus_2826.jpeg     \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1632_virus_2827.jpeg     \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1633_virus_2829.jpeg     \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1634_virus_2830.jpeg     \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1635_virus_2831.jpeg     \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1637_virus_2834.jpeg     \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1640_virus_2839.jpeg     \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1641_virus_2840.jpeg     \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1642_virus_2842.jpeg     \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1643_virus_2843.jpeg     \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1644_virus_2844.jpeg     \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1645_virus_2845.jpeg     \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1647_virus_2848.jpeg     \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1649_virus_2850.jpeg     \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1650_virus_2852.jpeg     \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1650_virus_2854.jpeg     \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1651_virus_2855.jpeg     \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1653_virus_2858.jpeg     \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1653_virus_2859.jpeg     \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1655_virus_2861.jpeg     \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1656_virus_2862.jpeg     \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1657_virus_2864.jpeg     \b\b\b\b 12%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1659_virus_2867.jpeg     \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1660_virus_2869.jpeg     \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1661_virus_2872.jpeg     \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1661_virus_2873.jpeg     \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1662_virus_2875.jpeg     \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1663_virus_2876.jpeg     \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1664_virus_2877.jpeg     \b\b\b\b 13%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1665_virus_2878.jpeg     \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1667_virus_2881.jpeg     \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1668_virus_2882.jpeg     \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1669_virus_2884.jpeg     \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1669_virus_2885.jpeg     \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1670_virus_2886.jpeg     \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1671_virus_2887.jpeg     \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1672_virus_2888.jpeg     \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1673_virus_2889.jpeg     \b\b\b\b 14%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1674_virus_2890.jpeg     \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1675_virus_2891.jpeg     \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1676_virus_2892.jpeg     \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1678_virus_2895.jpeg     \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1679_virus_2896.jpeg     \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1680_virus_2897.jpeg     \b\b\b\b 15%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1682_virus_2899.jpeg     \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1685_virus_2903.jpeg     \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person16_virus_47.jpeg     \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person171_bacteria_826.jpeg     \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person172_bacteria_827.jpeg     \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person172_bacteria_828.jpeg     \b\b\b\b 16%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person173_bacteria_829.jpeg     \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person173_bacteria_830.jpeg     \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person173_bacteria_831.jpeg     \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person174_bacteria_832.jpeg     \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person175_bacteria_833.jpeg     \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person175_bacteria_834.jpeg     \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person175_bacteria_835.jpeg     \b\b\b\b 17%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person17_virus_48.jpeg     \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person18_virus_49.jpeg     \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person19_virus_50.jpeg     \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1_virus_11.jpeg     \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1_virus_12.jpeg     \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1_virus_13.jpeg     \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1_virus_6.jpeg     \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1_virus_7.jpeg     \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1_virus_8.jpeg     \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person1_virus_9.jpeg     \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person20_virus_51.jpeg     \b\b\b\b 18%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person21_virus_52.jpeg     \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person21_virus_53.jpeg     \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person22_virus_54.jpeg     \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person22_virus_55.jpeg     \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person23_virus_56.jpeg     \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person24_virus_58.jpeg     \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person25_virus_59.jpeg     \b\b\b\b 19%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person26_virus_60.jpeg     \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person28_virus_62.jpeg     \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person28_virus_63.jpeg     \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person29_virus_64.jpeg     \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person30_virus_69.jpeg     \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person31_virus_70.jpeg     \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person32_virus_71.jpeg     \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person33_virus_72.jpeg     \b\b\b\b 20%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person34_virus_76.jpeg     \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person35_virus_80.jpeg     \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person36_virus_81.jpeg     \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person37_virus_82.jpeg     \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person38_virus_83.jpeg     \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person38_virus_84.jpeg     \b\b\b\b 21%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person39_virus_85.jpeg     \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person3_virus_15.jpeg     \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person3_virus_16.jpeg     \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person3_virus_17.jpeg     \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person40_virus_87.jpeg     \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person41_virus_88.jpeg     \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person42_virus_89.jpeg     \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person43_virus_92.jpeg     \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person44_virus_93.jpeg     \b\b\b\b 22%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person44_virus_94.jpeg     \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person45_virus_95.jpeg     \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person46_virus_96.jpeg     \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person47_virus_99.jpeg     \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person48_virus_100.jpeg     \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person49_virus_101.jpeg     \b\b\b\b 23%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person50_virus_102.jpeg     \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person51_virus_105.jpeg     \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person52_virus_106.jpeg     \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person53_virus_107.jpeg     \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person53_virus_108.jpeg     \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person54_virus_109.jpeg     \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person55_virus_110.jpeg     \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person56_virus_112.jpeg     \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person57_virus_113.jpeg     \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person59_virus_116.jpeg     \b\b\b\b 24%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person60_virus_117.jpeg     \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person61_virus_118.jpeg     \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person62_virus_119.jpeg     \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person63_virus_121.jpeg     \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person64_virus_122.jpeg     \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person65_virus_123.jpeg     \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person66_virus_125.jpeg     \b\b\b\b 25%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person67_virus_126.jpeg     \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person69_virus_129.jpeg     \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person70_virus_130.jpeg     \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person71_virus_131.jpeg     \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person71_virus_132.jpeg     \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person72_virus_133.jpeg     \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person74_virus_135.jpeg     \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person75_virus_136.jpeg     \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person76_virus_138.jpeg     \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person77_virus_139.jpeg     \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person78_bacteria_378.jpeg     \b\b\b\b 26%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person78_bacteria_380.jpeg     \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person78_bacteria_381.jpeg     \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person78_bacteria_382.jpeg     \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person78_bacteria_384.jpeg     \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person78_bacteria_385.jpeg     \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person78_bacteria_386.jpeg     \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person78_bacteria_387.jpeg     \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person78_virus_140.jpeg     \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person79_virus_148.jpeg     \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person80_bacteria_389.jpeg     \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person80_bacteria_390.jpeg     \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person80_bacteria_391.jpeg     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person80_bacteria_392.jpeg     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person80_bacteria_393.jpeg     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person81_bacteria_395.jpeg     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person81_bacteria_396.jpeg     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person81_bacteria_397.jpeg     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person81_bacteria_398.jpeg     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person82_bacteria_402.jpeg     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person82_bacteria_403.jpeg     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person82_bacteria_404.jpeg     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person82_bacteria_405.jpeg     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person83_bacteria_407.jpeg     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person83_bacteria_409.jpeg     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person83_bacteria_410.jpeg     \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person83_bacteria_411.jpeg     \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person83_bacteria_412.jpeg     \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person83_bacteria_414.jpeg     \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person85_bacteria_417.jpeg     \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person85_bacteria_419.jpeg     \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person85_bacteria_421.jpeg     \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person85_bacteria_422.jpeg     \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person85_bacteria_423.jpeg     \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person85_bacteria_424.jpeg     \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person86_bacteria_428.jpeg     \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person86_bacteria_429.jpeg     \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person87_bacteria_433.jpeg     \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person87_bacteria_434.jpeg     \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person88_bacteria_437.jpeg     \b\b\b\b 29%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person88_bacteria_438.jpeg     \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person88_bacteria_439.jpeg     \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person89_bacteria_440.jpeg     \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person8_virus_27.jpeg     \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person8_virus_28.jpeg     \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person90_bacteria_442.jpeg     \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person90_bacteria_443.jpeg     \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person91_bacteria_445.jpeg     \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person91_bacteria_446.jpeg     \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person91_bacteria_447.jpeg     \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person91_bacteria_448.jpeg     \b\b\b\b 30%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person91_bacteria_449.jpeg     \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person92_bacteria_450.jpeg     \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person92_bacteria_451.jpeg     \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person93_bacteria_453.jpeg     \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person93_bacteria_454.jpeg     \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person94_bacteria_456.jpeg     \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person94_bacteria_457.jpeg     \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person94_bacteria_458.jpeg     \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person95_bacteria_463.jpeg     \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person96_bacteria_464.jpeg     \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person96_bacteria_465.jpeg     \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person96_bacteria_466.jpeg     \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person97_bacteria_468.jpeg     \b\b\b\b 31%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person99_bacteria_473.jpeg     \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/nagative/person99_bacteria_474.jpeg     \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Creating    /content/lung cancer Detection/positive                   OK\n",
            "Extracting  /content/lung cancer Detection/positive/IM-0001-0001.jpeg     \b\b\b\b 32%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0003-0001.jpeg     \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0005-0001.jpeg     \b\b\b\b 33%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0006-0001.jpeg     \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0007-0001.jpeg     \b\b\b\b 34%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0009-0001.jpeg     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0010-0001.jpeg     \b\b\b\b 35%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0011-0001-0001.jpeg     \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0011-0001-0002.jpeg     \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0011-0001.jpeg     \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0013-0001.jpeg     \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0015-0001.jpeg     \b\b\b\b 38%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0016-0001.jpeg     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0017-0001.jpeg     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0019-0001.jpeg     \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0021-0001.jpeg     \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0022-0001.jpeg     \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0023-0001.jpeg     \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0025-0001.jpeg     \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0027-0001.jpeg     \b\b\b\b 42%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0028-0001.jpeg     \b\b\b\b 43%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0029-0001.jpeg     \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0030-0001.jpeg     \b\b\b\b 44%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0031-0001.jpeg     \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0033-0001-0001.jpeg     \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0033-0001-0002.jpeg     \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0033-0001.jpeg     \b\b\b\b 46%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0035-0001.jpeg     \b\b\b\b 47%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0036-0001.jpeg     \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0037-0001.jpeg     \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0039-0001.jpeg     \b\b\b\b 48%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0041-0001.jpeg     \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0043-0001.jpeg     \b\b\b\b 50%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0045-0001.jpeg     \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0046-0001.jpeg     \b\b\b\b 51%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0049-0001.jpeg     \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0050-0001.jpeg     \b\b\b\b 52%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0059-0001.jpeg     \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0061-0001.jpeg     \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0063-0001.jpeg     \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0065-0001.jpeg     \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0067-0001.jpeg     \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0069-0001.jpeg     \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0070-0001.jpeg     \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0071-0001.jpeg     \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0073-0001.jpeg     \b\b\b\b 54%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0075-0001.jpeg     \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0077-0001.jpeg     \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0079-0001.jpeg     \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0081-0001.jpeg     \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0083-0001.jpeg     \b\b\b\b 55%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0084-0001.jpeg     \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0085-0001.jpeg     \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0086-0001.jpeg     \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0087-0001.jpeg     \b\b\b\b 56%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0089-0001.jpeg     \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0091-0001.jpeg     \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0093-0001.jpeg     \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0095-0001.jpeg     \b\b\b\b 57%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0097-0001.jpeg     \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0099-0001.jpeg     \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0101-0001.jpeg     \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0102-0001.jpeg     \b\b\b\b 58%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0103-0001.jpeg     \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0105-0001.jpeg     \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0107-0001.jpeg     \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0109-0001.jpeg     \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0110-0001.jpeg     \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/IM-0111-0001.jpeg     \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0007-0001.jpeg     \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0012-0001.jpeg     \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0013-0001.jpeg     \b\b\b\b 60%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0019-0001.jpeg     \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0023-0001.jpeg     \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0027-0001.jpeg     \b\b\b\b 61%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0028-0001.jpeg     \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0029-0001.jpeg     \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0030-0001.jpeg     \b\b\b\b 62%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0033-0001.jpeg     \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0035-0001.jpeg     \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0041-0001.jpeg     \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0045-0001.jpeg     \b\b\b\b 63%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0051-0001.jpeg     \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0052-0001.jpeg     \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0058-0001.jpeg     \b\b\b\b 64%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0059-0001.jpeg     \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0060-0001.jpeg     \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0066-0001.jpeg     \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0072-0001.jpeg     \b\b\b\b 65%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0073-0001.jpeg     \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0079-0001.jpeg     \b\b\b\b 66%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0081-0001.jpeg     \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0086-0001.jpeg     \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0092-0001.jpeg     \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0095-0001.jpeg     \b\b\b\b 67%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0096-0001.jpeg     \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0098-0001.jpeg     \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0102-0001.jpeg     \b\b\b\b 68%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0105-0001.jpeg     \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0107-0001.jpeg     \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0110-0001.jpeg     \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0111-0001.jpeg     \b\b\b\b 69%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0112-0001.jpeg     \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0117-0001.jpeg     \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0120-0001.jpeg     \b\b\b\b 70%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0123-0001.jpeg     \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0129-0001.jpeg     \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0130-0001.jpeg     \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0131-0001.jpeg     \b\b\b\b 71%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0132-0001.jpeg     \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0135-0001.jpeg     \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0139-0001.jpeg     \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0141-0001.jpeg     \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0145-0001.jpeg     \b\b\b\b 72%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0146-0001.jpeg     \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0150-0001.jpeg     \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0171-0001.jpeg     \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0173-0001-0001.jpeg     \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0173-0001-0002.jpeg     \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0195-0001.jpeg     \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0196-0001.jpeg     \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0198-0001.jpeg     \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0199-0001.jpeg     \b\b\b\b 74%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0201-0001.jpeg     \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0206-0001.jpeg     \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0207-0001.jpeg     \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0210-0001.jpeg     \b\b\b\b 75%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0213-0001.jpeg     \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0217-0001.jpeg     \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0219-0001.jpeg     \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0221-0001.jpeg     \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0222-0001.jpeg     \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0229-0001.jpeg     \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0232-0001.jpeg     \b\b\b\b 76%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0233-0001.jpeg     \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0237-0001.jpeg     \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0238-0001.jpeg     \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0241-0001.jpeg     \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0246-0001-0001.jpeg     \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0246-0001-0002.jpeg     \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0246-0001.jpeg     \b\b\b\b 77%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0249-0001.jpeg     \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0251-0001.jpeg     \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0252-0001.jpeg     \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0256-0001.jpeg     \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0259-0001.jpeg     \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0267-0001.jpeg     \b\b\b\b 78%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0271-0001.jpeg     \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0272-0001.jpeg     \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0273-0001.jpeg     \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0274-0001.jpeg     \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0275-0001.jpeg     \b\b\b\b 79%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0276-0001.jpeg     \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0277-0001.jpeg     \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0278-0001.jpeg     \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0279-0001.jpeg     \b\b\b\b 80%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0280-0001.jpeg     \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0281-0001.jpeg     \b\b\b\b 81%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0282-0001.jpeg     \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0283-0001.jpeg     \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0285-0001.jpeg     \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0286-0001.jpeg     \b\b\b\b 82%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0287-0001.jpeg     \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0288-0001.jpeg     \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0289-0001.jpeg     \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0290-0001.jpeg     \b\b\b\b 83%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0292-0001.jpeg     \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0294-0001.jpeg     \b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0297-0001.jpeg     \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0300-0001.jpeg     \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0301-0001.jpeg     \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0302-0001.jpeg     \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0303-0001.jpeg     \b\b\b\b 85%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0304-0001.jpeg     \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0305-0001.jpeg     \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0307-0001.jpeg     \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0309-0001.jpeg     \b\b\b\b 86%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0310-0001.jpeg     \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0311-0001.jpeg     \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0312-0001.jpeg     \b\b\b\b 87%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0313-0001.jpeg     \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0315-0001.jpeg     \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0316-0001.jpeg     \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0317-0001.jpeg     \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0319-0001.jpeg     \b\b\b\b 88%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0321-0001.jpeg     \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0322-0001.jpeg     \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0323-0001.jpeg     \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0325-0001.jpeg     \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0326-0001.jpeg     \b\b\b\b 89%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0327-0001.jpeg     \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0328-0001.jpeg     \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0329-0001.jpeg     \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0330-0001.jpeg     \b\b\b\b 90%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0331-0001.jpeg     \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0332-0001.jpeg     \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0333-0001.jpeg     \b\b\b\b 91%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0335-0001.jpeg     \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0336-0001.jpeg     \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0337-0001.jpeg     \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0338-0001.jpeg     \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0339-0001.jpeg     \b\b\b\b 92%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0340-0001.jpeg     \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0341-0001.jpeg     \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0343-0001.jpeg     \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0345-0001.jpeg     \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0346-0001.jpeg     \b\b\b\b 93%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0347-0001.jpeg     \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0348-0001.jpeg     \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0349-0001.jpeg     \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0350-0001.jpeg     \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0351-0001.jpeg     \b\b\b\b 94%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0352-0001.jpeg     \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0353-0001.jpeg     \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0354-0001.jpeg     \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0357-0001.jpeg     \b\b\b\b 95%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0359-0001.jpeg     \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0360-0001.jpeg     \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0361-0001.jpeg     \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0362-0001.jpeg     \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0364-0001.jpeg     \b\b\b\b 96%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0366-0001.jpeg     \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0368-0001.jpeg     \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0369-0001.jpeg     \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0370-0001.jpeg     \b\b\b\b 97%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0372-0001.jpeg     \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0373-0001.jpeg     \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0374-0001-0001.jpeg     \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0374-0001-0002.jpeg     \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0374-0001.jpeg     \b\b\b\b 99%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0376-0001.jpeg     \b\b\b\b 99%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0378-0001.jpeg     \b\b\b\b 99%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0380-0001.jpeg     \b\b\b\b 99%\b\b\b\b\b  OK \n",
            "Extracting  /content/lung cancer Detection/positive/NORMAL2-IM-0381-0001.jpeg     \b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ]
        }
      ],
      "source": [
        "!apt-get install unrar\n",
        "!unrar x '/content/drive/MyDrive/lung_cancer_dataset.rar' '/content/lung cancer Detection'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "462024ef",
      "metadata": {
        "id": "462024ef"
      },
      "source": [
        "#### IMPORTING Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e0404a5",
      "metadata": {
        "id": "7e0404a5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import models,layers,preprocessing,optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eafd22a4",
      "metadata": {
        "id": "eafd22a4"
      },
      "source": [
        "#### Path Of The Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9ef231f",
      "metadata": {
        "id": "c9ef231f"
      },
      "outputs": [],
      "source": [
        "base_dir = r'/content/lung cancer Detection'\n",
        "positive = os.path.join(base_dir,'positive')\n",
        "negative = os.path.join(base_dir,'nagative')\n",
        "train_dir = os.path.join(base_dir,'train')\n",
        "test_dir = os.path.join(base_dir,'test')\n",
        "validation_dir = os.path.join(base_dir,'validation')\n",
        "train_pos_dir = os.path.join(train_dir,'positive')\n",
        "train_neg_dir = os.path.join(train_dir,'negative')\n",
        "test_pos_dir = os.path.join(test_dir,'positive')\n",
        "test_neg_dir = os.path.join(test_dir,'negative')\n",
        "validation_pos_dir = os.path.join(validation_dir,'positive')\n",
        "validation_neg_dir = os.path.join(validation_dir,'negative')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41002d3f",
      "metadata": {
        "id": "41002d3f"
      },
      "source": [
        "#### Making Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e86cd648",
      "metadata": {
        "id": "e86cd648"
      },
      "outputs": [],
      "source": [
        "os.mkdir(train_dir)\n",
        "os.mkdir(test_dir)\n",
        "os.mkdir(validation_dir)\n",
        "os.mkdir(train_pos_dir)\n",
        "os.mkdir(train_neg_dir)\n",
        "os.mkdir(test_pos_dir)\n",
        "os.mkdir(test_neg_dir)\n",
        "os.mkdir(validation_pos_dir)\n",
        "os.mkdir(validation_neg_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b39a8e9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b39a8e9b",
        "outputId": "95271798-78d3-449f-880d-5ec285987f89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234\n",
            "300\n"
          ]
        }
      ],
      "source": [
        "print(len(os.listdir(os.path.join(base_dir,'positive'))))\n",
        "print(len(os.listdir(os.path.join(base_dir,'nagative'))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9203d1ae",
      "metadata": {
        "id": "9203d1ae"
      },
      "source": [
        "#### Shuffle Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38fbe2b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38fbe2b6",
        "outputId": "43d21638-1be5-42a8-c2e4-6395e44b0ca5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NORMAL2-IM-0376-0001.jpeg',\n",
              " 'NORMAL2-IM-0359-0001.jpeg',\n",
              " 'NORMAL2-IM-0343-0001.jpeg',\n",
              " 'IM-0089-0001.jpeg',\n",
              " 'NORMAL2-IM-0368-0001.jpeg',\n",
              " 'NORMAL2-IM-0059-0001.jpeg',\n",
              " 'IM-0011-0001-0001.jpeg',\n",
              " 'NORMAL2-IM-0035-0001.jpeg',\n",
              " 'NORMAL2-IM-0145-0001.jpeg',\n",
              " 'IM-0027-0001.jpeg',\n",
              " 'IM-0071-0001.jpeg',\n",
              " 'NORMAL2-IM-0221-0001.jpeg',\n",
              " 'NORMAL2-IM-0173-0001-0002.jpeg',\n",
              " 'NORMAL2-IM-0139-0001.jpeg',\n",
              " 'NORMAL2-IM-0213-0001.jpeg',\n",
              " 'IM-0067-0001.jpeg',\n",
              " 'IM-0075-0001.jpeg',\n",
              " 'NORMAL2-IM-0285-0001.jpeg',\n",
              " 'IM-0019-0001.jpeg',\n",
              " 'NORMAL2-IM-0120-0001.jpeg',\n",
              " 'NORMAL2-IM-0374-0001.jpeg',\n",
              " 'IM-0028-0001.jpeg',\n",
              " 'NORMAL2-IM-0381-0001.jpeg',\n",
              " 'NORMAL2-IM-0229-0001.jpeg',\n",
              " 'NORMAL2-IM-0066-0001.jpeg',\n",
              " 'IM-0017-0001.jpeg',\n",
              " 'NORMAL2-IM-0195-0001.jpeg',\n",
              " 'NORMAL2-IM-0060-0001.jpeg',\n",
              " 'IM-0105-0001.jpeg',\n",
              " 'NORMAL2-IM-0370-0001.jpeg',\n",
              " 'IM-0107-0001.jpeg',\n",
              " 'IM-0013-0001.jpeg',\n",
              " 'NORMAL2-IM-0249-0001.jpeg',\n",
              " 'NORMAL2-IM-0051-0001.jpeg',\n",
              " 'IM-0021-0001.jpeg',\n",
              " 'NORMAL2-IM-0030-0001.jpeg',\n",
              " 'NORMAL2-IM-0135-0001.jpeg',\n",
              " 'NORMAL2-IM-0312-0001.jpeg',\n",
              " 'NORMAL2-IM-0311-0001.jpeg',\n",
              " 'IM-0046-0001.jpeg',\n",
              " 'NORMAL2-IM-0364-0001.jpeg',\n",
              " 'NORMAL2-IM-0105-0001.jpeg',\n",
              " 'IM-0102-0001.jpeg',\n",
              " 'NORMAL2-IM-0129-0001.jpeg',\n",
              " 'NORMAL2-IM-0058-0001.jpeg',\n",
              " 'IM-0093-0001.jpeg',\n",
              " 'NORMAL2-IM-0246-0001-0001.jpeg',\n",
              " 'NORMAL2-IM-0352-0001.jpeg',\n",
              " 'IM-0073-0001.jpeg',\n",
              " 'NORMAL2-IM-0279-0001.jpeg',\n",
              " 'IM-0037-0001.jpeg',\n",
              " 'IM-0031-0001.jpeg',\n",
              " 'NORMAL2-IM-0313-0001.jpeg',\n",
              " 'NORMAL2-IM-0271-0001.jpeg',\n",
              " 'NORMAL2-IM-0199-0001.jpeg',\n",
              " 'IM-0030-0001.jpeg',\n",
              " 'IM-0025-0001.jpeg',\n",
              " 'IM-0005-0001.jpeg',\n",
              " 'NORMAL2-IM-0292-0001.jpeg',\n",
              " 'NORMAL2-IM-0081-0001.jpeg',\n",
              " 'NORMAL2-IM-0322-0001.jpeg',\n",
              " 'NORMAL2-IM-0073-0001.jpeg',\n",
              " 'IM-0036-0001.jpeg',\n",
              " 'NORMAL2-IM-0210-0001.jpeg',\n",
              " 'NORMAL2-IM-0045-0001.jpeg',\n",
              " 'NORMAL2-IM-0315-0001.jpeg',\n",
              " 'NORMAL2-IM-0123-0001.jpeg',\n",
              " 'NORMAL2-IM-0012-0001.jpeg',\n",
              " 'NORMAL2-IM-0112-0001.jpeg',\n",
              " 'IM-0077-0001.jpeg',\n",
              " 'NORMAL2-IM-0107-0001.jpeg',\n",
              " 'NORMAL2-IM-0252-0001.jpeg',\n",
              " 'NORMAL2-IM-0347-0001.jpeg',\n",
              " 'NORMAL2-IM-0132-0001.jpeg',\n",
              " 'NORMAL2-IM-0196-0001.jpeg',\n",
              " 'NORMAL2-IM-0310-0001.jpeg',\n",
              " 'NORMAL2-IM-0171-0001.jpeg',\n",
              " 'NORMAL2-IM-0232-0001.jpeg',\n",
              " 'NORMAL2-IM-0366-0001.jpeg',\n",
              " 'NORMAL2-IM-0316-0001.jpeg',\n",
              " 'NORMAL2-IM-0360-0001.jpeg',\n",
              " 'NORMAL2-IM-0095-0001.jpeg',\n",
              " 'NORMAL2-IM-0357-0001.jpeg',\n",
              " 'NORMAL2-IM-0238-0001.jpeg',\n",
              " 'NORMAL2-IM-0351-0001.jpeg',\n",
              " 'IM-0099-0001.jpeg',\n",
              " 'NORMAL2-IM-0305-0001.jpeg',\n",
              " 'NORMAL2-IM-0079-0001.jpeg',\n",
              " 'NORMAL2-IM-0111-0001.jpeg',\n",
              " 'NORMAL2-IM-0348-0001.jpeg',\n",
              " 'NORMAL2-IM-0321-0001.jpeg',\n",
              " 'NORMAL2-IM-0336-0001.jpeg',\n",
              " 'NORMAL2-IM-0274-0001.jpeg',\n",
              " 'IM-0041-0001.jpeg',\n",
              " 'IM-0033-0001-0001.jpeg',\n",
              " 'IM-0061-0001.jpeg',\n",
              " 'NORMAL2-IM-0335-0001.jpeg',\n",
              " 'NORMAL2-IM-0333-0001.jpeg',\n",
              " 'NORMAL2-IM-0276-0001.jpeg',\n",
              " 'NORMAL2-IM-0259-0001.jpeg',\n",
              " 'NORMAL2-IM-0339-0001.jpeg',\n",
              " 'NORMAL2-IM-0374-0001-0001.jpeg',\n",
              " 'NORMAL2-IM-0206-0001.jpeg',\n",
              " 'IM-0015-0001.jpeg',\n",
              " 'IM-0081-0001.jpeg',\n",
              " 'IM-0101-0001.jpeg',\n",
              " 'NORMAL2-IM-0288-0001.jpeg',\n",
              " 'IM-0011-0001-0002.jpeg',\n",
              " 'NORMAL2-IM-0283-0001.jpeg',\n",
              " 'NORMAL2-IM-0241-0001.jpeg',\n",
              " 'NORMAL2-IM-0302-0001.jpeg',\n",
              " 'NORMAL2-IM-0304-0001.jpeg',\n",
              " 'NORMAL2-IM-0300-0001.jpeg',\n",
              " 'IM-0065-0001.jpeg',\n",
              " 'IM-0006-0001.jpeg',\n",
              " 'IM-0085-0001.jpeg',\n",
              " 'NORMAL2-IM-0096-0001.jpeg',\n",
              " 'NORMAL2-IM-0286-0001.jpeg',\n",
              " 'NORMAL2-IM-0372-0001.jpeg',\n",
              " 'NORMAL2-IM-0246-0001-0002.jpeg',\n",
              " 'NORMAL2-IM-0222-0001.jpeg',\n",
              " 'IM-0070-0001.jpeg',\n",
              " 'IM-0010-0001.jpeg',\n",
              " 'IM-0033-0001.jpeg',\n",
              " 'NORMAL2-IM-0280-0001.jpeg',\n",
              " 'NORMAL2-IM-0331-0001.jpeg',\n",
              " 'NORMAL2-IM-0273-0001.jpeg',\n",
              " 'NORMAL2-IM-0374-0001-0002.jpeg',\n",
              " 'IM-0097-0001.jpeg',\n",
              " 'NORMAL2-IM-0275-0001.jpeg',\n",
              " 'IM-0091-0001.jpeg',\n",
              " 'NORMAL2-IM-0287-0001.jpeg',\n",
              " 'NORMAL2-IM-0303-0001.jpeg',\n",
              " 'NORMAL2-IM-0369-0001.jpeg',\n",
              " 'NORMAL2-IM-0086-0001.jpeg',\n",
              " 'IM-0023-0001.jpeg',\n",
              " 'NORMAL2-IM-0361-0001.jpeg',\n",
              " 'NORMAL2-IM-0281-0001.jpeg',\n",
              " 'IM-0059-0001.jpeg',\n",
              " 'IM-0086-0001.jpeg',\n",
              " 'NORMAL2-IM-0256-0001.jpeg',\n",
              " 'NORMAL2-IM-0297-0001.jpeg',\n",
              " 'NORMAL2-IM-0277-0001.jpeg',\n",
              " 'NORMAL2-IM-0332-0001.jpeg',\n",
              " 'NORMAL2-IM-0131-0001.jpeg',\n",
              " 'NORMAL2-IM-0028-0001.jpeg',\n",
              " 'NORMAL2-IM-0327-0001.jpeg',\n",
              " 'IM-0022-0001.jpeg',\n",
              " 'IM-0103-0001.jpeg',\n",
              " 'NORMAL2-IM-0267-0001.jpeg',\n",
              " 'NORMAL2-IM-0289-0001.jpeg',\n",
              " 'NORMAL2-IM-0072-0001.jpeg',\n",
              " 'NORMAL2-IM-0294-0001.jpeg',\n",
              " 'NORMAL2-IM-0337-0001.jpeg',\n",
              " 'IM-0109-0001.jpeg',\n",
              " 'NORMAL2-IM-0110-0001.jpeg',\n",
              " 'NORMAL2-IM-0278-0001.jpeg',\n",
              " 'NORMAL2-IM-0362-0001.jpeg',\n",
              " 'NORMAL2-IM-0319-0001.jpeg',\n",
              " 'NORMAL2-IM-0301-0001.jpeg',\n",
              " 'NORMAL2-IM-0350-0001.jpeg',\n",
              " 'NORMAL2-IM-0353-0001.jpeg',\n",
              " 'IM-0035-0001.jpeg',\n",
              " 'NORMAL2-IM-0330-0001.jpeg',\n",
              " 'NORMAL2-IM-0307-0001.jpeg',\n",
              " 'NORMAL2-IM-0033-0001.jpeg',\n",
              " 'NORMAL2-IM-0251-0001.jpeg',\n",
              " 'NORMAL2-IM-0013-0001.jpeg',\n",
              " 'NORMAL2-IM-0130-0001.jpeg',\n",
              " 'NORMAL2-IM-0092-0001.jpeg',\n",
              " 'NORMAL2-IM-0340-0001.jpeg',\n",
              " 'NORMAL2-IM-0237-0001.jpeg',\n",
              " 'NORMAL2-IM-0141-0001.jpeg',\n",
              " 'IM-0039-0001.jpeg',\n",
              " 'NORMAL2-IM-0052-0001.jpeg',\n",
              " 'NORMAL2-IM-0117-0001.jpeg',\n",
              " 'IM-0001-0001.jpeg',\n",
              " 'NORMAL2-IM-0029-0001.jpeg',\n",
              " 'NORMAL2-IM-0354-0001.jpeg',\n",
              " 'NORMAL2-IM-0150-0001.jpeg',\n",
              " 'NORMAL2-IM-0282-0001.jpeg',\n",
              " 'NORMAL2-IM-0380-0001.jpeg',\n",
              " 'IM-0029-0001.jpeg',\n",
              " 'IM-0049-0001.jpeg',\n",
              " 'NORMAL2-IM-0309-0001.jpeg',\n",
              " 'IM-0045-0001.jpeg',\n",
              " 'NORMAL2-IM-0233-0001.jpeg',\n",
              " 'IM-0033-0001-0002.jpeg',\n",
              " 'IM-0083-0001.jpeg',\n",
              " 'IM-0110-0001.jpeg',\n",
              " 'NORMAL2-IM-0007-0001.jpeg',\n",
              " 'NORMAL2-IM-0326-0001.jpeg',\n",
              " 'NORMAL2-IM-0173-0001-0001.jpeg',\n",
              " 'IM-0007-0001.jpeg',\n",
              " 'NORMAL2-IM-0027-0001.jpeg',\n",
              " 'NORMAL2-IM-0341-0001.jpeg',\n",
              " 'NORMAL2-IM-0349-0001.jpeg',\n",
              " 'NORMAL2-IM-0338-0001.jpeg',\n",
              " 'NORMAL2-IM-0317-0001.jpeg',\n",
              " 'IM-0084-0001.jpeg',\n",
              " 'IM-0111-0001.jpeg',\n",
              " 'NORMAL2-IM-0023-0001.jpeg',\n",
              " 'NORMAL2-IM-0329-0001.jpeg',\n",
              " 'NORMAL2-IM-0345-0001.jpeg',\n",
              " 'IM-0003-0001.jpeg',\n",
              " 'IM-0011-0001.jpeg',\n",
              " 'IM-0079-0001.jpeg',\n",
              " 'NORMAL2-IM-0217-0001.jpeg',\n",
              " 'IM-0063-0001.jpeg',\n",
              " 'NORMAL2-IM-0201-0001.jpeg',\n",
              " 'NORMAL2-IM-0019-0001.jpeg',\n",
              " 'IM-0087-0001.jpeg',\n",
              " 'NORMAL2-IM-0346-0001.jpeg',\n",
              " 'NORMAL2-IM-0041-0001.jpeg',\n",
              " 'NORMAL2-IM-0102-0001.jpeg',\n",
              " 'NORMAL2-IM-0373-0001.jpeg',\n",
              " 'NORMAL2-IM-0378-0001.jpeg',\n",
              " 'NORMAL2-IM-0272-0001.jpeg',\n",
              " 'IM-0050-0001.jpeg',\n",
              " 'NORMAL2-IM-0198-0001.jpeg',\n",
              " 'IM-0069-0001.jpeg',\n",
              " 'NORMAL2-IM-0098-0001.jpeg',\n",
              " 'NORMAL2-IM-0146-0001.jpeg',\n",
              " 'IM-0095-0001.jpeg',\n",
              " 'NORMAL2-IM-0328-0001.jpeg',\n",
              " 'NORMAL2-IM-0323-0001.jpeg',\n",
              " 'NORMAL2-IM-0290-0001.jpeg',\n",
              " 'NORMAL2-IM-0246-0001.jpeg',\n",
              " 'IM-0043-0001.jpeg',\n",
              " 'NORMAL2-IM-0325-0001.jpeg',\n",
              " 'IM-0009-0001.jpeg',\n",
              " 'NORMAL2-IM-0219-0001.jpeg',\n",
              " 'NORMAL2-IM-0207-0001.jpeg',\n",
              " 'IM-0016-0001.jpeg']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "a = os.listdir(os.path.join(base_dir,'positive'))\n",
        "random.shuffle(a)\n",
        "a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35bd179b",
      "metadata": {
        "id": "35bd179b"
      },
      "source": [
        "#### Divide images into 3 parts Training,Test,Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d1d8aa1",
      "metadata": {
        "id": "8d1d8aa1"
      },
      "outputs": [],
      "source": [
        "positive_images = os.listdir(positive)\n",
        "random.shuffle(positive_images)\n",
        "negative_images = os.listdir(negative)\n",
        "random.shuffle(negative_images)\n",
        "train_pos_images = positive_images[:150]\n",
        "train_neg_images = negative_images[:150]\n",
        "validation_pos_images = positive_images[150:190]\n",
        "validation_neg_images = negative_images[150:230]\n",
        "test_pos_images = positive_images[190:]\n",
        "test_neg_images = negative_images[230:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "034b1652",
      "metadata": {
        "id": "034b1652"
      },
      "source": [
        "#### Copying each part into Their respective folded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd9dba89",
      "metadata": {
        "id": "dd9dba89"
      },
      "outputs": [],
      "source": [
        "for img in train_pos_images:\n",
        "    src = os.path.join(positive,img)\n",
        "    dst = os.path.join(train_pos_dir,img)\n",
        "    shutil.copy(src,dst)\n",
        "\n",
        "for img in train_neg_images:\n",
        "    src = os.path.join(negative,img)\n",
        "    dst = os.path.join(train_neg_dir,img)\n",
        "    shutil.copy(src,dst)\n",
        "\n",
        "for img in validation_pos_images:\n",
        "    src = os.path.join(positive,img)\n",
        "    dst = os.path.join(validation_pos_dir,img)\n",
        "    shutil.copy(src,dst)\n",
        "\n",
        "for img in validation_neg_images:\n",
        "    src = os.path.join(negative,img)\n",
        "    dst = os.path.join(validation_neg_dir,img)\n",
        "    shutil.copy(src,dst)\n",
        "\n",
        "for img in test_pos_images:\n",
        "    src = os.path.join(positive,img)\n",
        "    dst = os.path.join(test_pos_dir,img)\n",
        "    shutil.copy(src,dst)\n",
        "\n",
        "for img in test_neg_images:\n",
        "    src = os.path.join(negative,img)\n",
        "    dst = os.path.join(test_neg_dir,img)\n",
        "    shutil.copy(src,dst)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "754afb36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "754afb36",
        "outputId": "af0c52ce-a50a-42f8-a041-514a5ff310c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44\n",
            "70\n"
          ]
        }
      ],
      "source": [
        "print(len(test_pos_images))\n",
        "print(len(test_neg_images))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d894e0d6",
      "metadata": {
        "id": "d894e0d6"
      },
      "source": [
        "#### Making Generator By using inbuilt Tensorflow Function for applying data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b33f2f6",
      "metadata": {
        "id": "6b33f2f6"
      },
      "outputs": [],
      "source": [
        "train_data_gen = ImageDataGenerator(rescale=1./255,brightness_range=(0.2,0.6),vertical_flip=True,horizontal_flip=True)\n",
        "test_data_gen = ImageDataGenerator(rescale=1./255)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd7bafd9",
      "metadata": {
        "id": "bd7bafd9"
      },
      "source": [
        "#### passing validaton images and train images into generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cabc85cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cabc85cf",
        "outputId": "5902e37a-8a41-4fff-a264-bb4471a5e928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 300 images belonging to 2 classes.\n",
            "Found 120 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_gen = train_data_gen.flow_from_directory(train_dir,target_size=(150,150),class_mode='binary',batch_size=32)\n",
        "val_gen = test_data_gen.flow_from_directory(validation_dir,target_size=(150,150),class_mode='binary',batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "338677df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "338677df",
        "outputId": "c7579d54-4185-416c-e24a-3007514b4567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 150, 150, 3)\n",
            "(32,)\n"
          ]
        }
      ],
      "source": [
        "for data,label in train_gen:\n",
        "    print(data.shape)\n",
        "    print(label.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4549b7",
      "metadata": {
        "id": "1c4549b7"
      },
      "source": [
        "#### Making Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ef92bbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ef92bbe",
        "outputId": "d58d41aa-9433-4461-91ea-470d92776b99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 64)      1792      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 148, 148, 64)      0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 74, 74, 64)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 350464)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                11214880  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11216705 (42.79 MB)\n",
            "Trainable params: 11216705 (42.79 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64,(3,3),activation='relu',padding=\"same\",input_shape=(150,150,3)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "\n",
        "# model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
        "# model.add(layers.MaxPool2D((2,2)))\n",
        "# model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(32,activation='relu'))\n",
        "model.add(layers.Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4216bded",
      "metadata": {
        "id": "4216bded"
      },
      "source": [
        "#### Training or Fitting and Validating model by using training and validation images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dd1f6b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dd1f6b0",
        "outputId": "d761424c-0f7e-43ab-c646-4bb8932f994c",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-49aa7fca2cf5>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(train_gen,steps_per_epoch=10,epochs=10,validation_data=val_gen,validation_steps=4)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 42s 4s/step - loss: 2.7088 - accuracy: 0.4967 - val_loss: 0.7124 - val_accuracy: 0.3583\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 22s 2s/step - loss: 0.6969 - accuracy: 0.5333 - val_loss: 0.7102 - val_accuracy: 0.3583\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 26s 3s/step - loss: 0.6863 - accuracy: 0.5500 - val_loss: 0.6695 - val_accuracy: 0.4250\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 22s 2s/step - loss: 0.7058 - accuracy: 0.7167 - val_loss: 1.6420 - val_accuracy: 0.3333\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 21s 2s/step - loss: 0.6739 - accuracy: 0.6233 - val_loss: 0.6782 - val_accuracy: 0.4250\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 23s 2s/step - loss: 0.7546 - accuracy: 0.6433 - val_loss: 0.8255 - val_accuracy: 0.3667\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 21s 2s/step - loss: 0.6481 - accuracy: 0.7100 - val_loss: 0.4480 - val_accuracy: 0.8167\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 22s 2s/step - loss: 0.5476 - accuracy: 0.7333 - val_loss: 1.7038 - val_accuracy: 0.3417\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 26s 2s/step - loss: 0.5591 - accuracy: 0.7367 - val_loss: 0.4337 - val_accuracy: 0.7833\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 25s 2s/step - loss: 0.4741 - accuracy: 0.7967 - val_loss: 0.4512 - val_accuracy: 0.7583\n"
          ]
        }
      ],
      "source": [
        "history = model.fit_generator(train_gen,steps_per_epoch=10,epochs=10,validation_data=val_gen,validation_steps=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l7unOSE5Ji10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7unOSE5Ji10",
        "outputId": "1aaa22ec-4af3-46b2-d333-469b1df10d84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('/content/drive/MyDrive/lungs_cancer_classification.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9dc334c",
      "metadata": {
        "id": "e9dc334c"
      },
      "source": [
        "#### passing Test Images Into Generator for evalution of model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac85795f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac85795f",
        "outputId": "c5a2cd48-07a5-4ee4-ba97-84c7df532a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 114 images belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-4e5984ae3a61>:2: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  test = model.evaluate_generator(val_gen)\n"
          ]
        }
      ],
      "source": [
        "val_gen = test_data_gen.flow_from_directory(test_dir,target_size=(150,150),class_mode='binary',batch_size=32)\n",
        "test = model.evaluate_generator(val_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6372d99e",
      "metadata": {
        "id": "6372d99e"
      },
      "source": [
        "####  Printing Test Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88f014fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88f014fd",
        "outputId": "eb976a93-d15d-472f-998b-0cfa4462afb7",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.47960278391838074, 0.7719298005104065]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3277b8ae",
      "metadata": {
        "id": "3277b8ae"
      },
      "source": [
        "#### ======================================================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47575e32",
      "metadata": {
        "id": "47575e32"
      },
      "source": [
        "#### Making Model by using Feature Extraction fron pretrained convent (VGG16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "839c626b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "839c626b",
        "outputId": "c335c63b-2674-4b74-8e1d-2f00b3b09eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 14714688 (56.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "conv = VGG16(include_top=False,input_shape=(150,150,3))\n",
        "conv.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "606cd692",
      "metadata": {
        "id": "606cd692"
      },
      "source": [
        "#### Featture Extraction Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34eabea8",
      "metadata": {
        "id": "34eabea8"
      },
      "outputs": [],
      "source": [
        "def feature_extraction(directory,sample_count,vertical_flip=False,horizontal_flip=False,zoom_range=0.0):\n",
        "    features = np.zeros(shape=(sample_count,4,4,512))\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    gen = ImageDataGenerator(rescale=1./255,vertical_flip=vertical_flip,horizontal_flip=horizontal_flip,zoom_range=zoom_range)\n",
        "    data_gen = gen.flow_from_directory(directory,target_size=(150,150),class_mode='binary',batch_size=30)\n",
        "    i=0\n",
        "    for img_batch,label_batch in data_gen:\n",
        "        prediction = conv.predict(img_batch)\n",
        "        features[i*30 : (i+1)*30] = prediction\n",
        "        labels[i*30:(i+1)*30] = label_batch\n",
        "        i+=1\n",
        "        if i*30 >= sample_count:\n",
        "            return features,labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0da9a705",
      "metadata": {
        "id": "0da9a705"
      },
      "source": [
        "#### passing train images and validation images to Feature Extraction method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0802dd11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0802dd11",
        "outputId": "a0c6a3e9-59a8-4ab1-a5f1-17f896f1f85c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 300 images belonging to 2 classes.\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "Found 120 images belonging to 2 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 6s 6s/step\n"
          ]
        }
      ],
      "source": [
        "train_features = feature_extraction(train_dir,300,vertical_flip=True,horizontal_flip=True,zoom_range=(0.2,0.5))\n",
        "validation_features = feature_extraction(validation_dir,120)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d53bb588",
      "metadata": {
        "id": "d53bb588"
      },
      "source": [
        "#### making model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d06563d3",
      "metadata": {
        "id": "d06563d3"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "# model2.add(layers.MaxPool2D((2,2)))\n",
        "# model2.add(layers.Flatten())\n",
        "# model2.add(layers.Dropout(0.5))\n",
        "# model2.add(layers.Dense(32,activation='relu'))\n",
        "# # model2.add(layers.Dropout(0.5))\n",
        "# model2.add(layers.Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.add(layers.MaxPool2D((2,2)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(32,activation='relu'))\n",
        "model.add(layers.Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.compile(optimizer=optimizers.RMSprop(learning_rate=1e-5),loss='binary_crossentropy',metrics=['acc'])\n",
        "# model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a89380f",
      "metadata": {
        "id": "8a89380f"
      },
      "source": [
        "#### Training Or Fitting The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1a42b6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1a42b6d",
        "outputId": "6c248fa0-b146-41f5-ac6f-a667ab1d697b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(300, 4, 4, 512)\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 1s 30ms/step - loss: 0.7003 - acc: 0.5633 - val_loss: 0.6921 - val_acc: 0.5167\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.6986 - acc: 0.5567 - val_loss: 0.6959 - val_acc: 0.4917\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 0.7090 - acc: 0.5033 - val_loss: 0.6987 - val_acc: 0.4917\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.6966 - acc: 0.5433 - val_loss: 0.7018 - val_acc: 0.4833\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.7047 - acc: 0.5133 - val_loss: 0.7051 - val_acc: 0.4667\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.6994 - acc: 0.5133 - val_loss: 0.7078 - val_acc: 0.4583\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.6977 - acc: 0.5500 - val_loss: 0.7117 - val_acc: 0.4667\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.6826 - acc: 0.5833 - val_loss: 0.7134 - val_acc: 0.4583\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.6782 - acc: 0.5767 - val_loss: 0.7156 - val_acc: 0.4333\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.6961 - acc: 0.5433 - val_loss: 0.7191 - val_acc: 0.4250\n"
          ]
        }
      ],
      "source": [
        "print((train_features[0]).shape)\n",
        "history = model.fit(train_features[0],train_features[1],epochs=10,batch_size=32,validation_data=validation_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84cbc766",
      "metadata": {
        "id": "84cbc766"
      },
      "source": [
        "#### ======================================================================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18d982a3",
      "metadata": {
        "id": "18d982a3"
      },
      "source": [
        "#### Making Model Architecture By using convent as an layer but freeze convent from training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b0e7007",
      "metadata": {
        "id": "3b0e7007"
      },
      "outputs": [],
      "source": [
        "conv.trainable = False\n",
        "model = models.Sequential()\n",
        "model.add(conv)\n",
        "model.add(layers.MaxPool2D((2,2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(32,activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer=optimizers.RMSprop(learning_rate=2e-5),loss='binary_crossentropy',metrics=['acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7748e434",
      "metadata": {
        "id": "7748e434"
      },
      "source": [
        "#### Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1297df4",
      "metadata": {
        "id": "c1297df4"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c806f190",
      "metadata": {
        "id": "c806f190"
      },
      "source": [
        "####  Training or Fitting Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "775a30e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "775a30e7",
        "outputId": "a2d16ca9-d1f6-4213-d8f6-68c415c2550c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "10/10 [==============================] - 127s 13s/step - loss: 0.8699 - acc: 0.5067 - val_loss: 1.0378 - val_acc: 0.3860\n",
            "Epoch 2/5\n",
            "10/10 [==============================] - 116s 12s/step - loss: 0.8088 - acc: 0.5067 - val_loss: 0.9509 - val_acc: 0.3860\n",
            "Epoch 3/5\n",
            "10/10 [==============================] - 126s 13s/step - loss: 0.8042 - acc: 0.4967 - val_loss: 0.8845 - val_acc: 0.3860\n",
            "Epoch 4/5\n",
            "10/10 [==============================] - 116s 12s/step - loss: 0.7882 - acc: 0.5067 - val_loss: 0.8343 - val_acc: 0.3860\n",
            "Epoch 5/5\n",
            "10/10 [==============================] - 128s 13s/step - loss: 0.7489 - acc: 0.5233 - val_loss: 0.7988 - val_acc: 0.3947\n"
          ]
        }
      ],
      "source": [
        "# model.fit(train_gen,steps_per_epoch=10,epochs=5,validation_data=val_gen,validation_steps=5)\n",
        "history = model.fit(train_gen,epochs=5,steps_per_epoch=10,validation_data=val_gen,validation_steps=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ffa3cb5",
      "metadata": {
        "id": "3ffa3cb5"
      },
      "source": [
        "#### =================================================Fine Tuning==================================="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c10d3a",
      "metadata": {
        "id": "25c10d3a"
      },
      "source": [
        "#### Freeze convent From learning except few lower layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5935b0e7",
      "metadata": {
        "id": "5935b0e7"
      },
      "outputs": [],
      "source": [
        "conv.trainable = True\n",
        "set_trainable = False\n",
        "for layer in conv.layers:\n",
        "    if layer.name == 'block5':\n",
        "        set_trainableinable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f48bb439",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f48bb439",
        "outputId": "f6688bd5-893c-4917-aa3a-67ee0378d722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "conv.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53366d70",
      "metadata": {
        "id": "53366d70"
      },
      "source": [
        "#### Making Architeture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36df599d",
      "metadata": {
        "id": "36df599d"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(conv)\n",
        "model.add(layers.MaxPool2D((2,2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(32,activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(learning_rate=2e-5),loss='binary_crossentropy',metrics=['acc'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f976d1a5",
      "metadata": {
        "id": "f976d1a5"
      },
      "source": [
        "#### Summary Of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "013c2914",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "013c2914",
        "outputId": "7189c944-a9f4-4850-df17-1a78b49bca81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 2, 2, 512)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 32)                65568     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14780289 (56.38 MB)\n",
            "Trainable params: 65601 (256.25 KB)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8762b722",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8762b722",
        "outputId": "2be9287e-e5a0-4dd3-c1b0-3e1f375a8880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 129s 13s/step - loss: 0.7138 - acc: 0.5000 - val_loss: 0.6805 - val_acc: 0.5351\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 127s 13s/step - loss: 0.7217 - acc: 0.4800 - val_loss: 0.6774 - val_acc: 0.5614\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 128s 13s/step - loss: 0.7057 - acc: 0.5400 - val_loss: 0.6772 - val_acc: 0.5614\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 117s 12s/step - loss: 0.7051 - acc: 0.5200 - val_loss: 0.6734 - val_acc: 0.6053\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 127s 13s/step - loss: 0.6933 - acc: 0.5567 - val_loss: 0.6801 - val_acc: 0.5175\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 118s 12s/step - loss: 0.7010 - acc: 0.5333 - val_loss: 0.6774 - val_acc: 0.5351\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 128s 13s/step - loss: 0.7122 - acc: 0.5333 - val_loss: 0.6746 - val_acc: 0.5702\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 127s 13s/step - loss: 0.6615 - acc: 0.6100 - val_loss: 0.6742 - val_acc: 0.5789\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 117s 12s/step - loss: 0.7082 - acc: 0.5033 - val_loss: 0.6713 - val_acc: 0.5965\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 118s 12s/step - loss: 0.7057 - acc: 0.5100 - val_loss: 0.6722 - val_acc: 0.5877\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_gen,epochs=10,steps_per_epoch=10,validation_data=val_gen,validation_steps=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60fcdf5",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d60fcdf5",
        "outputId": "379a7ebb-fd08-48a9-9872-a91ae0bbb941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 300 images belonging to 2 classes.\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "Found 120 images belonging to 2 classes.\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "1/1 [==============================] - 7s 7s/step\n"
          ]
        }
      ],
      "source": [
        "train_features,train_labels = feature_extraction(train_dir,300,vertical_flip=True,horizontal_flip=True,zoom_range=(0.2,0.5))\n",
        "validation_features,validation_labels = feature_extraction(validation_dir,120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c356f7b",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4c356f7b",
        "outputId": "777d5846-799f-4544-acaa-fa9d95a8424b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(420,)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-c3357b69c5a8>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconcatenated_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcatenated_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (9,) (90,) "
          ]
        }
      ],
      "source": [
        "a = np.array(train_labels)\n",
        "b = np.array(validation_labels)\n",
        "concatenated_array = np.concatenate((a, b), axis=0)\n",
        "print(concatenated_array.shape)\n",
        "a[1:10] + a[10:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fc7fe1e",
      "metadata": {
        "id": "7fc7fe1e"
      },
      "source": [
        "####  ============================= Applying K-fold Technique ==============================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df6ef1cd",
      "metadata": {
        "id": "df6ef1cd"
      },
      "outputs": [],
      "source": [
        "def feature_extraction(directory,sample_count,vertical_flip=False,horizontal_flip=False,zoom_range=0.0):\n",
        "    features = np.zeros(shape=(sample_count,4,4,512))\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    gen = ImageDataGenerator(rescale=1./255,vertical_flip=vertical_flip,horizontal_flip=horizontal_flip,zoom_range=zoom_range)\n",
        "    data_gen = gen.flow_from_directory(directory,target_size=(150,150),class_mode='binary',batch_size=30)\n",
        "    i=0\n",
        "    for img_batch,label_batch in data_gen:\n",
        "        prediction = conv.predict(img_batch)\n",
        "        features[i*30 : (i+1)*30] = prediction\n",
        "        labels[i*30:(i+1)*30] = label_batch\n",
        "        i+=1\n",
        "        if i*30 >= sample_count:\n",
        "            return features,labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv = VGG16(include_top=False,input_shape=(150,150,3))\n",
        "conv.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIQgxWdTG7Mm",
        "outputId": "be5d1c3a-87f1-4c26-f5bc-9c2df7f456f1"
      },
      "id": "MIQgxWdTG7Mm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 14714688 (56.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "899b9439",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "899b9439",
        "outputId": "84d7851e-a24f-4054-c25c-1b84ea405d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 300 images belonging to 2 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Found 120 images belonging to 2 classes.\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "(420, 4, 4, 512)\n"
          ]
        }
      ],
      "source": [
        "train_features,train_labels = feature_extraction(train_dir,300,vertical_flip=True,horizontal_flip=True,zoom_range=(0.2,0.5))\n",
        "validation_features,validation_labels = feature_extraction(validation_dir,120)\n",
        "train_features = np.concatenate((train_features,validation_features), axis=0)\n",
        "train_labels =np.concatenate((train_labels,validation_labels),axis=0)\n",
        "\n",
        "print((train_features.shape))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd92151",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddd92151",
        "outputId": "6a051d55-fdfb-4799-baef-5db7b9447a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 4, 512)\n"
          ]
        }
      ],
      "source": [
        "print((train_features[0].shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7422027",
      "metadata": {
        "id": "a7422027"
      },
      "source": [
        "#### Applying Feature Extraction Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d926582f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d926582f",
        "outputId": "736d65d5-6341-4e13-d8be-abe07ebea94f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1.\n",
            " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0.\n",
            " 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 1. 1.\n",
            " 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1.\n",
            " 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0.\n",
            " 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1.\n",
            " 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0.\n",
            " 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0.\n",
            " 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
            " 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
            " 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1.\n",
            " 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0.\n",
            " 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "420"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "print((np.reshape(train_labels,(420))))\n",
        "# print((validation_labels))\n",
        "len(train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33de7b1b",
      "metadata": {
        "id": "33de7b1b"
      },
      "source": [
        "####  Making build Model Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7e979b9",
      "metadata": {
        "id": "a7e979b9"
      },
      "outputs": [],
      "source": [
        "def build_model(input_shape):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Flatten(input_shape=input_shape))  # Adjust input shape here\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=optimizers.RMSprop(learning_rate=1e-5), loss='binary_crossentropy', metrics=['acc'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c307a722",
      "metadata": {
        "id": "c307a722"
      },
      "source": [
        "#### applying K- Fold Technique By using above function In which Number of Folds is 5 also training the model and Number Of Ecpochs 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc006103",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc006103",
        "outputId": "0d9c28f4-2339-460e-e24b-610427febac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold #  0\n",
            "(336,)\n",
            "(336, 4, 4, 512)\n",
            "(84, 4, 4, 512)\n",
            "(84,)\n",
            "Epoch 1/10\n",
            "11/11 [==============================] - 3s 5ms/step - loss: 0.7603 - acc: 0.4732\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.7158 - acc: 0.5417\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7081 - acc: 0.5565\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6853 - acc: 0.5982\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.7007 - acc: 0.5625\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7138 - acc: 0.5417\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6838 - acc: 0.5685\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.6788 - acc: 0.6042\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7142 - acc: 0.5446\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.6870 - acc: 0.5685\n",
            "fold #  1\n",
            "(336,)\n",
            "(336, 4, 4, 512)\n",
            "(84, 4, 4, 512)\n",
            "(84,)\n",
            "Epoch 1/10\n",
            "11/11 [==============================] - 1s 4ms/step - loss: 0.7467 - acc: 0.5446\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.7264 - acc: 0.5506\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7454 - acc: 0.5119\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.7332 - acc: 0.5208\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.7375 - acc: 0.5000\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6791 - acc: 0.5863\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6944 - acc: 0.5685\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7000 - acc: 0.5595\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6970 - acc: 0.5476\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6839 - acc: 0.5863\n",
            "fold #  2\n",
            "(336,)\n",
            "(336, 4, 4, 512)\n",
            "(84, 4, 4, 512)\n",
            "(84,)\n",
            "Epoch 1/10\n",
            "11/11 [==============================] - 1s 4ms/step - loss: 0.7654 - acc: 0.5149\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.7218 - acc: 0.5357\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.6950 - acc: 0.5833\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.7050 - acc: 0.5357\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7007 - acc: 0.5536\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.6933 - acc: 0.5655\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.7218 - acc: 0.5625\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7192 - acc: 0.5357\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6916 - acc: 0.5565\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.7011 - acc: 0.5357\n",
            "fold #  3\n",
            "(336,)\n",
            "(336, 4, 4, 512)\n",
            "(84, 4, 4, 512)\n",
            "(84,)\n",
            "Epoch 1/10\n",
            "11/11 [==============================] - 1s 4ms/step - loss: 0.7335 - acc: 0.5357\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.7633 - acc: 0.5030\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7369 - acc: 0.5030\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7348 - acc: 0.5149\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7408 - acc: 0.5119\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7432 - acc: 0.4702\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7337 - acc: 0.5000\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7308 - acc: 0.4881\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7641 - acc: 0.4970\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.6970 - acc: 0.5625\n",
            "fold #  4\n",
            "(336,)\n",
            "(336, 4, 4, 512)\n",
            "(84, 4, 4, 512)\n",
            "(84,)\n",
            "Epoch 1/10\n",
            "11/11 [==============================] - 1s 4ms/step - loss: 0.7220 - acc: 0.5149\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7047 - acc: 0.5357\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7102 - acc: 0.5060\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.7153 - acc: 0.5149\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7151 - acc: 0.5268\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7271 - acc: 0.5119\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.7088 - acc: 0.5268\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.6865 - acc: 0.5714\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.6970 - acc: 0.5446\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.6972 - acc: 0.5298\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "folds = 5\n",
        "sample_per_fold = len(train_labels)//folds\n",
        "validation_score = []\n",
        "for fold in range(folds) :\n",
        "    print('fold # ', fold)\n",
        "    validatoin_data = train_features[fold * sample_per_fold : (fold+1)*sample_per_fold]\n",
        "    validation_label = train_labels[fold*sample_per_fold : (fold+1)*sample_per_fold]\n",
        "    train_data = np.concatenate([train_features[:fold * sample_per_fold],train_features[(fold + 1) * sample_per_fold:]],axis=0)\n",
        "    train_label = np.concatenate([train_labels[:fold * sample_per_fold],train_labels[(fold + 1) * sample_per_fold:]],axis=0)\n",
        "    input_shape = (4, 4, 512)\n",
        "    model = build_model(input_shape)\n",
        "    history = model.fit(train_data, train_label,epochs=10, batch_size=32)\n",
        "    validation_score.append(history.history['val_acc'])\n",
        "\n",
        "\n",
        "\n",
        "print(validation_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dc36d32",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3dc36d32"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3e011304",
      "metadata": {
        "id": "3e011304"
      },
      "source": [
        "#### Applying K- Fold Method With number of epochs 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38266622",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38266622",
        "outputId": "31e2a567-fa72-487e-afbf-47f245074fa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing fold # 0\n",
            "processing fold # 1\n",
            "processing fold # 2\n",
            "processing fold # 3\n",
            "processing fold # 4\n"
          ]
        }
      ],
      "source": [
        "folds=5\n",
        "num_val_samples = len(train_labels)//folds\n",
        "num_epochs = 500\n",
        "all_mae_histories = []\n",
        "for i in range(folds):\n",
        "    print('processing fold #', i)\n",
        "    val_data = train_features[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_labels[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    partial_train_data = np.concatenate([train_features[:i * num_val_samples],train_features[(i + 1) * num_val_samples:]],axis=0)\n",
        "    partial_train_targets = np.concatenate([train_labels[:i * num_val_samples],train_labels[(i + 1) * num_val_samples:]],axis=0)\n",
        "    input_shape = (4, 4, 512)\n",
        "    model = build_model(input_shape)\n",
        "    history = model.fit(train_features,train_labels,epochs=num_epochs, batch_size=32,validation_data=(val_data,val_targets),verbose=0)\n",
        "    val_history = history.history['val_acc']\n",
        "    validation_score.append(val_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "681c787f",
      "metadata": {
        "id": "681c787f"
      },
      "source": [
        "#### Checking Validation Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a4c59d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95a4c59d",
        "outputId": "e2ec0256-c647-45fa-d539-b64f57133f7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.511904776096344,\n",
              "  0.5714285969734192,\n",
              "  0.5833333134651184,\n",
              "  0.511904776096344,\n",
              "  0.5,\n",
              "  0.5357142686843872,\n",
              "  0.5595238208770752,\n",
              "  0.6547619104385376,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.7023809552192688,\n",
              "  0.726190447807312,\n",
              "  0.6904761791229248,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.726190447807312,\n",
              "  0.7023809552192688,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.738095223903656,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.7023809552192688,\n",
              "  0.75,\n",
              "  0.7142857313156128,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7023809552192688,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.75,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.738095223903656,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.75,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.8095238208770752,\n",
              "  0.7857142686843872,\n",
              "  0.75,\n",
              "  0.773809552192688,\n",
              "  0.8095238208770752,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.8095238208770752,\n",
              "  0.7857142686843872,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.761904776096344,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8095238208770752,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.7976190447807312,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8690476417541504,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8690476417541504,\n",
              "  0.8333333134651184,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8333333134651184,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8452380895614624,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496],\n",
              " [0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.488095223903656,\n",
              "  0.5,\n",
              "  0.523809552192688,\n",
              "  0.5714285969734192,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7976190447807312,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7976190447807312,\n",
              "  0.8214285969734192,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.773809552192688,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8452380895614624,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8452380895614624,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8690476417541504,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8809523582458496,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8452380895614624,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8333333134651184,\n",
              "  0.8690476417541504,\n",
              "  0.8928571343421936,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8452380895614624,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8571428656578064,\n",
              "  0.8928571343421936,\n",
              "  0.8571428656578064,\n",
              "  0.8928571343421936,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9166666865348816,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9166666865348816,\n",
              "  0.9047619104385376,\n",
              "  0.9166666865348816,\n",
              "  0.8928571343421936],\n",
              " [0.5476190447807312,\n",
              "  0.6309523582458496,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6666666865348816,\n",
              "  0.6904761791229248,\n",
              "  0.6547619104385376,\n",
              "  0.6666666865348816,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.6428571343421936,\n",
              "  0.6785714030265808,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6666666865348816,\n",
              "  0.7023809552192688,\n",
              "  0.6666666865348816,\n",
              "  0.7023809552192688,\n",
              "  0.6785714030265808,\n",
              "  0.7023809552192688,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6666666865348816,\n",
              "  0.6547619104385376,\n",
              "  0.6785714030265808,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6428571343421936,\n",
              "  0.6666666865348816,\n",
              "  0.6547619104385376,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6547619104385376,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6785714030265808,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6547619104385376,\n",
              "  0.6428571343421936,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6428571343421936,\n",
              "  0.6547619104385376,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6666666865348816,\n",
              "  0.6547619104385376,\n",
              "  0.6785714030265808,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6666666865348816,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.6785714030265808,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6785714030265808,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.738095223903656,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.726190447807312,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.726190447807312,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.726190447807312,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.7976190447807312,\n",
              "  0.761904776096344,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752],\n",
              " [0.5595238208770752,\n",
              "  0.5357142686843872,\n",
              "  0.5595238208770752,\n",
              "  0.5833333134651184,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6547619104385376,\n",
              "  0.6428571343421936,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.7142857313156128,\n",
              "  0.6547619104385376,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.773809552192688,\n",
              "  0.738095223903656,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8452380895614624,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8214285969734192,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8690476417541504,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496],\n",
              " [0.6071428656578064,\n",
              "  0.6666666865348816,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6666666865348816,\n",
              "  0.6547619104385376,\n",
              "  0.6666666865348816,\n",
              "  0.6547619104385376,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.726190447807312,\n",
              "  0.738095223903656,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7857142686843872,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9166666865348816,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9285714030265808,\n",
              "  0.9047619104385376,\n",
              "  0.9166666865348816,\n",
              "  0.9047619104385376,\n",
              "  0.9285714030265808,\n",
              "  0.9047619104385376,\n",
              "  0.9285714030265808,\n",
              "  0.9047619104385376,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9285714030265808,\n",
              "  0.9285714030265808,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9285714030265808,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9285714030265808,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9285714030265808,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9404761791229248,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9642857313156128,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9642857313156128,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9642857313156128,\n",
              "  0.9642857313156128,\n",
              "  0.988095223903656,\n",
              "  0.9642857313156128,\n",
              "  0.9523809552192688,\n",
              "  0.9642857313156128,\n",
              "  0.9642857313156128,\n",
              "  0.976190447807312,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.9642857313156128,\n",
              "  0.9642857313156128,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.976190447807312,\n",
              "  0.9642857313156128,\n",
              "  0.988095223903656,\n",
              "  0.9642857313156128,\n",
              "  0.9642857313156128,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.976190447807312,\n",
              "  0.988095223903656,\n",
              "  0.9642857313156128,\n",
              "  0.976190447807312,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "validation_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de2648e",
      "metadata": {
        "id": "6de2648e"
      },
      "source": [
        "#### Finding Mean Validation Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6a01fb3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6a01fb3",
        "scrolled": true,
        "outputId": "a23a8496-2cd4-4815-e986-a31086054f44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8233619040489196"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "np.mean(validation_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "096325c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "096325c9",
        "outputId": "3beaabf4-11a3-47c3-85bc-d96506bb79e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "len(validation_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "639831c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "639831c2",
        "outputId": "9f6b9930-1048-4ebc-c0c5-abd1cbbb154d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 114 images belonging to 2 classes.\n",
            "(32, 150, 150, 3)\n",
            "(32,)\n"
          ]
        }
      ],
      "source": [
        "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "test_gen = test_data_gen.flow_from_directory(test_dir,target_size=(150,150),class_mode='binary',batch_size=32)\n",
        "for data,label in test_gen:\n",
        "    print(data.shape)\n",
        "    print(label.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69d1513a",
      "metadata": {
        "id": "69d1513a"
      },
      "source": [
        "#### Evaluate Model on test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cdf4853",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cdf4853",
        "outputId": "0b6875c7-524b-4888-be02-512ca0d1816e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 114 images belonging to 2 classes.\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2774 - acc: 0.8509\n"
          ]
        }
      ],
      "source": [
        "test_feature,test_label = feature_extraction(test_dir,114)\n",
        "test = model.evaluate(test_feature,test_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f29281c0",
      "metadata": {
        "id": "f29281c0"
      },
      "source": [
        "#### Test Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "202ee414",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "202ee414",
        "outputId": "f5d9aef0-2889-491b-9e56-8e0ed0890f77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.277378648519516, 0.8508771657943726]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17fad29d",
      "metadata": {
        "id": "17fad29d"
      },
      "source": [
        "####  checking mean As per Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e201d0db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e201d0db",
        "scrolled": false,
        "outputId": "0e1c461c-0863-45e2-f178-055f65ce9092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "500\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "(5, 500)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5452381014823914"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "print(len(validation_score))\n",
        "print(len(validation_score[0]))\n",
        "for i in range(num_epochs):\n",
        "    a = []\n",
        "    for x in validation_score:\n",
        "#         print(x)\n",
        "        a.append(x[i])\n",
        "        print(i)\n",
        "#     print(a)\n",
        "    m = np.mean(a)\n",
        "    break\n",
        "# print(m)\n",
        "\n",
        "\n",
        "val = np.array(validation_score)\n",
        "print(val.shape)\n",
        "mean = np.mean(val,axis=0)\n",
        "mean.shape\n",
        "mean[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2418b19",
      "metadata": {
        "id": "d2418b19"
      },
      "outputs": [],
      "source": [
        "average_val_history = np.mean(val,axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12cb9ab4",
      "metadata": {
        "id": "12cb9ab4"
      },
      "source": [
        "####  Taking mean As per Epochs and Visualize The result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31d5bc30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "31d5bc30",
        "outputId": "dad7501a-9b14-4879-a0f5-e77bc6c585eb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "average_val_history = [np.mean([x[i] for x in validation_score]) for i in range(num_epochs)]\n",
        "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1, len(average_mae_history) + 1), average_val_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.savefig('/content/drive/MyDrive/lung_cancer_model/validation_accuracy_plot85%.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de29be5c",
      "metadata": {
        "id": "de29be5c"
      },
      "source": [
        "#### Saving And Loading The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fdcc9b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fdcc9b6",
        "outputId": "e2a55173-2f20-4615-f91b-6140dd610d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "model.save('/content/drive/MyDrive/lung_cancer_model/lungs_cancer_classification85%.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "folds = 5\n",
        "num_val_samples = len(train_labels) // folds\n",
        "num_epochs = 500\n",
        "all_mae_histories = []\n",
        "validation_score = []\n",
        "\n",
        "# Define the filepath to save the best weights\n",
        "checkpoint_filepath = 'best_weights.h5'\n",
        "\n",
        "# Initialize the ModelCheckpoint callback\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_acc',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "for i in range(folds):\n",
        "    print('Processing fold #', i)\n",
        "    val_data = train_features[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_labels[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    partial_train_data = np.concatenate([train_features[:i * num_val_samples],train_features[(i + 1) * num_val_samples:]], axis=0)\n",
        "    partial_train_targets = np.concatenate([train_labels[:i * num_val_samples],train_labels[(i + 1) * num_val_samples:]], axis=0)\n",
        "    input_shape = (4, 4, 512)\n",
        "    model = build_model(input_shape)\n",
        "    history = model.fit(\n",
        "        train_features, train_labels, epochs=num_epochs, batch_size=32,\n",
        "        validation_data=(val_data, val_targets), verbose=0,\n",
        "        callbacks=[checkpoint_callback]  # Include the ModelCheckpoint callback\n",
        "    )\n",
        "    val_history = history.history['val_acc']\n",
        "    validation_score.append(val_history)\n",
        "\n",
        "# Restore the best weights of the model\n",
        "model.load_weights(checkpoint_filepath)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R26J9gNtO4nx",
        "outputId": "6bc37123-a2e8-4d39-b31b-cb91773c9caf"
      },
      "id": "R26J9gNtO4nx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "Epoch 3: val_acc did not improve from 0.50000\n",
            "\n",
            "Epoch 4: val_acc did not improve from 0.50000\n",
            "\n",
            "Epoch 5: val_acc did not improve from 0.50000\n",
            "\n",
            "Epoch 6: val_acc did not improve from 0.50000\n",
            "\n",
            "Epoch 7: val_acc did not improve from 0.50000\n",
            "\n",
            "Epoch 8: val_acc did not improve from 0.50000\n",
            "\n",
            "Epoch 9: val_acc did not improve from 0.50000\n",
            "\n",
            "Epoch 10: val_acc did not improve from 0.50000\n",
            "\n",
            "Epoch 11: val_acc did not improve from 0.50000\n",
            "\n",
            "Epoch 12: val_acc did not improve from 0.50000\n",
            "\n",
            "Epoch 13: val_acc did not improve from 0.50000\n",
            "\n",
            "Epoch 14: val_acc did not improve from 0.50000\n",
            "\n",
            "Epoch 15: val_acc did not improve from 0.50000\n",
            "\n",
            "Epoch 16: val_acc improved from 0.50000 to 0.51190, saving model to best_weights.h5\n",
            "\n",
            "Epoch 17: val_acc did not improve from 0.51190\n",
            "\n",
            "Epoch 18: val_acc did not improve from 0.51190\n",
            "\n",
            "Epoch 19: val_acc improved from 0.51190 to 0.52381, saving model to best_weights.h5\n",
            "\n",
            "Epoch 20: val_acc did not improve from 0.52381\n",
            "\n",
            "Epoch 21: val_acc did not improve from 0.52381\n",
            "\n",
            "Epoch 22: val_acc did not improve from 0.52381\n",
            "\n",
            "Epoch 23: val_acc did not improve from 0.52381\n",
            "\n",
            "Epoch 24: val_acc did not improve from 0.52381\n",
            "\n",
            "Epoch 25: val_acc did not improve from 0.52381\n",
            "\n",
            "Epoch 26: val_acc improved from 0.52381 to 0.55952, saving model to best_weights.h5\n",
            "\n",
            "Epoch 27: val_acc improved from 0.55952 to 0.59524, saving model to best_weights.h5\n",
            "\n",
            "Epoch 28: val_acc did not improve from 0.59524\n",
            "\n",
            "Epoch 29: val_acc did not improve from 0.59524\n",
            "\n",
            "Epoch 30: val_acc did not improve from 0.59524\n",
            "\n",
            "Epoch 31: val_acc improved from 0.59524 to 0.60714, saving model to best_weights.h5\n",
            "\n",
            "Epoch 32: val_acc did not improve from 0.60714\n",
            "\n",
            "Epoch 33: val_acc improved from 0.60714 to 0.61905, saving model to best_weights.h5\n",
            "\n",
            "Epoch 34: val_acc improved from 0.61905 to 0.64286, saving model to best_weights.h5\n",
            "\n",
            "Epoch 35: val_acc did not improve from 0.64286\n",
            "\n",
            "Epoch 36: val_acc did not improve from 0.64286\n",
            "\n",
            "Epoch 37: val_acc improved from 0.64286 to 0.66667, saving model to best_weights.h5\n",
            "\n",
            "Epoch 38: val_acc improved from 0.66667 to 0.67857, saving model to best_weights.h5\n",
            "\n",
            "Epoch 39: val_acc did not improve from 0.67857\n",
            "\n",
            "Epoch 40: val_acc did not improve from 0.67857\n",
            "\n",
            "Epoch 41: val_acc improved from 0.67857 to 0.69048, saving model to best_weights.h5\n",
            "\n",
            "Epoch 42: val_acc did not improve from 0.69048\n",
            "\n",
            "Epoch 43: val_acc improved from 0.69048 to 0.71429, saving model to best_weights.h5\n",
            "\n",
            "Epoch 44: val_acc improved from 0.71429 to 0.72619, saving model to best_weights.h5\n",
            "\n",
            "Epoch 45: val_acc did not improve from 0.72619\n",
            "\n",
            "Epoch 46: val_acc did not improve from 0.72619\n",
            "\n",
            "Epoch 47: val_acc did not improve from 0.72619\n",
            "\n",
            "Epoch 48: val_acc did not improve from 0.72619\n",
            "\n",
            "Epoch 49: val_acc did not improve from 0.72619\n",
            "\n",
            "Epoch 50: val_acc did not improve from 0.72619\n",
            "\n",
            "Epoch 51: val_acc did not improve from 0.72619\n",
            "\n",
            "Epoch 52: val_acc improved from 0.72619 to 0.73810, saving model to best_weights.h5\n",
            "\n",
            "Epoch 53: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 54: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 55: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 56: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 57: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 58: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 59: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 60: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 61: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 62: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 63: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 64: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 65: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 66: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 67: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 68: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 69: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 70: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 71: val_acc did not improve from 0.73810\n",
            "\n",
            "Epoch 72: val_acc improved from 0.73810 to 0.75000, saving model to best_weights.h5\n",
            "\n",
            "Epoch 73: val_acc improved from 0.75000 to 0.76190, saving model to best_weights.h5\n",
            "\n",
            "Epoch 74: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 75: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 76: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 77: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 78: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 79: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 80: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 81: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 82: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 83: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 84: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 85: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 86: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 87: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 88: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 89: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 90: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 91: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 92: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 93: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 94: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 95: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 96: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 97: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 98: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 99: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 100: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 101: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 102: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 103: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 104: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 105: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 106: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 107: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 108: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 109: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 110: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 111: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 112: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 113: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 114: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 115: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 116: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 117: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 118: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 119: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 120: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 121: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 122: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 123: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 124: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 125: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 126: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 127: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 128: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 129: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 130: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 131: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 132: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 133: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 134: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 135: val_acc did not improve from 0.76190\n",
            "\n",
            "Epoch 136: val_acc improved from 0.76190 to 0.77381, saving model to best_weights.h5\n",
            "\n",
            "Epoch 137: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 138: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 139: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 140: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 141: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 142: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 143: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 144: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 145: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 146: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 147: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 148: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 149: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 150: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 151: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 152: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 153: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 154: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 155: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 156: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 157: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 158: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 159: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 160: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 161: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 162: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 163: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 164: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 165: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 166: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 167: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 168: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 169: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 170: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 171: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 172: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 173: val_acc did not improve from 0.77381\n",
            "\n",
            "Epoch 174: val_acc improved from 0.77381 to 0.79762, saving model to best_weights.h5\n",
            "\n",
            "Epoch 175: val_acc did not improve from 0.79762\n",
            "\n",
            "Epoch 176: val_acc improved from 0.79762 to 0.80952, saving model to best_weights.h5\n",
            "\n",
            "Epoch 177: val_acc did not improve from 0.80952\n",
            "\n",
            "Epoch 178: val_acc improved from 0.80952 to 0.82143, saving model to best_weights.h5\n",
            "\n",
            "Epoch 179: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 180: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 181: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 182: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 183: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 184: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 185: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 186: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 187: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 188: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 189: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 190: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 191: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 192: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 193: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 194: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 195: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 196: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 197: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 198: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 199: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 200: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 201: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 202: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 203: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 204: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 205: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 206: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 207: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 208: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 209: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 210: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 211: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 212: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 213: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 214: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 215: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 216: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 217: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 218: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 219: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 220: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 221: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 222: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 223: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 224: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 225: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 226: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 227: val_acc did not improve from 0.82143\n",
            "\n",
            "Epoch 228: val_acc improved from 0.82143 to 0.83333, saving model to best_weights.h5\n",
            "\n",
            "Epoch 229: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 230: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 231: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 232: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 233: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 234: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 235: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 236: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 237: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 238: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 239: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 240: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 241: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 242: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 243: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 244: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 245: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 246: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 247: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 248: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 249: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 250: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 251: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 252: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 253: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 254: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 255: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 256: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 257: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 258: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 259: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 260: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 261: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 262: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 263: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 264: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 265: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 266: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 267: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 268: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 269: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 270: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 271: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 272: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 273: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 274: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 275: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 276: val_acc did not improve from 0.83333\n",
            "\n",
            "Epoch 277: val_acc improved from 0.83333 to 0.84524, saving model to best_weights.h5\n",
            "\n",
            "Epoch 278: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 279: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 280: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 281: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 282: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 283: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 284: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 285: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 286: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 287: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 288: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 289: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 290: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 291: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 292: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 293: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 294: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 295: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 296: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 297: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 298: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 299: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 300: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 301: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 302: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 303: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 304: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 305: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 306: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 307: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 308: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 309: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 310: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 311: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 312: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 313: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 314: val_acc did not improve from 0.84524\n",
            "\n",
            "Epoch 315: val_acc improved from 0.84524 to 0.85714, saving model to best_weights.h5\n",
            "\n",
            "Epoch 316: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 317: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 318: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 319: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 320: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 321: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 322: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 323: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 324: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 325: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 326: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 327: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 328: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 329: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 330: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 331: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 332: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 333: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 334: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 335: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 336: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 337: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 338: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 339: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 340: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 341: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 342: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 343: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 344: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 345: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 346: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 347: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 348: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 349: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 350: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 351: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 352: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 353: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 354: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 355: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 356: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 357: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 358: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 359: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 360: val_acc did not improve from 0.85714\n",
            "\n",
            "Epoch 361: val_acc improved from 0.85714 to 0.86905, saving model to best_weights.h5\n",
            "\n",
            "Epoch 362: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 363: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 364: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 365: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 366: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 367: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 368: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 369: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 370: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 371: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 372: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 373: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 374: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 375: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 376: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 377: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 378: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 379: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 380: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 381: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 382: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 383: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 384: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 385: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 386: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 387: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 388: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 389: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 390: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 391: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 392: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 393: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 394: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 395: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 396: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 397: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 398: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 399: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 400: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 401: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 402: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 403: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 404: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 405: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 406: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 407: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 408: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 409: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 410: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 411: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 412: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 413: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 414: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 415: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 416: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 417: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 418: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 419: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 420: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 421: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 422: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 423: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 424: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 425: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 426: val_acc did not improve from 0.86905\n",
            "\n",
            "Epoch 427: val_acc improved from 0.86905 to 0.88095, saving model to best_weights.h5\n",
            "\n",
            "Epoch 428: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 429: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 430: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 431: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 432: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 433: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 434: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 435: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 436: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 437: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 438: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 439: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 440: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 441: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 442: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 443: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 444: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 445: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 446: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 447: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 448: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 449: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 450: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 451: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 452: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 453: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 454: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 455: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 456: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 457: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 458: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 459: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 460: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 461: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 462: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 463: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 464: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 465: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 466: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 467: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 468: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 469: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 470: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 471: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 472: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 473: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 474: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 475: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 476: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 477: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 478: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 479: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 480: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 481: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 482: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 483: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 484: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 485: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 486: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 487: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 488: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 489: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 490: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 491: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 492: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 493: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 494: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 495: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 496: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 497: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 498: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 499: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 500: val_acc did not improve from 0.88095\n",
            "Processing fold # 1\n",
            "\n",
            "Epoch 1: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 2: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 3: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 4: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 5: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 6: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 7: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 8: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 9: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 10: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 11: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 12: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 13: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 14: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 15: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 16: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 17: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 18: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 19: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 20: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 21: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 22: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 23: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 24: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 25: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 26: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 27: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 28: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 29: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 30: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 31: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 32: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 33: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 34: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 35: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 36: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 37: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 38: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 39: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 40: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 41: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 42: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 43: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 44: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 45: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 46: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 47: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 48: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 49: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 50: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 51: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 52: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 53: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 54: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 55: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 56: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 57: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 58: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 59: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 60: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 61: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 62: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 63: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 64: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 65: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 66: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 67: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 68: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 69: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 70: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 71: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 72: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 73: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 74: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 75: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 76: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 77: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 78: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 79: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 80: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 81: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 82: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 83: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 84: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 85: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 86: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 87: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 88: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 89: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 90: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 91: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 92: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 93: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 94: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 95: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 96: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 97: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 98: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 99: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 100: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 101: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 102: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 103: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 104: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 105: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 106: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 107: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 108: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 109: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 110: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 111: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 112: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 113: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 114: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 115: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 116: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 117: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 118: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 119: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 120: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 121: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 122: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 123: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 124: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 125: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 126: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 127: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 128: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 129: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 130: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 131: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 132: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 133: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 134: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 135: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 136: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 137: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 138: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 139: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 140: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 141: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 142: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 143: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 144: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 145: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 146: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 147: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 148: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 149: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 150: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 151: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 152: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 153: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 154: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 155: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 156: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 157: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 158: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 159: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 160: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 161: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 162: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 163: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 164: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 165: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 166: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 167: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 168: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 169: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 170: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 171: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 172: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 173: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 174: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 175: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 176: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 177: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 178: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 179: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 180: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 181: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 182: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 183: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 184: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 185: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 186: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 187: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 188: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 189: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 190: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 191: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 192: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 193: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 194: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 195: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 196: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 197: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 198: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 199: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 200: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 201: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 202: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 203: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 204: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 205: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 206: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 207: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 208: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 209: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 210: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 211: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 212: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 213: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 214: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 215: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 216: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 217: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 218: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 219: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 220: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 221: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 222: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 223: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 224: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 225: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 226: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 227: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 228: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 229: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 230: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 231: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 232: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 233: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 234: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 235: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 236: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 237: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 238: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 239: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 240: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 241: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 242: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 243: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 244: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 245: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 246: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 247: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 248: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 249: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 250: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 251: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 252: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 253: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 254: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 255: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 256: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 257: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 258: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 259: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 260: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 261: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 262: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 263: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 264: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 265: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 266: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 267: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 268: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 269: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 270: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 271: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 272: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 273: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 274: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 275: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 276: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 277: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 278: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 279: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 280: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 281: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 282: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 283: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 284: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 285: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 286: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 287: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 288: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 289: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 290: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 291: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 292: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 293: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 294: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 295: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 296: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 297: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 298: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 299: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 300: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 301: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 302: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 303: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 304: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 305: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 306: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 307: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 308: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 309: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 310: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 311: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 312: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 313: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 314: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 315: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 316: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 317: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 318: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 319: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 320: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 321: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 322: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 323: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 324: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 325: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 326: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 327: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 328: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 329: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 330: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 331: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 332: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 333: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 334: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 335: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 336: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 337: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 338: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 339: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 340: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 341: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 342: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 343: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 344: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 345: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 346: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 347: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 348: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 349: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 350: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 351: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 352: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 353: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 354: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 355: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 356: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 357: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 358: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 359: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 360: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 361: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 362: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 363: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 364: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 365: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 366: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 367: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 368: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 369: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 370: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 371: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 372: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 373: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 374: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 375: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 376: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 377: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 378: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 379: val_acc did not improve from 0.88095\n",
            "\n",
            "Epoch 380: val_acc improved from 0.88095 to 0.89286, saving model to best_weights.h5\n",
            "\n",
            "Epoch 381: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 382: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 383: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 384: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 385: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 386: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 387: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 388: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 389: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 390: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 391: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 392: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 393: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 394: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 395: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 396: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 397: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 398: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 399: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 400: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 401: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 402: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 403: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 404: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 405: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 406: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 407: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 408: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 409: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 410: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 411: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 412: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 413: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 414: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 415: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 416: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 417: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 418: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 419: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 420: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 421: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 422: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 423: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 424: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 425: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 426: val_acc did not improve from 0.89286\n",
            "\n",
            "Epoch 427: val_acc improved from 0.89286 to 0.90476, saving model to best_weights.h5\n",
            "\n",
            "Epoch 428: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 429: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 430: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 431: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 432: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 433: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 434: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 435: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 436: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 437: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 438: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 439: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 440: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 441: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 442: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 443: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 444: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 445: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 446: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 447: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 448: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 449: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 450: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 451: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 452: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 453: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 454: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 455: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 456: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 457: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 458: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 459: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 460: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 461: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 462: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 463: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 464: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 465: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 466: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 467: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 468: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 469: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 470: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 471: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 472: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 473: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 474: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 475: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 476: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 477: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 478: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 479: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 480: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 481: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 482: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 483: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 484: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 485: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 486: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 487: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 488: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 489: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 490: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 491: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 492: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 493: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 494: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 495: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 496: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 497: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 498: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 499: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 500: val_acc did not improve from 0.90476\n",
            "Processing fold # 2\n",
            "\n",
            "Epoch 1: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 2: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 3: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 4: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 5: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 6: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 7: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 8: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 9: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 10: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 11: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 12: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 13: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 14: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 15: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 16: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 17: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 18: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 19: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 20: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 21: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 22: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 23: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 24: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 25: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 26: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 27: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 28: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 29: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 30: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 31: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 32: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 33: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 34: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 35: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 36: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 37: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 38: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 39: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 40: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 41: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 42: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 43: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 44: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 45: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 46: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 47: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 48: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 49: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 50: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 51: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 52: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 53: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 54: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 55: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 56: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 57: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 58: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 59: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 60: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 61: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 62: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 63: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 64: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 65: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 66: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 67: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 68: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 69: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 70: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 71: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 72: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 73: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 74: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 75: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 76: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 77: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 78: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 79: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 80: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 81: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 82: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 83: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 84: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 85: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 86: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 87: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 88: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 89: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 90: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 91: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 92: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 93: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 94: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 95: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 96: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 97: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 98: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 99: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 100: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 101: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 102: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 103: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 104: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 105: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 106: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 107: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 108: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 109: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 110: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 111: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 112: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 113: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 114: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 115: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 116: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 117: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 118: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 119: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 120: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 121: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 122: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 123: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 124: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 125: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 126: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 127: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 128: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 129: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 130: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 131: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 132: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 133: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 134: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 135: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 136: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 137: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 138: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 139: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 140: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 141: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 142: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 143: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 144: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 145: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 146: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 147: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 148: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 149: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 150: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 151: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 152: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 153: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 154: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 155: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 156: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 157: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 158: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 159: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 160: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 161: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 162: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 163: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 164: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 165: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 166: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 167: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 168: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 169: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 170: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 171: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 172: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 173: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 174: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 175: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 176: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 177: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 178: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 179: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 180: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 181: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 182: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 183: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 184: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 185: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 186: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 187: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 188: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 189: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 190: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 191: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 192: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 193: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 194: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 195: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 196: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 197: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 198: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 199: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 200: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 201: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 202: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 203: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 204: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 205: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 206: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 207: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 208: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 209: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 210: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 211: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 212: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 213: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 214: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 215: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 216: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 217: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 218: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 219: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 220: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 221: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 222: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 223: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 224: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 225: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 226: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 227: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 228: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 229: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 230: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 231: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 232: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 233: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 234: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 235: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 236: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 237: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 238: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 239: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 240: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 241: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 242: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 243: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 244: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 245: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 246: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 247: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 248: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 249: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 250: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 251: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 252: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 253: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 254: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 255: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 256: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 257: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 258: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 259: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 260: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 261: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 262: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 263: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 264: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 265: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 266: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 267: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 268: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 269: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 270: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 271: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 272: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 273: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 274: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 275: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 276: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 277: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 278: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 279: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 280: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 281: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 282: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 283: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 284: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 285: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 286: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 287: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 288: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 289: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 290: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 291: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 292: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 293: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 294: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 295: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 296: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 297: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 298: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 299: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 300: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 301: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 302: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 303: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 304: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 305: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 306: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 307: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 308: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 309: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 310: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 311: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 312: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 313: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 314: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 315: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 316: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 317: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 318: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 319: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 320: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 321: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 322: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 323: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 324: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 325: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 326: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 327: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 328: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 329: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 330: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 331: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 332: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 333: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 334: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 335: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 336: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 337: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 338: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 339: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 340: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 341: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 342: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 343: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 344: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 345: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 346: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 347: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 348: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 349: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 350: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 351: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 352: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 353: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 354: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 355: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 356: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 357: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 358: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 359: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 360: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 361: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 362: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 363: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 364: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 365: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 366: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 367: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 368: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 369: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 370: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 371: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 372: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 373: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 374: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 375: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 376: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 377: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 378: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 379: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 380: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 381: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 382: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 383: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 384: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 385: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 386: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 387: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 388: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 389: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 390: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 391: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 392: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 393: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 394: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 395: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 396: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 397: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 398: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 399: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 400: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 401: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 402: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 403: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 404: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 405: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 406: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 407: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 408: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 409: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 410: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 411: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 412: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 413: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 414: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 415: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 416: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 417: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 418: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 419: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 420: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 421: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 422: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 423: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 424: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 425: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 426: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 427: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 428: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 429: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 430: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 431: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 432: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 433: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 434: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 435: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 436: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 437: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 438: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 439: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 440: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 441: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 442: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 443: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 444: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 445: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 446: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 447: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 448: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 449: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 450: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 451: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 452: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 453: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 454: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 455: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 456: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 457: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 458: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 459: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 460: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 461: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 462: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 463: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 464: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 465: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 466: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 467: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 468: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 469: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 470: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 471: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 472: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 473: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 474: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 475: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 476: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 477: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 478: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 479: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 480: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 481: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 482: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 483: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 484: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 485: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 486: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 487: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 488: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 489: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 490: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 491: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 492: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 493: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 494: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 495: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 496: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 497: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 498: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 499: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 500: val_acc did not improve from 0.90476\n",
            "Processing fold # 3\n",
            "\n",
            "Epoch 1: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 2: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 3: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 4: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 5: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 6: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 7: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 8: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 9: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 10: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 11: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 12: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 13: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 14: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 15: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 16: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 17: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 18: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 19: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 20: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 21: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 22: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 23: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 24: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 25: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 26: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 27: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 28: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 29: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 30: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 31: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 32: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 33: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 34: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 35: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 36: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 37: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 38: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 39: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 40: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 41: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 42: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 43: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 44: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 45: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 46: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 47: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 48: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 49: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 50: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 51: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 52: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 53: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 54: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 55: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 56: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 57: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 58: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 59: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 60: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 61: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 62: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 63: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 64: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 65: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 66: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 67: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 68: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 69: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 70: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 71: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 72: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 73: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 74: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 75: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 76: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 77: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 78: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 79: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 80: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 81: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 82: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 83: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 84: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 85: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 86: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 87: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 88: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 89: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 90: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 91: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 92: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 93: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 94: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 95: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 96: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 97: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 98: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 99: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 100: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 101: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 102: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 103: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 104: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 105: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 106: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 107: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 108: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 109: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 110: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 111: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 112: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 113: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 114: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 115: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 116: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 117: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 118: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 119: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 120: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 121: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 122: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 123: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 124: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 125: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 126: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 127: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 128: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 129: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 130: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 131: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 132: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 133: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 134: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 135: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 136: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 137: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 138: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 139: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 140: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 141: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 142: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 143: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 144: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 145: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 146: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 147: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 148: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 149: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 150: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 151: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 152: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 153: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 154: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 155: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 156: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 157: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 158: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 159: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 160: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 161: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 162: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 163: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 164: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 165: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 166: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 167: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 168: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 169: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 170: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 171: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 172: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 173: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 174: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 175: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 176: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 177: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 178: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 179: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 180: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 181: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 182: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 183: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 184: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 185: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 186: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 187: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 188: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 189: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 190: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 191: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 192: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 193: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 194: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 195: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 196: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 197: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 198: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 199: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 200: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 201: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 202: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 203: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 204: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 205: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 206: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 207: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 208: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 209: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 210: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 211: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 212: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 213: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 214: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 215: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 216: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 217: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 218: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 219: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 220: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 221: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 222: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 223: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 224: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 225: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 226: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 227: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 228: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 229: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 230: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 231: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 232: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 233: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 234: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 235: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 236: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 237: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 238: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 239: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 240: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 241: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 242: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 243: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 244: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 245: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 246: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 247: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 248: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 249: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 250: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 251: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 252: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 253: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 254: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 255: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 256: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 257: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 258: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 259: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 260: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 261: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 262: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 263: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 264: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 265: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 266: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 267: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 268: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 269: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 270: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 271: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 272: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 273: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 274: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 275: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 276: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 277: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 278: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 279: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 280: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 281: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 282: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 283: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 284: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 285: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 286: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 287: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 288: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 289: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 290: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 291: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 292: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 293: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 294: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 295: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 296: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 297: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 298: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 299: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 300: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 301: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 302: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 303: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 304: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 305: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 306: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 307: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 308: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 309: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 310: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 311: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 312: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 313: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 314: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 315: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 316: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 317: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 318: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 319: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 320: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 321: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 322: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 323: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 324: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 325: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 326: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 327: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 328: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 329: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 330: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 331: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 332: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 333: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 334: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 335: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 336: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 337: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 338: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 339: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 340: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 341: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 342: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 343: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 344: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 345: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 346: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 347: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 348: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 349: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 350: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 351: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 352: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 353: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 354: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 355: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 356: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 357: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 358: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 359: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 360: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 361: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 362: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 363: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 364: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 365: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 366: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 367: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 368: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 369: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 370: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 371: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 372: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 373: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 374: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 375: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 376: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 377: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 378: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 379: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 380: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 381: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 382: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 383: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 384: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 385: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 386: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 387: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 388: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 389: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 390: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 391: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 392: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 393: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 394: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 395: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 396: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 397: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 398: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 399: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 400: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 401: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 402: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 403: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 404: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 405: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 406: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 407: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 408: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 409: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 410: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 411: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 412: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 413: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 414: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 415: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 416: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 417: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 418: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 419: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 420: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 421: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 422: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 423: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 424: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 425: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 426: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 427: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 428: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 429: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 430: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 431: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 432: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 433: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 434: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 435: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 436: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 437: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 438: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 439: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 440: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 441: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 442: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 443: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 444: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 445: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 446: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 447: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 448: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 449: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 450: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 451: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 452: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 453: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 454: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 455: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 456: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 457: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 458: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 459: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 460: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 461: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 462: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 463: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 464: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 465: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 466: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 467: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 468: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 469: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 470: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 471: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 472: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 473: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 474: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 475: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 476: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 477: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 478: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 479: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 480: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 481: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 482: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 483: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 484: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 485: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 486: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 487: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 488: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 489: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 490: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 491: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 492: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 493: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 494: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 495: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 496: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 497: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 498: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 499: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 500: val_acc did not improve from 0.90476\n",
            "Processing fold # 4\n",
            "\n",
            "Epoch 1: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 2: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 3: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 4: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 5: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 6: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 7: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 8: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 9: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 10: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 11: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 12: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 13: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 14: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 15: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 16: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 17: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 18: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 19: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 20: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 21: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 22: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 23: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 24: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 25: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 26: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 27: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 28: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 29: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 30: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 31: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 32: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 33: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 34: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 35: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 36: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 37: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 38: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 39: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 40: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 41: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 42: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 43: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 44: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 45: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 46: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 47: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 48: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 49: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 50: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 51: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 52: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 53: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 54: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 55: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 56: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 57: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 58: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 59: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 60: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 61: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 62: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 63: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 64: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 65: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 66: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 67: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 68: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 69: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 70: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 71: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 72: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 73: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 74: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 75: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 76: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 77: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 78: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 79: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 80: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 81: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 82: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 83: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 84: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 85: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 86: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 87: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 88: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 89: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 90: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 91: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 92: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 93: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 94: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 95: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 96: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 97: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 98: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 99: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 100: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 101: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 102: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 103: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 104: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 105: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 106: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 107: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 108: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 109: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 110: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 111: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 112: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 113: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 114: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 115: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 116: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 117: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 118: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 119: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 120: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 121: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 122: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 123: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 124: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 125: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 126: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 127: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 128: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 129: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 130: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 131: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 132: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 133: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 134: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 135: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 136: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 137: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 138: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 139: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 140: val_acc did not improve from 0.90476\n",
            "\n",
            "Epoch 141: val_acc improved from 0.90476 to 0.91667, saving model to best_weights.h5\n",
            "\n",
            "Epoch 142: val_acc did not improve from 0.91667\n",
            "\n",
            "Epoch 143: val_acc did not improve from 0.91667\n",
            "\n",
            "Epoch 144: val_acc did not improve from 0.91667\n",
            "\n",
            "Epoch 145: val_acc did not improve from 0.91667\n",
            "\n",
            "Epoch 146: val_acc did not improve from 0.91667\n",
            "\n",
            "Epoch 147: val_acc did not improve from 0.91667\n",
            "\n",
            "Epoch 148: val_acc did not improve from 0.91667\n",
            "\n",
            "Epoch 149: val_acc did not improve from 0.91667\n",
            "\n",
            "Epoch 150: val_acc did not improve from 0.91667\n",
            "\n",
            "Epoch 151: val_acc did not improve from 0.91667\n",
            "\n",
            "Epoch 152: val_acc improved from 0.91667 to 0.92857, saving model to best_weights.h5\n",
            "\n",
            "Epoch 153: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 154: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 155: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 156: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 157: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 158: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 159: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 160: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 161: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 162: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 163: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 164: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 165: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 166: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 167: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 168: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 169: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 170: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 171: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 172: val_acc did not improve from 0.92857\n",
            "\n",
            "Epoch 173: val_acc improved from 0.92857 to 0.94048, saving model to best_weights.h5\n",
            "\n",
            "Epoch 174: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 175: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 176: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 177: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 178: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 179: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 180: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 181: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 182: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 183: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 184: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 185: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 186: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 187: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 188: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 189: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 190: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 191: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 192: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 193: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 194: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 195: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 196: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 197: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 198: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 199: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 200: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 201: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 202: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 203: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 204: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 205: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 206: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 207: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 208: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 209: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 210: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 211: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 212: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 213: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 214: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 215: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 216: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 217: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 218: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 219: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 220: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 221: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 222: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 223: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 224: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 225: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 226: val_acc did not improve from 0.94048\n",
            "\n",
            "Epoch 227: val_acc improved from 0.94048 to 0.95238, saving model to best_weights.h5\n",
            "\n",
            "Epoch 228: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 229: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 230: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 231: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 232: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 233: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 234: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 235: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 236: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 237: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 238: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 239: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 240: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 241: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 242: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 243: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 244: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 245: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 246: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 247: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 248: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 249: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 250: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 251: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 252: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 253: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 254: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 255: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 256: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 257: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 258: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 259: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 260: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 261: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 262: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 263: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 264: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 265: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 266: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 267: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 268: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 269: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 270: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 271: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 272: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 273: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 274: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 275: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 276: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 277: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 278: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 279: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 280: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 281: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 282: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 283: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 284: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 285: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 286: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 287: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 288: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 289: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 290: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 291: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 292: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 293: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 294: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 295: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 296: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 297: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 298: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 299: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 300: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 301: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 302: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 303: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 304: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 305: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 306: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 307: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 308: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 309: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 310: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 311: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 312: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 313: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 314: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 315: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 316: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 317: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 318: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 319: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 320: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 321: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 322: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 323: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 324: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 325: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 326: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 327: val_acc did not improve from 0.95238\n",
            "\n",
            "Epoch 328: val_acc improved from 0.95238 to 0.97619, saving model to best_weights.h5\n",
            "\n",
            "Epoch 329: val_acc did not improve from 0.97619\n",
            "\n",
            "Epoch 330: val_acc did not improve from 0.97619\n",
            "\n",
            "Epoch 331: val_acc did not improve from 0.97619\n",
            "\n",
            "Epoch 332: val_acc did not improve from 0.97619\n",
            "\n",
            "Epoch 333: val_acc did not improve from 0.97619\n",
            "\n",
            "Epoch 334: val_acc did not improve from 0.97619\n",
            "\n",
            "Epoch 335: val_acc did not improve from 0.97619\n",
            "\n",
            "Epoch 336: val_acc did not improve from 0.97619\n",
            "\n",
            "Epoch 337: val_acc did not improve from 0.97619\n",
            "\n",
            "Epoch 338: val_acc did not improve from 0.97619\n",
            "\n",
            "Epoch 339: val_acc improved from 0.97619 to 0.98810, saving model to best_weights.h5\n",
            "\n",
            "Epoch 340: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 341: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 342: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 343: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 344: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 345: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 346: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 347: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 348: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 349: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 350: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 351: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 352: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 353: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 354: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 355: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 356: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 357: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 358: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 359: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 360: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 361: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 362: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 363: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 364: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 365: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 366: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 367: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 368: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 369: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 370: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 371: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 372: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 373: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 374: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 375: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 376: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 377: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 378: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 379: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 380: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 381: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 382: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 383: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 384: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 385: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 386: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 387: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 388: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 389: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 390: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 391: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 392: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 393: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 394: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 395: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 396: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 397: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 398: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 399: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 400: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 401: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 402: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 403: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 404: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 405: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 406: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 407: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 408: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 409: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 410: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 411: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 412: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 413: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 414: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 415: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 416: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 417: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 418: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 419: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 420: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 421: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 422: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 423: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 424: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 425: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 426: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 427: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 428: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 429: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 430: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 431: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 432: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 433: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 434: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 435: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 436: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 437: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 438: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 439: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 440: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 441: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 442: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 443: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 444: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 445: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 446: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 447: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 448: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 449: val_acc did not improve from 0.98810\n",
            "\n",
            "Epoch 450: val_acc improved from 0.98810 to 1.00000, saving model to best_weights.h5\n",
            "\n",
            "Epoch 451: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 452: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 453: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 454: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 455: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 456: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 457: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 458: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 459: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 460: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 461: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 462: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 463: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 464: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 465: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 466: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 467: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 468: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 469: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 470: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 471: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 472: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 473: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 474: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 475: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 476: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 477: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 478: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 479: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 480: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 481: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 482: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 483: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 484: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 485: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 486: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 487: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 488: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 489: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 490: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 491: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 492: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 493: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 494: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 495: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 496: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 497: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 498: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 499: val_acc did not improve from 1.00000\n",
            "\n",
            "Epoch 500: val_acc did not improve from 1.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyIIM5AuQiPS",
        "outputId": "0de6c957-9224-47e0-d27b-4babb0870a4b"
      },
      "id": "lyIIM5AuQiPS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.5,\n",
              "  0.511904776096344,\n",
              "  0.511904776096344,\n",
              "  0.511904776096344,\n",
              "  0.523809552192688,\n",
              "  0.523809552192688,\n",
              "  0.523809552192688,\n",
              "  0.523809552192688,\n",
              "  0.523809552192688,\n",
              "  0.523809552192688,\n",
              "  0.523809552192688,\n",
              "  0.5595238208770752,\n",
              "  0.5952380895614624,\n",
              "  0.5952380895614624,\n",
              "  0.5833333134651184,\n",
              "  0.5833333134651184,\n",
              "  0.6071428656578064,\n",
              "  0.5952380895614624,\n",
              "  0.6190476417541504,\n",
              "  0.6428571343421936,\n",
              "  0.6190476417541504,\n",
              "  0.6309523582458496,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.726190447807312,\n",
              "  0.738095223903656,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.738095223903656,\n",
              "  0.7142857313156128,\n",
              "  0.738095223903656,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.761904776096344,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.773809552192688,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.8095238208770752,\n",
              "  0.773809552192688,\n",
              "  0.8095238208770752,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.773809552192688,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.773809552192688,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.7857142686843872,\n",
              "  0.8095238208770752,\n",
              "  0.7857142686843872,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8809523582458496,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8690476417541504,\n",
              "  0.8452380895614624,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504],\n",
              " [0.4642857015132904,\n",
              "  0.511904776096344,\n",
              "  0.6071428656578064,\n",
              "  0.6071428656578064,\n",
              "  0.6428571343421936,\n",
              "  0.6309523582458496,\n",
              "  0.6666666865348816,\n",
              "  0.726190447807312,\n",
              "  0.6904761791229248,\n",
              "  0.738095223903656,\n",
              "  0.6904761791229248,\n",
              "  0.726190447807312,\n",
              "  0.6904761791229248,\n",
              "  0.75,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.8214285969734192,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.8214285969734192,\n",
              "  0.7857142686843872,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.8214285969734192,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8095238208770752,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8095238208770752,\n",
              "  0.8452380895614624,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.9047619104385376,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.9047619104385376,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.8690476417541504,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.9047619104385376,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.8809523582458496,\n",
              "  0.9047619104385376,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504],\n",
              " [0.523809552192688,\n",
              "  0.523809552192688,\n",
              "  0.523809552192688,\n",
              "  0.523809552192688,\n",
              "  0.523809552192688,\n",
              "  0.5357142686843872,\n",
              "  0.5714285969734192,\n",
              "  0.5714285969734192,\n",
              "  0.5357142686843872,\n",
              "  0.5833333134651184,\n",
              "  0.6071428656578064,\n",
              "  0.6190476417541504,\n",
              "  0.6190476417541504,\n",
              "  0.6309523582458496,\n",
              "  0.6190476417541504,\n",
              "  0.6071428656578064,\n",
              "  0.6071428656578064,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6190476417541504,\n",
              "  0.6190476417541504,\n",
              "  0.6071428656578064,\n",
              "  0.6071428656578064,\n",
              "  0.6190476417541504,\n",
              "  0.6071428656578064,\n",
              "  0.6190476417541504,\n",
              "  0.6309523582458496,\n",
              "  0.6904761791229248,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6666666865348816,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.7142857313156128,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.7142857313156128,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.6904761791229248,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.7023809552192688,\n",
              "  0.726190447807312,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.726190447807312,\n",
              "  0.7142857313156128,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.726190447807312,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.726190447807312,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.726190447807312,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.726190447807312,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.726190447807312,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.738095223903656,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.773809552192688,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.738095223903656,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.738095223903656,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872],\n",
              " [0.4523809552192688,\n",
              "  0.4642857015132904,\n",
              "  0.488095223903656,\n",
              "  0.5,\n",
              "  0.4642857015132904,\n",
              "  0.4761904776096344,\n",
              "  0.5476190447807312,\n",
              "  0.5714285969734192,\n",
              "  0.5952380895614624,\n",
              "  0.6071428656578064,\n",
              "  0.6428571343421936,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.7023809552192688,\n",
              "  0.6309523582458496,\n",
              "  0.6309523582458496,\n",
              "  0.6309523582458496,\n",
              "  0.6547619104385376,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6428571343421936,\n",
              "  0.6666666865348816,\n",
              "  0.6904761791229248,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6428571343421936,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.7023809552192688,\n",
              "  0.6666666865348816,\n",
              "  0.6547619104385376,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.738095223903656,\n",
              "  0.7023809552192688,\n",
              "  0.738095223903656,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.738095223903656,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.75,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7857142686843872,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.7976190447807312,\n",
              "  0.8333333134651184,\n",
              "  0.7976190447807312,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8095238208770752,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8214285969734192,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8452380895614624,\n",
              "  0.8452380895614624,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8452380895614624,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504],\n",
              " [0.369047611951828,\n",
              "  0.5714285969734192,\n",
              "  0.6547619104385376,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6666666865348816,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6666666865348816,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6785714030265808,\n",
              "  0.6547619104385376,\n",
              "  0.6547619104385376,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6785714030265808,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.6904761791229248,\n",
              "  0.7023809552192688,\n",
              "  0.7023809552192688,\n",
              "  0.7142857313156128,\n",
              "  0.7142857313156128,\n",
              "  0.726190447807312,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.761904776096344,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.773809552192688,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7857142686843872,\n",
              "  0.7976190447807312,\n",
              "  0.7857142686843872,\n",
              "  0.8214285969734192,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8333333134651184,\n",
              "  0.8809523582458496,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8571428656578064,\n",
              "  0.8333333134651184,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8690476417541504,\n",
              "  0.8452380895614624,\n",
              "  0.8809523582458496,\n",
              "  0.8690476417541504,\n",
              "  0.8809523582458496,\n",
              "  0.8928571343421936,\n",
              "  0.9047619104385376,\n",
              "  0.8809523582458496,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9166666865348816,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9285714030265808,\n",
              "  0.9285714030265808,\n",
              "  0.9285714030265808,\n",
              "  0.9285714030265808,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9285714030265808,\n",
              "  0.9047619104385376,\n",
              "  0.9047619104385376,\n",
              "  0.9285714030265808,\n",
              "  0.9166666865348816,\n",
              "  0.9047619104385376,\n",
              "  0.9166666865348816,\n",
              "  0.9166666865348816,\n",
              "  0.9285714030265808,\n",
              "  0.9285714030265808,\n",
              "  0.9166666865348816,\n",
              "  0.9047619104385376,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9166666865348816,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9404761791229248,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9404761791229248,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9404761791229248,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9404761791229248,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9404761791229248,\n",
              "  0.9404761791229248,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.976190447807312,\n",
              "  0.9523809552192688,\n",
              "  0.9642857313156128,\n",
              "  0.976190447807312,\n",
              "  0.976190447807312,\n",
              "  0.9642857313156128,\n",
              "  0.9523809552192688,\n",
              "  0.976190447807312,\n",
              "  0.976190447807312,\n",
              "  0.9523809552192688,\n",
              "  0.9523809552192688,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.9642857313156128,\n",
              "  0.976190447807312,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  0.988095223903656,\n",
              "  0.988095223903656,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  1.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(validation_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JERzI_8QQqzw",
        "outputId": "358b6f26-9127-437d-c64b-0e526ba5b4d6"
      },
      "id": "JERzI_8QQqzw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8148190479874611"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "test_gen = test_data_gen.flow_from_directory(test_dir,target_size=(150,150),class_mode='binary',batch_size=32)\n",
        "for data,label in test_gen:\n",
        "    print(data.shape)\n",
        "    print(label.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOd0zjqPQ3k9",
        "outputId": "e5e674b2-2110-404c-f44d-e8ef5096305f"
      },
      "id": "mOd0zjqPQ3k9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 114 images belonging to 2 classes.\n",
            "(32, 150, 150, 3)\n",
            "(32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_feature,test_label = feature_extraction(test_dir,114)\n",
        "test = model.evaluate(test_feature,test_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy27HNg2Q61t",
        "outputId": "65732db5-41a4-4eda-985d-2106f72c4ccb"
      },
      "id": "oy27HNg2Q61t",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 114 images belonging to 2 classes.\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2638 - acc: 0.8684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Kel5BJtREYC",
        "outputId": "211a08d9-7d79-4bbf-83de-d56008295f05"
      },
      "id": "3Kel5BJtREYC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.26375821232795715, 0.8684210777282715]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_val_history = [np.mean([x[i] for x in validation_score]) for i in range(num_epochs)]\n",
        "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1, len(average_mae_history) + 1), average_val_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.savefig('/content/drive/MyDrive/lung_cancer_model/validation_accuracy_plot86%.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "4CDVzIoTRZkP",
        "outputId": "2625813f-3f63-4db9-acd9-c067dbae327c"
      },
      "id": "4CDVzIoTRZkP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABefElEQVR4nO3deVxU9d4H8M/MwMww7MiObAruCq6Ie0matmhZmlmalqZp2aXlyVtqyy3rdjOtTFtcKivLrSyXMtxNJXFXUFAUZEeEYR+YOc8fAwdGQBmdYWD4vF8vXs/MOWfOfOfUc8+n33J+EkEQBBARERFZCamlCyAiIiIyJYYbIiIisioMN0RERGRVGG6IiIjIqjDcEBERkVVhuCEiIiKrwnBDREREVsXG0gU0NZ1Oh/T0dDg6OkIikVi6HCIiImoEQRBQWFgIX19fSKU3b5tpdeEmPT0d/v7+li6DiIiIbkNqairatm1702NaXbhxdHQEoL84Tk5OFq6GiIiIGkOtVsPf31+8j99Mqws31V1RTk5ODDdEREQtTGOGlHBAMREREVkVhhsiIiKyKgw3REREZFUsHm6WLVuGoKAgKJVKREREIDY2tsFjKyoq8Pbbb6N9+/ZQKpUICwvDjh07mrBaIiIiau4sGm5++uknREdHY+HChTh27BjCwsIwcuRIZGdn13v8G2+8gS+++AKffvopzp07h5kzZ+Khhx7C8ePHm7hyIiIiaq4kgiAIlvryiIgI9O3bF5999hkA/QP2/P398fzzz+O1116rc7yvry9ef/11zJ49W9w2btw42NnZYe3atfV+R3l5OcrLy8X31VPJCgoKOFuKiIiohVCr1XB2dm7U/dtiLTcajQZxcXGIioqqKUYqRVRUFA4dOlTvZ8rLy6FUKg222dnZ4cCBAw1+z6JFi+Ds7Cz+8QF+RERE1s1i4SY3NxdarRZeXl4G2728vJCZmVnvZ0aOHInFixcjMTEROp0OO3fuxKZNm5CRkdHg98ybNw8FBQXiX2pqqkl/BxERETUvFh9QbIylS5ciNDQUnTp1glwux5w5czB16tSbrjGhUCjEB/bxwX1ERETWz2Lhxt3dHTKZDFlZWQbbs7Ky4O3tXe9nPDw88Msvv6C4uBhXrlxBQkICHBwc0K5du6YomYiIiFoAi4UbuVyO3r17IyYmRtym0+kQExODyMjIm35WqVTCz88PlZWV2LhxI8aMGWPucomIiKiFsOjaUtHR0ZgyZQr69OmDfv36YcmSJSguLsbUqVMBAJMnT4afnx8WLVoEADhy5AjS0tIQHh6OtLQ0vPnmm9DpdHj11Vct+TOIiIioGbFouJkwYQJycnKwYMECZGZmIjw8HDt27BAHGaekpBiMpykrK8Mbb7yBS5cuwcHBAaNHj8Z3330HFxcXC/0CIiKi1kcQBOgEQCa99SKWlmDR59xYgjHz5ImIiKiu1QeT8fbv5/Dj9P7o365Nk3xni3jODRERERnnerEGmQVljTo2o6AUBSUVZqnjrd/OQRCAVzacREZBqVm+404w3BAREbUQYz8/iHsW70V+ieamx2WryxC5aBfGfn7Q5DWoy2oCU2peKYb8dzeSc4tN/j13guGGiIjIBA4k5iJL3bhWlcYor9QiJj4LxeWVAICi8kpcuVaCwvJKHEnOu+lnd5zVPww3ObcYWp3h6JMsdRkOJuWK74+lXEdqXkmdc6TmleDnf1JxPOW6wfa4y4bvK7QCfjuZ3vgf1gQYboiIiO7Qvgs5eGLlEYxdZrqWkq/3J+Ppb47ipZ9PAtC3xlSLvUW4ScgsFF+rSw27pkYu2YdJXx/BgcRcxF3Jw7jlf+PRFYdQVqEVj9FU6vDYl4fx6sZTGP/FIVwrqlmj8dgNYQfQB6/mxKKzpYiIiKzB1lP6ZYAyGjkepjG++fsyAH0rzHeHr6Cdu72471bh5tiVmgCSX1oBV3t5zfuqcTh7zmfjfFYhBAHIVJdh/dFUPBkZBADYdOwq0vL1Y2kqtAIu5RajjYMCAJBbK+hU+3LfJdjKJJBAgnu6eCHM38Xo32tKbLkhIiK6Q6W1Wj1KNKZpxQhqUxNm5v9yBl/tvyS+P5tegMKy+gcLl2q0OJ9V03JTe3xO7dpyi8qxP7Gme2rF3kvQVOoAABuPXTU4Z9r1mkHD+Q0MUl62+yI+252E574/BktPxGa4ISIiukO1x9qk5xvOHvrtZDq+2HvR6HNW6nQG7/eczxFf6wQg7sp15BaV4/XNp/HaxlO4el0/biYtvwS1s0VBrW6p2rUdqBp309nHCR6OCqTll+KX42kAgCvX9OcK9XQAAPxn6zn8WTWOp/p8c4eH4oXhoeL52rnbw1YmQVp+KVLzLDuDiuGGiIjoDl3MKRJfX63VyiEIAp7/8TgWbU9AfIbaqHNmF9bt/qktNjkPG+Ku4vsjKVj3TypWH7xc5/sBw3BTe19ukb5FZ0ioO54dol+j8fM9SSjVaMXv7hfsJh4747s4lFdqxZabngEuiL6nA3ydlQCA9x7ujrC2LgCAI8nXjPqtpsZwQ0REdAfySzRiUACAP89lYca3R3Expwjq0ppuoILSxj9zRhAEMWAEtVEZ7Ksee/P5nov4eOcFcfuFrEJsPn4VT63+54b6ar43Lb9ui0q/YDc8HhEAV5UtLl8rEbu/lLZSdPdzNjj2TFqB+DtcVPpxPN8+HYGfn41E/3ZtxDB0qzFB5sZwQ0REdAdOpxUYvP/hSAr+PJeF//1xHtmFNd1VtWcj3Yq6tFIc//LSiI4G+x7q6Se+Lq+s6bq6kFWIf/10ss65aoeqtBtadSQSoE+gG1RyG0yKCAQAfHf4CgDA18UObV0Ng9WR5DxxDI+LnS0AIMTTQQw1Yri5zHBDRERkNnsv5ODeJftwMjX/psdtO52BkR/vw/la06gb8mlMIvr8ZyceWf439l3IqfeYa0Ua5NTqWjKm5SanSB+KnJQ28HczDBhh/i74+dnIOp/JUht2Y1Uv+1S75ebG8UCdvJ3grNKHlI7ejvrvrqrZz8UOfq52Bsf/nXQNxRp9SHOuCje19Qlyw3/H9cCaqf1u/gPNjOGGiIiapeoZN8XllajU6m5xdP2frdTq8MYvp5GQWYhfTqTVe6ymUodrReV47vtjOJ9VaNDVc6OyCi2SsouwNCYRuUUaHL1yHV/tTwYAjO/TFpJa60iWVFQajJtpaJZRfTOLsquCiqeTEp6OCoN9AW4q9At2g6uqbriorauvvkspv1Qjfs+5G8b9RFS1tACoE2TautqhraudwRT0o1dqWmSc6gk3DgobjO/rj+Ban7EEPueGiIiane2nMxD980m4qmyRXlAGdwc5ts0dDE9H5U0/V16pxdhlf8NRYYMfpkdgy8l0ceZO7S6ZEk0lxi47CEHQt2ZUt0YAwKXcmsHBecUajFl2AJHt2mBYR088/+PxOk/8rTZzaHu8MrIT4q7kYebaYziTpsaLP50Q9zfUcvPvzafxV3w2tswZCB9nfcCoDkWejgq4O9SEGxupBIFVY3Dc7OW4XhWY+gS64ugVw4frhXg64HRaATYdS0NCRmGdYAPUdCMBQFsXw3Dj52IHW5kUO14cgtTrJRj+0V6UVehDppPSptmuCA6w5YaIiO5QYVkFyiv14SC3qNygJaK4vBKlVcGhoKQCFY1sgXnzt7MordAiveqheLlFGhy5VP84Dp1OEJ+gezwlH/EZasRezsNvp9KxbHeSeFztwbRHkvNwIasIidlFBsEGABKzi1BQWoGi8kp8e+gyUvNK8fPRq1i0PV4MNiq5TJwmDehv9sHu9vBwVCCyvXu9deaXVOB6scZg7M3Z9AL8GJuKnMJy/H4yQ9xePRXbz8UOcpuaW3WQuz0kVc1DKnlN+8RDvfwgk0rgpLRBV18ndPdzxpAONXXUDjaBbVR4qKcfAtxUGBhSc4y7gwJyWc13hXjqu6nkNlL43RB8nG/RamRpbLkhIqLblpZfinsW78XgUHcMDHHHgl/PYtHD3TGxXwBKNJUY/cl+VFTq8OXkPnh4+d94MMwX/3s07Kbn1OoEXC+u28qRlF1Uz9HA67+cwY+xKdg4a4DBLJ03Np9BsUYLmVQCrU4wCDdXb1hL6e0xXTGxXwBGfLwPybnF2HkuCx/vvGDwmdS8UriqbLH75WFwUNhg2e6L+PgvfRdWqJejGDrqG4sC6KdH//D+FfT0d8UP0yMgkUiw6sBlcf+Jq/ni68Rs/bifkFoBCgB6BbiIrwPbqMTBzJMiAvFob3/IpBKxRWV/Yv1jgdp7OODjCeEQBEGsGQCkUgns5DJoSvUBtG+Qq7hPaSuDs51tzUwpOzmaM7bcEBFRo5RVaBF35TquFZUjIVPfEvD7yXSUaLT442yWOFaleoDtD0dScOVaCdILyvDvzaehqdRhQ9xVVGh1KNVocTzler3jTeIz1NBUtfB09nHCw730s4OSaj1LRhAEHEjMxfqjqfgxNgUAsOVEmkG4qW6RmVK1pEB+SQWKyyuRUVCKP89lGXzngPZtYCuTYmRXbwDAy+tP1jtt+pnB7eCiksNGJjUYoxLi4VDn2BudTVejrEKHQ5euYV9iLo6lXMfptHxx/5FL17DjTAZS80rEIFcdbt4e0xVh/i549d5O4vH/Ht0Z3f2c8eEjPQDoW1hqdxX1DXLD4FDDVqQOXg549V797KvawaZa7a6zNg6GY31qj/1xYcsNERFZg1c3nMKWqtWfpRLg52cjDW6G1eM/ErOLUFahxZf7apYLqD0D6Wy6Giv2XMSOs5n4YFx3TOgbYPA9e85nAwCGdvDAN9P6YVdCFjYdS8PFWi03aw9fwfxfzxp8LktdjriqcScju3rhj7NZsJfL8MLwEGyIS4W6rBIXsgox47s4g1lMgL41AwCmDQrC6oPJBlOsqzkqbfBkZKD4vnZXzY0tLLYyCSq0DS9BMGVVbJ1tuUUazFx7zKCFpPq8kyODMLkqpFXzdbHDb88PavA7lLYyfPd0BGZ/fwxbT+u7vP7819AGjwf0z9C5lFuMgBtmaAGAh6MCiVX/DOobTNycMNwQEZGBM2kFKKvQok9QzWDTxKxCMdgA+sf/j//iUL03ucu5xfgxNsVgplDtsLDmYDJ2VD3Kf/4vZ8Ubec8AVyTnFON/f+pbgB4M8wUAhFaN/biUU4xKrQ5aQcCy3frlDDp5O4orYFef01Vli/8+EgZ7xVkM6+gJF5Ucfq4qqDPU+O+O8wbBpquvE2YObS+2Yng6KvHOmG5YH5eKwDb2cFTaoI29HJdyizGiizeclDW/t22tlpv2noazg356NhLfHbqCzj6O+ObvK2IrkLeTEpnquotrvnpvR+yKz8bFnCIxJMptpHWeM3M73h7TFTKpBFMGBN7y2C+e7I1PdyXhlZEd6+yr3XLTycvxjusyJ4YbIiISlVVocf+nBwAAh+cNh3fVo/U/31N3bSSdYDi92cdZiYLSCpRotHjrt3MA9N0XN06B/uVETUjSaHV4b1tCnXP7u9nhwXB9uPFzsYPSVoqyCh0uXytGYlYRMtVl8HJS4Nc5A5FZUIahH+4RP9s3yA3OdrZYPD5c3ObnYof4DDUOXTJcFmDdjP5wVBoGtPF9/TG+r3+D16ha9bUBDBe5BIBeAa7oFaAfszIwxB33faK/pmPCfZFdWI7Nx2umpTvb2eK5YSF4blgINh27iuif9Q/iC/V0MMmMpDYOCnwysWejjg31cmzwWE+nmt8b0a7NHddlTgw3REQEADh9tcBgdtGhS7nILdTA3VGOX6ueEfP2mK7ILSxHW1cVXt14Sjx2fJ+2mBQRiPm/nsGpq/pBru4OcjzZP0gcdNujrbO4DwDefKALzqSroRMEHLtyHZerZgi52cux9LGesK2auSOVShDW1gVHkvPw8vpTyK2aGTWqmw8UNjK0dVVBbiMVn+hbe3pztfYe9vgrvub9a6M6wcdZWSfYGMNWJsXSx8KhLq1Au5uMualepgDQB6++QW6QSIBNx/TX1KNWi8iYcD+k5JUg7XppowJWUyour1lKokdb55scaXkMN0REBACY/u1Rgy6TGx/lP6yjh8G4Dx8XJZ5cGQulrRQfjOsBiUSCrr5OYoCZPridwaDbB8N80aOtM9YeTsGILl54amCwuO9gUi4mfX0EDgob7H5pWJ2pxhHBbjiSnIcTtZ4yXB1iZFIJOvs4iU8gHlDPVOz2tcbEuKpsMXNo+0ZelZsbE+53y2PcaoWb3oGucFbZ4qNHw8Rwo6nVZSeTSvBiVAeT1GZq4f4u+P5ICuQyKZS2MkuXc1MMN0RErciag8koqdBiVtU4k9S8EiyNScT0we3qHQtS2/N3hxi8HxzqgVVP9UEbe4U4ZuVfUR0Q4GYPO1spJvUPxJla6y71DXLDpIhAdPR2wgM9fAzONTDEHSue6A0fZ2W9z1DpF9wGQJLBtr61xgT975Ee2H4mE0Hu9uji61Tn87UH/N44+Nfc7OQyfDOtH2QSCVzt9UGn9kylUiPWnLKksT39oNUJiGzfvLukAIYbIqJWo7CsAm/9fg6CAIS1dcHAEHdM+OIQ0gvKGlwfqdqgEHf0Dqzb3XN3Jy+D955OSswaVtMq0s7dAfZyGRyqHi5nI5Piyf71D2y9t5t3g9/fK9AFTkobqMv0XSPd/ZwNunNCvRwRepNBrrUDjZfTzZ9ybA5DO3jU2Taqmze2n8nEM4OC6/lE82Mrk+KxfgG3PrAZYLghIrJCqXkleG3TKcweFoIBVU+hTcsvRfVjZT7dlQhnO1vxCcDVM5vc7OX4dlo/SCUSJGSq0TfIDal5Jejqd3tjLJxVtvh1zkAobGSwkd3+o9VUchv8MnsgdIJ+6QRjA0rtWU71Pd/FEv73aBgmRQQiol3d0Eh3huGGiMgKzf7hGE5dLcDBpGu4/P59AAzXVjp8KQ8f/Xm+zucGhrijW1WQqe7euXFVamNVP8b/Tt1s0G5jjO7ujW2nm09Lib3CBoNC61+qge4Mww0RkRWqPSvp6/2XsOpAMkZ1Nxznsvt83a6oxjxpt6VaMqEn/j26zCTPjqHmjcsvEBG1QBVaHQRBgCAIyCgoxdXrJeKCjNk3DAz+z9Z4pBeUYeWBZACo80j+6vdymRRDO9YdG2ItTPVQPGr+2HJDRNTC7E7IxtQ1/+Ddh7ohPkONtYf1ayt5Oirw7dP9cO+S/Tf9/NAOHkjILBSf1Pvmg13Rxl4OW5kU9greFqjl47/FRETNyJVrxfBxtoPcpm7Desq1EqQXlGLaN/8AAF7ffMZgfaPswnIsqudpvzdq62qHtq52YrgJdFPd0WBfouaG/zYTETUTf53LwtAP99Q70Pfq9RIMX7wHj315GLUX0q5es2hEF/2U7L1VU7pfHtEBjzXwhFs/FxXC2rqI7xlsyNrw32giomYg7koenvn2KADgi1qraVfbn5jb4CrT7g5yjOxq+IyYfsFt6n1YnY1Ugg7eDoge0QH3dvXGZ483bs0hopaE3VJERBZ2IasQ45YfMtiWUVAKH+eaLqfY5LwGP9/ew8FgPSW5jRQ92jqjRFNZ59inBwVDYSODwkaGFU/2NkH1RM0Pww0RkYnFXbmOXQlZ6OrrjNFV06/LKrRYfzQVI7t5w9PR8AF0Ry9fr3OO2OQ8hHg6YPvpTAS2UeFI1WrWnz3eE/8k58FZJccnMYkAAHcHBfzdVHhtVCccvZyHe7v5QGkrq9NyM3Noe7wYFWqOn0zUrDDcEBGZUGFZBaaujhWXCdj/6l3wd1Ph1Q2nsOVkOg4n52HZ470MPnP5WnGd8/x6Ih3n0tUG6z3ZSCW4q6Mn7u/hC0EQ8OmuRAgC0CfIFYA+vKDWgpC+znaws5WhtEKLh3v64bVRnczxk4maHYYbIqJG2p2Qjc3H0xDi6YDn7w4RH+OfrS7D53suYsqAIGw/kyEGGwDYl5iDs+lqbDmZDgDYeioDT/a/hnWxKdAKQCdvRyRlFwHQr380d3gonv/xOHYlZNf5/gfDfcWp2hKJBPteuQvbTmdgYgPr/UilEoR4OuB0WgE8nBT1HkNkjRhuiIgaQV1WgRfWHUdhVXC5u5OnuEzByxtOYd+FHPwVn4USjf5Bet5OSmSqy/D65jN1zvXYl4fF17+drNn+n7Hd0L9dG2w7nYHtZzIBALOGtcfqg8mo0Ap4bpjhqtz+bio8W6ulpj7d2zrjdFoBgtvYG/+jiVoohhsionpcKyrHm7+dw4Nhvrinixe+O3RFDDaAfhBwkLs9nvnmHxy+pB/se7Vq7SZ/NzssvL+rOPsJAPq3c8PV66XiMV5OCng4KnAmTS0eUz1GZtHD3TEwxB0quQxjwv1wX3cflFfq6p39dCuvjOiIge3dEdXF0/iLQNRCMdwQEdXj/zaexl/xWfjtZDrOvT0SX+/XT892UNigqLwSSdlFkEmzxGBT2/N3hyKinRscFTYoLK9EW1c7fDOtH77en4wP/9A/w2b2XSEo1WjFcOPlpEAbezkAwEUlxxP9A8XzdbvNFbkBwNVejvt6+Nz6QCIrwnBDRBah0wmY/cMx2Cts8OEjPcTxK8b4Yu9F7D6fja8m94Gj0vaOa9LqBDz73VEcS8lHXrFG3P7priRcL6lAYBsVnuwfiP9sjcfney6K+9u62mHr84NxKi0fSlsZ+gS6QiKRYOsLg5GYXYjubZ2hsJFhxpB26OnvAqlUgohgN5xIzRfPMWVA0G1dAyKqi+GGiCziUm6xOK7kvh4+uKujcd0mWp2ARdv1Sw1sPp6GyZFBDR5bXF4JnSDAXm4DqbRugCjRVEKrE/DH2Sz8FV93IO/yqiDz3LD28HOpu/DitIHBcFbZYnCo4aKTAW1UCGhTc7ytTIoBITWLVnbzc0ZQGxUqdQKerNVSQ0R3huGGiCwivWrZAAD4NCYRwzp4NLrlYu3hK3jjl5qBugt+PYtNx9KwfmYkbG9YSuCd38+Jq2G397DH1hcGQ2krE/cv250kdhVVe3ZoOzzauy1WHkjGj7GpAABfZyUe6tnWoEWn2u2MhQH0YWfHi0OgEwSo5PyfYyJT4fILRGQRabXCzbGUfBy6dA2CICCvWINKrQ7Xq0JEQUkFzqYXoLxSKx5fO9hUO5GajzNpBQD06zAlZRdCpxPw+6l08ZiLOcX4Kz4LZ9MLcDa9AHFXrmPZ7iSD8wS1UeG5YSEI8XTEuF5tYWcrg41Uglfu7Qi5jRReTgoM6WDYQnO74QYAlLYyBhsiE+P/RxGRRaRVzRqSSSXQ6gR8vT8ZF7OLMP/Xs3BU2KC8UoflT/TCqxtO4VqxBj0DXLD5uYEoKq+7pEC1fy7noaC0Ak+t1q+aPWNIO2Sp9StfD+nggX0XcjDnh+N1PtfV1wkbZw0AAMhlUrHrqk+QG06/OQICILYISSQSfDutH775+zIWbjkLAPBxVtY5JxFZDsMNERklNa8EMqkEvi520OkEnLiaj+5+ziir0CItvxSdvJ0AAMm5xbCXy+DpVPfGn11YJq5ePbKrF7adzsSpq/nig+sKqwLM09/UTKU+fbUAWp2ApX9dELe52csNuon+9+cFDKk17uWrqhlOno4K3NPZE/uqvtNeLhMfhmcnl+GN+7oYdFXV1tCK2eP7+GPfhRz0qho8TETNB8MNETVaWn4phn64G+4OChx87W78djId0T+fxANhvqjU6rD9TCZWTumDADcV7vl4H4Ld7bHrpaEGN//ySi3GLf8bqXn6lpuhHTyw7XQmcovqjmWprVInYGlMIr7arx8/M65XW/z3kR5o/+9t4jGaSh3+is8S3wtVi2iHejmgX3Abcfu3T0egd6DrHV0LO7kMK5/qe0fnICLzYLghokb7cu9F6AQgu7AcKXklWH/0KgDgt5M141qe/uYo7OX6VpDk3GJkFJQhNjkPA0Pc4eGowMa4NDHYAPrxKn4udgZjcKr1DXJFBy9HbDx2FWUVuloLRcoxfUgwZFIJ3n+4Oy7mFKG0Qou1h1PEz74zpivm/6rvNgrxcEAHLwfMuSsEdnLZHQcbImreGG6IqFG0OgHr466K75Oyi1Cp09V7bLGmZvDva5tOY9+FHHTydsSOF4dg7eErBsf6u6kQ4ulQJ9y097DH+pn6cTD7EnPEQOTnYoc9rwwTx8A8VmtdJYWNDCsPJMNFZYvHIwKx6uBlJOcWo6O3EyQSCV4e2fEOrgARtRQMN0RUr9yicnyx9yKkUglmDG6HovJKcd0kQB9uEqsWfLyZ6nEuCZmFyC/RID5T/0TeBfd3gYPSBp6OSrhVPZm3IW4quRhuIoLd6kz3rvbKyI5wtrPFoFB3yKQSLH+iF347mY6He/k16jcTkXVguCGier23NR6bjqcBAKQSCXoHGHblHEnOQ35JRb2f/d+jYUjMKsQX+y4ZbD98KQ+CALTzsMe0QcHi9sGh7th8PA0OCht4OSlwMacYD4bVBJLa4cfP1a7BmpW2MrwwPFR838nbSRzgTEStB8MNkZW5kFWIj/48D7mNDAvu7wIPR4XB/mx1Gd7+/RwqtQJeHtmx3me0pFwrwa+1xtEsr7XUgNxGCk2lDn8n5Rq8BwClrRQfPRqO0d29Dbqwqs1cGwdA3/pS25hwP1RodRjW0RNanYA953MMWlvc7Gt+g59Lw+GGiAjgQ/yIrM6qA8n446x+wcdNx+oGjLWHr+D3UxnYcTazzviXar+dSodWJyCoTd2lBh4K94NMKkGlTj8VaUQXL3FfUBt73NfDBxKJBKE3ebDdkBuWKZBJJZjQNwBeTkr4utjh8YgAg6nZbvY160bdrOWGiAhgyw2RVbhWVI5Z3x9DXrEGSbXGwdQ3JuZIcs0q1lnqMqyLTcHu89lwUNjiWMp1APpZTgAwOTIIb/9+zuDzg0Ld8Vg/fyTnFkMmlWBoBw/8fioDAFBaUTMmp30D4WbJhHDc283bqN/nWrtbii03RHQLDDdEVmDF3ouIrRVaqt0YbsortTheayXq7MJyvLbpdIPnjWjnhmkDg7HqoP7ZMkpbKfoGucHbWYmetcbgPDUgCGv+vox5ozqJ25yUtvByUohPCAaA6Hs6YGxP4wf3Sms9J8eX4YaIboHhhqiFKyipMHi+S20Xs4sgCAIkEgmuF2vQ6z87xQfbAcDlqhaaap28HVFeqRNbbjp5O2HeaEc8PTgYLna2KNFo64zhAYD593fBtIHBBitgA/pn2GSpyyGRAHteHoYAt7rdXI1RUVkz5byhJwkTEVXjmBuiFu7QpVyUVmjRzsPeYLtMKkFReSUuVQUV/cKU+n33VI2TuXbDCtdDOnjg80m94KiwwZTIQMikEtjKpPBzsYO9wqbeYFP9XTcGGwAI9XQEALg7KBDYxv62lykY39cfjkobPNK77W19nohaF4YbohauegzNwPbuBuEjsCpsDP9oLzIKSsWxOHd38sSnE3vWe65ufs7o7OOE4wvuwVtjut1xbdXjbjwbCEWN5eWkxNE3ovDhIz3uuCYisn4MN0QtXPVYm37Bblj9VF+0c7fHl0/2xmN9/cVjNhy9isOXronHKW1lcFQa9kr3C3bDPZ31LToNLRZprOGdPBHURoWx4Xf+ED2FjYwLVBJRo3DMDVELVlReiXMZ+if+9gt2g5eTErteHlZrvxafxCTio501K2mHeOhbUzwcFSgs06++/daDXTFlQJDJ6/N1scOeV+4y+XmJiG6G4YaoBcpWl2HPhRyUarQQBMBVZQsvJ2Wd4258WB6gXyEbAHILa2YxDe3gUec4IqKWiuGGqAV6af1J7E/MFd839GC7ngEudba1ddWPxenk44TY5DwEuKkQ5G5f5zgiopaK4YaoBblerMHvp9IRn1FosL2hB9up5Db436NhuHq9BID+CcIyqX7cyjtjumHt4St4fniIeYsmImpiDDdELciIJfuQU6s7qZqfS8PPj2lo+nRHb0e8M/bOZ0QRETU3nC1F1EKcSSuoN9gAXG+JiKg2hhuiFmJjPYtgVuN6S0RENRhuiJqh1QeT8frm09BU6nC9WIPZPxzD6oOXGzze27nuTCkiotaKY26ImpkSTSXe+k2/EncHL0dcyCrE1qpVt2vzdlIiU10GAHWWXiAias0YboiaiZ3nsrDkrwsY36fmycIf/XkeZbUWjazNRWWLX+cMRHmFDk5K26Yqk4io2bN4t9SyZcsQFBQEpVKJiIgIxMbG3vT4JUuWoGPHjrCzs4O/vz/+9a9/oaysrImqJWpYcXklKrX1B5HGmP7tUZxNV2PhlrPiNnVZJTRV4UYqAR7qWbOMgbOd/sF99S1YSUTUmlm05eann35CdHQ0VqxYgYiICCxZsgQjR47E+fPn4enpWef4H374Aa+99hpWrVqFAQMG4MKFC3jqqacgkUiwePFiC/wCIr0Fv57Bt4euwNNRga0vDG5w9ez6HEjMxZOrjtz0mKkDg/BiVAfIpBJsPp4GAJDbWPy/TYiImiWL/q/j4sWLMX36dEydOhVdunTBihUroFKpsGrVqnqP//vvvzFw4EA8/vjjCAoKwogRIzBx4sRbtvYQmVOlVocNcfqZTNmF5fh6/yXkl2hQVqEFAOQUlkOnEwAAZRVa5JdoUFhWgRJNJbILy/DEyiMQhLrnndivpnsqItgNzna2sJfLxG1aXT0fIiIiy7XcaDQaxMXFYd68eeI2qVSKqKgoHDp0qN7PDBgwAGvXrkVsbCz69euHS5cuYdu2bXjyyScb/J7y8nKUl9c8G0StVpvuRxABOJuuRolGK77/Yt8lfLHvEsL9XfB4vwC8uvEUXh/dGc8MDsaTK4/gn8vXb3nOcH8XPNDDFz/GpgIA+gbp14iqvSo2ww0RUf0sFm5yc3Oh1Wrh5eVlsN3LywsJCQn1fubxxx9Hbm4uBg0aBEEQUFlZiZkzZ+Lf//53g9+zaNEivPXWWyatnVq3hEw1PBwUcFXJEZdyHUerwsrwTp7IVJfhbLo+QJ9IzceJ1HwAwLvb4pGSV9JgsLm/hw8uXytGN19nJGYXYcmEcPg4K3FfDx/4OCnRxqFuN5euvuYeIiJqWbOl9uzZg/feew+ff/45IiIikJSUhLlz5+Kdd97B/Pnz6/3MvHnzEB0dLb5Xq9Xw9/ev91iiWzl6OQ/jvziE9h4OGNHVC8t2XxT39Qt2Q2AbFWauPVbvZ787fKXe7a+N6oSZQ9vXu2/Z470arCWwDad/ExHVx2Lhxt3dHTKZDFlZWQbbs7Ky4O3tXe9n5s+fjyeffBLPPPMMAKB79+4oLi7GjBkz8Prrr0MqrTuESKFQQKFo/OBOotpS80rwx9lMeDsrcV93H3z05wXoBCAxuwiJ2UUGx47u7gM/FztMHxwMqVSC1LwSXCvS4EhynnhM/3Zu6BngiuV7DEORMb5/JgI/xqZg3qhOd/bjiIislMXCjVwuR+/evRETE4OxY8cCAHQ6HWJiYjBnzpx6P1NSUlInwMhk+gGWApvoycR0OgHTvz2KhEz9Ctzxd6lx6NK1eo8d1c0b/m76Kdmv39fFYN/Ta/5BTEI2uvo6Yd2MSADA3Z088egK/diybr7ORtU1MMQdA0PcjfoMEVFrYtFuqejoaEyZMgV9+vRBv379sGTJEhQXF2Pq1KkAgMmTJ8PPzw+LFi0CADzwwANYvHgxevbsKXZLzZ8/Hw888IAYcoiMtTshG9dLNHiopx8qdQJWH0xGV19nLP0rUQw2AMQuqPt6+MDXWQl1aSWGdvTAidT8BruVAOCtMV0R6uWIpwcFi9v6BrnhnTFd4edqxyndREQmZtFwM2HCBOTk5GDBggXIzMxEeHg4duzYIQ4yTklJMWipeeONNyCRSPDGG28gLS0NHh4eeOCBB/Duu+9a6idQC1eh1eG574+htEILXxc7HL2ch//9ecHgmMf6+uPXE+kordBCKgGi7+mA9h4O4v7R3X1u+h1tXVV4rZ4upCcjg0zyG4iIyJBEaGX9OWq1Gs7OzigoKICTk5OlyyELS8ouRNTifQCAwaHuUJdV4mTVDCcAiGzXBqun9sXptAIcTMpFmL8L7upY9wGTRERkXsbcv1vUbCkiU0uqNSh4f2Junf3Ln+gFpa0MfYPcxGfNEBFR88ZwQ63S7oRsrDqYjMBbrMvkopI3UUVERGQqDDfUKk1d8w8AYH+i/v34Pm3x64l0lNdagTv6ng6WKI2IiO4Qww0RgKjOXpgb1QGZBWXo6uuE3KJy+DjbWbosIiK6DQw31KqUV2phW8/DHsP9XeDppISfiz7QtHW9eXcVERE1Xww31GrEXcnDoysO4eFebQ22R7ZrA08npYWqIiIiU+PTw8iqXS/W4FqRflX4305mQCcAG+Kuivs7eTviv4/0sFR5RERkBmy5IatVodXh/k8PQKPVYc/LwwzWeAKAvkGuWD9zgIWqIyIic2G4Iat1Oq0AafmlAICYhGwkZKoN9lePryEiIuvCbimyWrG1Wmq+3HcRggAEu9sjzN8FABDgxkHDRETWiC03ZJXOpBXg/e0Jtd7rW226+jrhxagOWHUwGZP6B1qqPCIiMiOGG7I6ZRVaTKt6SN+NQjwdEOLpgPce6t7EVRERUVNhtxRZRFmFFu9tizfoOvr7Yi7e356ACq3uJp+8tfVHU5FdqJ8h9faYrnB3UIj7QjwdGvoYERFZCYYbsohv/r6ML/ddwvgvDonbHv/qCFbsvYifj6be0bl/O5UBAJh/fxdMjgxCRHDNgpehno53dG4iImr+GG7IIk6lFYivdToBlbVaa5JzivHBjgR8vifpts6dmFUIAGKo6Vf1f6USIMidg4iJiKwdx9yQRWhqLVCZll8KTa1wE5+pxsGkawCAR3v7w8NRUefzDblWVI7rJRWQSID2HvouqKEdPCC3kSKsrTMUNjIT/QIiImquGG6oyQiCgLnrTqBCq8OlnCJx+/K9F/HDkRTxfXWwAYAfY1Ow+XgaSjSV6O7ngtS8Evi7qaAurYCzyhZOSlv879EekEgkAICkbP15/VzsYCfXB5kgd3v89a+hcLLjv+5ERK0B/9eemkxesQZbTqbX2V472Nxo8c4L4ussdRYA4HxVt1O1WcPaI8BNhfJKLc5l6Kd8h94wcDigDbujiIhaC4YbajLVTws2tR+OpOCnf1JQrNGK2zgrioio9eKAYmoyadcNw42tTAIHhQ1kUglCPB0gk0oM9ntWjbV5PCIASx8Lh4PCBu097Oucd9XBZINgo5LLcE8XbzP8AiIiagnYckNN5saWm6cHtcPLIzpAAGArk0IQBATP2wYAmBwZiDcf6IoKnU4cBDwm3A+Avnur1zs765z/zQe64LF+AbCRSmAjY24nImqtGG7ILLQ6AYcvXUNhWQXsFTYY2N69TriJCHYzCCESiQQv3dMBcSnX8crIjpBKJVBI685ucrOXY8aQdriYXYTDl66JrTYDQtyhtOVsKCKi1o7hhsxiY9xVvLrxlPh+/v1d6nRL9Q5yrfO554eHNur8/x7dGQCwaFs8vth3CQAQ4sFxNkREdBvh5tKlS2jXrp05aiErsudCNgDAw1GBnMJyrNh7Ec52tgCAzj5OeKyvP5yUtnf8PXOjQqEuq0T/dm6Q3jBmh4iIWiejByaEhITgrrvuwtq1a1FWVmaOmqiFEwRBXDNq6WPh8HOxQ05hufgMmqWPhWPKgCCTfJdKboNFD3cXx+MQEREZHW6OHTuGHj16IDo6Gt7e3nj22WcRGxtrjtqohREEAasPJuOVDaeQW6SB3EaK3oGumDmsvXhMO3d7dh8REZFZGR1uwsPDsXTpUqSnp2PVqlXIyMjAoEGD0K1bNyxevBg5OTnmqJNagF0J2Xjrt3PYEHcVANA3yBUKGxke7d0WXk76ad0v3tOB3UdERGRWEkEQhDs5QXl5OT7//HPMmzcPGo0Gcrkc48ePxwcffAAfHx9T1WkyarUazs7OKCgogJOTk6XLsSpjlx3EidR83NXRA70DXfFAmC8C2+ifS3M+sxAJmWo8GOYrLpVARETUWMbcv2/7YSBHjx7Fc889Bx8fHyxevBgvv/wyLl68iJ07dyI9PR1jxoy53VNTC3S9WIMTqfkAgP8+EoY5d4eKwQYAOno7Yky4H4MNERGZndGzpRYvXozVq1fj/PnzGD16NL799luMHj0aUqk+JwUHB2PNmjUICgoyda3UDK0/morNx9Pw8siOAAAHhY1Rq3gTERGZmtHhZvny5Zg2bRqeeuqpBrudPD09sXLlyjsujpq/Vzbon2XzwfYEABCnexMREVmK0eEmMTHxlsfI5XJMmTLltgqilmHHmUws3HJGfJ9YNc3bRcVwQ0RElmX0mJvVq1dj/fr1dbavX78e33zzjUmKouZv5to4ZKnLxffXSzQAGG6IiMjyjA43ixYtgru7e53tnp6eeO+990xSFLU81XPu2C1FRESWZnS4SUlJQXBwcJ3tgYGBSElJMUlR1LxVanUN7nO2kzdhJURERHUZHW48PT1x6tSpOttPnjyJNm3amKQoat6u5JU0uI/dUkREZGlGDyieOHEiXnjhBTg6OmLIkCEAgL1792Lu3Ll47LHHTF4gNS/7E3Nw+VrD4YbdUkREZGlGh5t33nkHly9fxvDhw2Fjo/+4TqfD5MmTOebGysXEZ+Hpb47e9BgXhhsiIrIwo8ONXC7HTz/9hHfeeQcnT56EnZ0dunfvjsDAQHPUR83IpuNpBu8HhrRBgJs9zmeqcSwlHwC7pYiIyPKMDjfVOnTogA4dOpiyFmrm1KUVBu/v6eyFpwYG48t9F8VwwwHFRERkabcVbq5evYotW7YgJSUFGo3GYN/ixYtNUhg1PzeGGw9HJQCgX3DNQHJH5W3nZSIiIpMw+k4UExODBx98EO3atUNCQgK6deuGy5cvQxAE9OrVyxw1UjMgCEKdgcSeTvo1pLr61qzO6uWkbNK6iIiIbmT0VPB58+bh5ZdfxunTp6FUKrFx40akpqZi6NChePTRR81RI1mYVifg2e/iUHBDy41n1QKZtjIptr0wGD9O789FM4mIyOKMDjfx8fGYPHkyAMDGxgalpaVwcHDA22+/jQ8++MDkBZLlbT+TgT/PZdXZ7ulY00rTxdcJke35nCMiIrI8o8ONvb29OM7Gx8cHFy9eFPfl5uaarjJqFgRBwLLd+n/GA9q3gb+bnbjPTi6zVFlEREQNMjrc9O/fHwcOHAAAjB49Gi+99BLeffddTJs2Df379zd5gWRZV6+XIj5DDRupBJ9P6gU3FWdDERFR82b0gOLFixejqKgIAPDWW2+hqKgIP/30E0JDQzlTygrFJucBALq3dYaLSs7WGiIiavaMCjdarRZXr15Fjx49AOi7qFasWGGWwsgycovK4aaSI7eoHM4qW3x7+AoAoF+wGwBg4QNdMfGrw5hzV4glyyQiImqQUeFGJpNhxIgRiI+Ph4uLi5lKIks5kJiLJ1YeqXdfRFW46ezjhOPz74FEImnK0oiIiBrN6DE33bp1w6VLl8xRC1lQUnYhlu9Nqndf3yBXDGjvLr5nsCEioubM6HDzn//8By+//DJ+//13ZGRkQK1WG/xRy3M+sxBRi/fhYNK1OvseDPPF+pkDoLTlWBsiImoZjB5QPHr0aADAgw8+aPBf8IIgQCKRQKvVmq46ahKbb1gQ00Vli+mD2yE+Q43593exUFVERES3x+hws3v3bnPUQRaUlF1k8P6v6KFwd+CThomIqGUyOtwMHTrUHHWQheh0Av65nGewrY09n2VDREQtl9HhZt++fTfdP2TIkNsuhpre/qRcgzWj/FzsOGCYiIhaNKPDzbBhw+psq30z5Jib5qdUo8W7285hbLgf+gTpp3TvOJOB2OTriLuib7WZOjAI4f4u6BXgaslSiYiI7pjR4eb69esG7ysqKnD8+HHMnz8f7777rskKI9NZvicJaw+nYO3hFFx+/z4IgoCZa4+J++UyKZ4d0h7ezsqbnIWIiKhlMDrcODs719l2zz33QC6XIzo6GnFxcSYpjEwnLb9MfP37qXS8ueWcwf5H+rRlsCEiIqthdLhpiJeXF86fP2+q05EJ6QRBfD3nh+MG+xwUNpg1tH1Tl0RERGQ2RoebU6dOGbwXBAEZGRl4//33ER4ebqq6yITSrpfWu318n7Z4eWRHeDqy1YaIiKyH0eEmPDwcEokEQq3WAADo378/Vq1aZbLCyHTS8uuGG4WNFNMHt2OwISIiq2N0uElOTjZ4L5VK4eHhAaWSN8nmqEKrQ0aBYbjp5O2IrS8MhkzKKd9ERGR9jA43gYGB5qiDzCSzoAw6QT8j6vNJvfDmb2ex4IEuDDZERGS1jA43L7zwAkJCQvDCCy8YbP/ss8+QlJSEJUuWmKo2MoH0qi4pb2clorp4IaqLl4UrIiIiMi+jVwXfuHEjBg4cWGf7gAEDsGHDBpMURaaTVVgOAJzqTURErYbR4ebatWv1PuvGyckJubm5JimKTCdbrX/GjacjF8IkIqLWwehwExISgh07dtTZvn37drRr184kRdGdOZGaj41xVwEAOVUtNx4MN0RE1EoYPeYmOjoac+bMQU5ODu6++24AQExMDD766COOt2kGBEHA2GUHAQAhng7Irgo3nPJNRESthdEtN9OmTcNHH32ElStX4q677sJdd92FtWvXYvny5Zg+ffptFbFs2TIEBQVBqVQiIiICsbGxDR47bNgwSCSSOn/33XffbX23tUnOLRZfp+WXii037JYiIqLW4raWX5g1axZmzZqFnJwc2NnZwcHB4bYL+OmnnxAdHY0VK1YgIiICS5YswciRI3H+/Hl4enrWOX7Tpk3QaDTi+2vXriEsLAyPPvrobddgTWKT88TX+SUVyC6sGnPjxHBDREStg9EtN8nJyUhMTAQAeHh4iMEmMTERly9fNrqAxYsXY/r06Zg6dSq6dOmCFStWQKVSNfi0Yzc3N3h7e4t/O3fuhEqlajDclJeXQ61WG/xZsyO1wk12YRm7pYiIqNUxOtw89dRT+Pvvv+tsP3LkCJ566imjzqXRaBAXF4eoqKiagqRSREVF4dChQ406x8qVK/HYY4/B3t6+3v2LFi2Cs7Oz+Ofv729UjS3J9WIN/jibKb5Pu16K/JIKABxQTERErYfR4eb48eP1Puemf//+OHHihFHnys3NhVarhZeX4YPlvLy8kJmZ2cCnasTGxuLMmTN45plnGjxm3rx5KCgoEP9SU1ONqrEl+ebQZZRotOL7s+n6VipbmQSuKltLlUVERNSkjB5zI5FIUFhYWGd7QUEBtFptPZ8wn5UrV6J79+7o169fg8coFAooFK2j1eKPs1kAgFHdvLH9TCbOZejDTainIyQSLrdAREStg9EtN0OGDMGiRYsMgoxWq8WiRYswaNAgo87l7u4OmUyGrKwsg+1ZWVnw9va+6WeLi4uxbt06PP3000Z9p7UqKKlAQqY+zDzcq63BvulDgi1REhERkUUY3XLzwQcfYMiQIejYsSMGDx4MANi/fz/UajV27dpl1Lnkcjl69+6NmJgYjB07FgCg0+kQExODOXPm3PSz69evR3l5OZ544gljf4JVOnolD4IAtHO3RxdfJ3F7gJsKD/TwtWBlRERETcvolpsuXbrg1KlTGD9+PLKzs1FYWIjJkycjISEB3bp1M7qA6OhofPXVV/jmm28QHx+PWbNmobi4GFOnTgUATJ48GfPmzavzuZUrV2Ls2LFo06aN0d9pjaqngPcLdoOHQ0033Kxh7WEjM/ofMxERUYt1W8+58fX1xXvvvWewLT8/H5999tktW1xuNGHCBOTk5GDBggXIzMxEeHg4duzYIQ4yTklJgVRqeHM+f/48Dhw4gD///PN2yrdKR2qFG7mNFE8PCkZGQSke7uVn4cqIiIialkQQBOFOThATE4OVK1di8+bNUKlUuHbtmqlqMwu1Wg1nZ2cUFBTAycnp1h9oAUo0lejx5p+o1Ak48H93oa2rytIlERERmZQx9+/b6q9ITU3F22+/jeDgYIwYMQIAsHnz5kZN3ybTOHLpGs6kFaBCq8P/bTyNSp0APxc7BhsiImr1Gh1uKioqsH79eowcORIdO3bEiRMn8OGHH0IqleKNN97AvffeC1tbPkulKZxNL8BjXx3G418dxue7L+K3k+kAgIhgNwtXRkREZHmNHnPj5+eHTp064YknnsC6devg6uoKAJg4caLZiqP6LdudBEEA1GWV+HSXfimMjl6OmBsVauHKiIiILK/R4aayslJcgVsmk5mzJrqJxKxCbD9T0/1XqRMQ4KbC1hcGcVYUERERjOiWSk9Px4wZM/Djjz/C29sb48aNw+bNm/nk2yb2+Z6LEARAaVvzj+45TvcmIiISNfqOqFQqMWnSJOzatQunT59G586d8cILL6CyshLvvvsudu7c2eTLL7RGuxKyAQBvPtAVUgng72ZX54nERERErdlt/ed++/bt8Z///AdXrlzB1q1bUV5ejvvvv7/OAphkWlqdgIJS/SrfUV288OvsQVj/7ADIbdhqQ0REVO22HuJXTSqVYtSoURg1ahRycnLw3XffmaouqkdhWYX42tnOFu4OrWNBUCIiImOY7D/5PTw8EB0dbarTUT3yS/ThxkFhA1uOsSEiIqoX75AtSH5Vl5SzHZ8nRERE1BCGmxakgOGGiIjolhhuWpD8Eg0AwEXFcENERNQQhpsWhC03REREt2b0bCmtVos1a9YgJiYG2dnZ0Ol0Bvt37dplsuLIUPWAYrbcEBERNczocDN37lysWbMG9913H7p168YnFDeh6pYbJ7bcEBERNcjocLNu3Tr8/PPPGD16tDnqoZsQW27s5BauhIiIqPkyesyNXC5HSEiIOWqhWygo5YBiIiKiWzE63Lz00ktYunQpBEEwRz3UgOMp1/FXvH5dKQ4oJiIiapjR3VIHDhzA7t27sX37dnTt2hW2toY32k2bNpmsOKqxeOcF8XVbVzsLVkJERNS8GR1uXFxc8NBDD5mjFrqJ9PxSAMBTA4LQ3c/ZwtUQERE1X0aHm9WrV5ujDrqF7MJyAMAT/QM4Q42IiOgmbntV8JycHJw/fx4A0LFjR3h4eJisKDJUVqFFYVklAMDDUWnhaoiIiJo3owcUFxcXY9q0afDx8cGQIUMwZMgQ+Pr64umnn0ZJSYk5amy1dDr9oO1stb7VRm4jhZPytvMoERFRq2B0uImOjsbevXvx22+/IT8/H/n5+fj111+xd+9evPTSS+aosVW6mFOEsLf/xMc7LyCnqAwA4OmoYJcUERHRLRjdDLBx40Zs2LABw4YNE7eNHj0adnZ2GD9+PJYvX27K+lqtD3ecR2FZJZbGJKKTtyMAfbghIiKimzO65aakpAReXl51tnt6erJbyoSKyivF19WDiT053oaIiOiWjA43kZGRWLhwIcrKysRtpaWleOuttxAZGWnS4lqj1LwSpOWXQqOtWZA0U13VLeXElhsiIqJbMbpbaunSpRg5ciTatm2LsLAwAMDJkyehVCrxxx9/mLzA1iS7sAyjl+6HwlZq0EqTkKEGAHg4MNwQERHditHhplu3bkhMTMT333+PhIQEAMDEiRMxadIk2Nnxybl34uv9ySgsr0RhOZBbpBG3H76UBwAIdLe3VGlEREQtxm3NK1apVJg+fbqpa2nV8oo1WHv4Sr37Siu0AIAQD4emLImIiKhFalS42bJlC0aNGgVbW1ts2bLlpsc++OCDJimstVl9MBklGm2D+6USoJ0HW26IiIhupVHhZuzYscjMzISnpyfGjh3b4HESiQRabcM3aGrYbyfTAQBeTgpkVT20rzZ/NxWUtrKmLouIiKjFadRsKZ1OB09PT/F1Q38MNo2Xnl+Kl9efxNn0AgiCgIwC/YyoYR086z2eXVJERESNY/RU8G+//Rbl5XVbFjQaDb799luTFNUa/N/GU9gQdxX3f3oA6rJKlFfqp373DHARjxkU4i6+Dvd3AREREd2a0eFm6tSpKCgoqLO9sLAQU6dONUlRrcGFrEIAgCAAOYX6VhsnpQ3auqrEY94Z2w2rnuqDFU/0wvQh7SxSJxERUUtj9GwpQRDqXd/o6tWrcHZ2NklRrYG9wgaAvgWs+gnEHo4KdPdzhovKFp6OCgS1USGY07+JiIiM0uhw07NnT0gkEkgkEgwfPhw2NjUf1Wq1SE5Oxr333muWIq2Rvbzm+uXUWl7BWWWLPS8Pg41MykUyiYiIbkOjw031LKkTJ05g5MiRcHCoGeAql8sRFBSEcePGmbxAa6WS18x8unq9FEDN8gouKrlFaiIiIrIGjQ43CxcuBAAEBQVhwoQJUCq5iOOdsJXVDHc6V7W8Alf9JiIiunNGj7mZMmWKOepodUo0Nat+bz2VAYCrfhMREZmC0eFGq9Xi448/xs8//4yUlBRoNBqD/Xl5eSYrzpoVl9d9JhBX/SYiIrpzRk8Ff+utt7B48WJMmDABBQUFiI6OxsMPPwypVIo333zTDCVap+JaLTcA0NXXCUM7eFioGiIiIuthdLj5/vvv8dVXX+Gll16CjY0NJk6ciK+//hoLFizA4cOHzVGjVaq9jpS9XIatLwzmQGIiIiITMDrcZGZmonv37gAABwcH8YF+999/P7Zu3Wra6qxYUbm+5ebuTp74M3qohashIiKyHkaHm7Zt2yIjQz8Atn379vjzzz8BAP/88w8UCo4ZaYwKrQ6aquUWFo8Pg5+LnYUrIiIish5Gh5uHHnoIMTExAIDnn38e8+fPR2hoKCZPnoxp06aZvEBrU6HVYeGWs+J7ldzoMd1ERER0E0bfWd9//33x9YQJExAQEIBDhw4hNDQUDzzwgEmLs0abj6XhhyMpAAC5TAq5jdH5koiIiG7ijpsNIiMjERkZaYpaWoWr10vE1ww2REREpteocLNly5ZGn/DBBx+87WJaA6m0Zr2o6kHFREREZDqNCjfV60pVk0gkEAShzjZA/5A/alj1IplERERkHo3qF9HpdOLfn3/+ifDwcGzfvh35+fnIz8/H9u3b0atXL+zYscPc9bZ4DDdERETmZfSYmxdffBErVqzAoEGDxG0jR46ESqXCjBkzEB8fb9ICrU02ww0REZFZGT2i9eLFi3Bxcamz3dnZGZcvXzZBSdatdsvNG/d1tmAlRERE1snocNO3b19ER0cjKytL3JaVlYVXXnkF/fr1M2lx1kYQBDHc7H/1LjwzuJ2FKyIiIrI+RoebVatWISMjAwEBAQgJCUFISAgCAgKQlpaGlStXmqNGq5FfUgGNVv9kYq4ATkREZB5Gj7kJCQnBqVOnsHPnTiQkJAAAOnfujKioKHHGFNXvYk4RAMBFZQuFjczC1RAREVmn23qIn0QiwYgRIzBixAhT12PVvt6fDAC4q6OnhSshIiKyXo0KN5988glmzJgBpVKJTz755KbHvvDCCyYpzNoUlFZgx9lMAMBzw9pbuBoiIiLr1ahw8/HHH2PSpElQKpX4+OOPGzxOIpEw3DTgerEGAGAvlyHUy9HC1RAREVmvRoWb5OTkel9T41UvteCg5CrgRERE5sSVG5uIGG4UDDdERETm1Kg7bXR0dKNPuHjx4tsuxpoVlTHcEBERNYVG3WmPHz/eqJNxKnjD2C1FRETUNBp1p929e7e567B6heyWIiIiahIcc9NEisVwY2vhSoiIiKzbbTUjHD16FD///DNSUlKg0WgM9m3atMkkhVkTnU7AtSL9mlIOCj6ZmIiIyJyMbrlZt24dBgwYgPj4eGzevBkVFRU4e/Ysdu3aBWdnZ3PU2OLN+O4ovqp6OjHH3BAREZmX0eHmvffew8cff4zffvsNcrkcS5cuRUJCAsaPH4+AgACjC1i2bBmCgoKgVCoRERGB2NjYmx6fn5+P2bNnw8fHBwqFAh06dMC2bduM/t6m9Fd8tvia3VJERETmZXS4uXjxIu677z4AgFwuR3FxMSQSCf71r3/hyy+/NOpcP/30E6Kjo7Fw4UIcO3YMYWFhGDlyJLKzs+s9XqPR4J577sHly5exYcMGnD9/Hl999RX8/PyM/RlNprJqFfBqbLkhIiIyL6PDjaurKwoLCwEAfn5+OHPmDAB9i0pJSYlR51q8eDGmT5+OqVOnokuXLlixYgVUKhVWrVpV7/GrVq1CXl4efvnlFwwcOBBBQUEYOnQowsLCjP0ZTUZd9XybahxzQ0REZF5Gh5shQ4Zg586dAIBHH30Uc+fOxfTp0zFx4kQMHz680efRaDSIi4tDVFRUTTFSKaKionDo0KF6P7NlyxZERkZi9uzZ8PLyQrdu3fDee+9Bq9U2+D3l5eVQq9UGf02poLTC4D27pYiIiMyr0X0kZ86cQbdu3fDZZ5+hrKwMAPD666/D1tYWf//9N8aNG4c33nij0V+cm5sLrVYLLy8vg+1eXl5ISEio9zOXLl3Crl27MGnSJGzbtg1JSUl47rnnUFFRgYULF9b7mUWLFuGtt95qdF2mll9iOJuMz7khIiIyr0bfaXv06IG+ffvimWeewWOPPQZA39Ly2muvma24G+l0Onh6euLLL7+ETCZD7969kZaWhg8//LDBcDNv3jyD5SPUajX8/f2bqmTk39By48gxN0RERGbV6G6pvXv3omvXrnjppZfg4+ODKVOmYP/+/bf9xe7u7pDJZMjKyjLYnpWVBW9v73o/4+Pjgw4dOkAmqxm30rlzZ2RmZtZ53k41hUIBJycng7+mVFBiGG6UthxzQ0REZE6NDjeDBw/GqlWrkJGRgU8//RSXL1/G0KFD0aFDB3zwwQfIzMw06ovlcjl69+6NmJgYcZtOp0NMTAwiIyPr/czAgQORlJQEna5mBtKFCxfg4+MDuVxu1Pc3lRvH3LjZN886iYiIrIXRA4rt7e0xdepU7N27FxcuXMCjjz6KZcuWISAgAA8++KBR54qOjsZXX32Fb775BvHx8Zg1axaKi4sxdepUAMDkyZMxb9488fhZs2YhLy8Pc+fOxYULF7B161a89957mD17trE/o8nkV7XchLV1xs/PRjLcEBERmdkdDQAJCQnBv//9bwQGBmLevHnYunWrUZ+fMGECcnJysGDBAmRmZiI8PBw7duwQBxmnpKRAKq3JX/7+/vjjjz/wr3/9Cz169ICfnx/mzp2L//u//7uTn2FW+aX67rIBIe7oF+xm4WqIiIis322Hm3379mHVqlXYuHEjpFIpxo8fj6efftro88yZMwdz5sypd9+ePXvqbIuMjMThw4eN/h5LqR5z42zHKeBERERNwahwk56ejjVr1mDNmjVISkrCgAED8Mknn2D8+PGwt7c3V40tWvWYGxeGGyIioibR6HAzatQo/PXXX3B3d8fkyZMxbdo0dOzY0Zy1WYXqqeAuKoYbIiKiptDocGNra4sNGzbg/vvvN5iKTTdXVLX8Ap9MTERE1DQaHW62bNlizjqsVrFGH27s5AyERERETcHoqeBknFKNft0rey6YSURE1CQYbsyspCrcqGy57AIREVFTYLgxI51OQGmFPtywW4qIiKhpMNyYUXWwAdgtRURE1FQYbsyouksKAJQ2DDdERERNgeHGjKoHE9vZyiCVSixcDRERUevAcGNGJRX6aeAqjrchIiJqMgw3ZlRcXjVTiuNtiIiImgzDjRmVcho4ERFRk2O4MaMSPp2YiIioyTHcmFH1VHCOuSEiImo6DDdmJI65kbNbioiIqKkw3JhRdbcUW26IiIiaDsONGYkDihluiIiImgzDjRkVa7iuFBERUVNjuDGj0qpuKXuOuSEiImoyDDdmVMKWGyIioibHcGNGJZwKTkRE1OQYbsxEEAQkZRUBAJyUthauhoiIqPVguDGDjIJSjP38b5zPKoRKLsPdnTwtXRIREVGrwXBjBl/uu4STqfkAgCf7B8LVXm7ZgoiIiFoRhhszOHIpDwAwpIMH/nVPBwtXQ0RE1Low3JhYQWkF4jPVAID/PdIDSlsOJiYiImpKDDcmFnclD4IABLvbw9NJaelyiIiIWh2GGxNLuVYCAOjs42jhSoiIiFonhhsT02h1AAClDbujiIiILIHhxsQ0lfpwI7fhpSUiIrIE3oFNrLwq3CgYboiIiCyCd2ATY8sNERGRZfEObGLlDDdEREQWxTuwidV0S3FAMRERkSUw3JgYu6WIiIgsi3dgE6ueCi6X8dISERFZAu/AJlZeoQXAlhsiIiJL4R3YxKpbbjgVnIiIyDJ4BzYxjrkhIiKyLN6BTYwP8SMiIrIs3oFNjC03RERElsU7sIlp+JwbIiIii2K4MTFxKjhbboiIiCyCd2ATE6eC8zk3REREFsE7sImJU8FteWmJiIgsgXdgExMXzmTLDRERkUXwDmxiXBWciIjIsngHNiFBEDgVnIiIyMJ4BzahCq0gvuZUcCIiIstguDGh8kqt+JpPKCYiIrIM3oFNqLpLCuCAYiIiIkvhHdiEqqeB20glkEolFq6GiIiodWK4MaHyCi6aSUREZGm8C5sQl14gIiKyPN6FTYjTwImIiCyPd2ETqp4txWngRERElsNwY0J8OjEREZHl8S5sQhquK0VERGRxvAubEMfcEBERWR7vwiZUvfwCww0REZHl8C5sQhVVU8FtZXyAHxERkaUw3JiQRgw3vKxERESWwruwCVVWdUsx3BAREVkO78ImxG4pIiIiy2O4MaEKdksRERFZHO/CJlTBbikiIiKLaxZ34WXLliEoKAhKpRIRERGIjY1t8Ng1a9ZAIpEY/CmVyiastmHsliIiIrI8i4ebn376CdHR0Vi4cCGOHTuGsLAwjBw5EtnZ2Q1+xsnJCRkZGeLflStXmrDihlWyW4qIiMjiLH4XXrx4MaZPn46pU6eiS5cuWLFiBVQqFVatWtXgZyQSCby9vcU/Ly+vJqy4YRp2SxEREVmcRe/CGo0GcXFxiIqKErdJpVJERUXh0KFDDX6uqKgIgYGB8Pf3x5gxY3D27NkGjy0vL4darTb4M5fqbikbdksRERFZjEXDTW5uLrRabZ2WFy8vL2RmZtb7mY4dO2LVqlX49ddfsXbtWuh0OgwYMABXr16t9/hFixbB2dlZ/PP39zf576hW3S3FhTOJiIgsp8XdhSMjIzF58mSEh4dj6NCh2LRpEzw8PPDFF1/Ue/y8efNQUFAg/qWmppqtNnZLERERWZ6NJb/c3d0dMpkMWVlZBtuzsrLg7e3dqHPY2tqiZ8+eSEpKqne/QqGAQqG441obg91SRERElmfRJga5XI7evXsjJiZG3KbT6RATE4PIyMhGnUOr1eL06dPw8fExV5mNxm4pIiIiy7Noyw0AREdHY8qUKejTpw/69euHJUuWoLi4GFOnTgUATJ48GX5+fli0aBEA4O2330b//v0REhKC/Px8fPjhh7hy5QqeeeYZS/4MAHyIHxERUXNg8XAzYcIE5OTkYMGCBcjMzER4eDh27NghDjJOSUmBVFoTFq5fv47p06cjMzMTrq6u6N27N/7++2906dLFUj9BpGG3FBERkcVJBEEQLF1EU1Kr1XB2dkZBQQGcnJxMeu6pq2Ox+3wO/vtID4zvY75ZWURERK2NMfdv9p+YUHW3FMfcEBERWQ7vwibE2VJERESWx3BjQhVcW4qIiMjieBc2IXZLERERWR7vwibEbikiIiLLY7gxIXZLERERWR7vwiZU8xA/ttwQERFZCsONCVWy5YaIiMjieBc2Ia4KTkREZHm8C5tQpa665YbdUkRERJbCcGNCFZXsliIiIrI03oVNiKuCExERWR7vwiYiCAIqdHzODRERkaUx3JiIViegen11PqGYiIjIcngXNpHqLimA3VJERESWxLuwiVR3SQHsliIiIrIkhhsTqZ4pBQC2Ul5WIiIiS+Fd2EQqdfpuKRupBFIpW26IiIgsheHGRDSVnClFRETUHDDcmAhXBCciImoeeCc2kepuKU4DJyIisizeiU2E3VJERETNA8ONibBbioiIqHngndhEdAJgZyuDna3M0qUQERG1ajaWLsBa9A50Rfw791q6DCIiolaPLTdERERkVRhuiIiIyKow3BAREZFVYbghIiIiq8JwQ0RERFaF4YaIiIisCsMNERERWRWGGyIiIrIqDDdERERkVRhuiIiIyKow3BAREZFVYbghIiIiq8JwQ0RERFaF4YaIiIisio2lC2hqgiAAANRqtYUrISIiosaqvm9X38dvptWFm8LCQgCAv7+/hSshIiIiYxUWFsLZ2fmmx0iExkQgK6LT6ZCeng5HR0dIJBKTnVetVsPf3x+pqalwcnIy2XnJEK9z0+G1bhq8zk2D17npmOtaC4KAwsJC+Pr6Qiq9+aiaVtdyI5VK0bZtW7Od38nJif+P0wR4nZsOr3XT4HVuGrzOTccc1/pWLTbVOKCYiIiIrArDDREREVkVhhsTUSgUWLhwIRQKhaVLsWq8zk2H17pp8Do3DV7nptMcrnWrG1BMRERE1o0tN0RERGRVGG6IiIjIqjDcEBERkVVhuCEiIiKrwnBjAsuWLUNQUBCUSiUiIiIQGxtr6ZJanH379uGBBx6Ar68vJBIJfvnlF4P9giBgwYIF8PHxgZ2dHaKiopCYmGhwTF5eHiZNmgQnJye4uLjg6aefRlFRURP+iuZv0aJF6Nu3LxwdHeHp6YmxY8fi/PnzBseUlZVh9uzZaNOmDRwcHDBu3DhkZWUZHJOSkoL77rsPKpUKnp6eeOWVV1BZWdmUP6VZW758OXr06CE+xCwyMhLbt28X9/Mam8f7778PiUSCF198UdzGa20ab775JiQSicFfp06dxP3N7joLdEfWrVsnyOVyYdWqVcLZs2eF6dOnCy4uLkJWVpalS2tRtm3bJrz++uvCpk2bBADC5s2bDfa///77grOzs/DLL78IJ0+eFB588EEhODhYKC0tFY+59957hbCwMOHw4cPC/v37hZCQEGHixIlN/Euat5EjRwqrV68Wzpw5I5w4cUIYPXq0EBAQIBQVFYnHzJw5U/D39xdiYmKEo0ePCv379xcGDBgg7q+srBS6desmREVFCcePHxe2bdsmuLu7C/PmzbPET2qWtmzZImzdulW4cOGCcP78eeHf//63YGtrK5w5c0YQBF5jc4iNjRWCgoKEHj16CHPnzhW381qbxsKFC4WuXbsKGRkZ4l9OTo64v7ldZ4abO9SvXz9h9uzZ4nutViv4+voKixYtsmBVLduN4Uan0wne3t7Chx9+KG7Lz88XFAqF8OOPPwqCIAjnzp0TAAj//POPeMz27dsFiUQipKWlNVntLU12drYAQNi7d68gCPrramtrK6xfv148Jj4+XgAgHDp0SBAEfRCVSqVCZmameMzy5csFJycnoby8vGl/QAvi6uoqfP3117zGZlBYWCiEhoYKO3fuFIYOHSqGG15r01m4cKEQFhZW777meJ3ZLXUHNBoN4uLiEBUVJW6TSqWIiorCoUOHLFiZdUlOTkZmZqbBdXZ2dkZERIR4nQ8dOgQXFxf06dNHPCYqKgpSqRRHjhxp8ppbioKCAgCAm5sbACAuLg4VFRUG17pTp04ICAgwuNbdu3eHl5eXeMzIkSOhVqtx9uzZJqy+ZdBqtVi3bh2Ki4sRGRnJa2wGs2fPxn333WdwTQH++2xqiYmJ8PX1Rbt27TBp0iSkpKQAaJ7XudUtnGlKubm50Gq1Bv+wAMDLywsJCQkWqsr6ZGZmAkC917l6X2ZmJjw9PQ3229jYwM3NTTyGDOl0Orz44osYOHAgunXrBkB/HeVyOVxcXAyOvfFa1/fPonof6Z0+fRqRkZEoKyuDg4MDNm/ejC5duuDEiRO8xia0bt06HDt2DP/880+dffz32XQiIiKwZs0adOzYERkZGXjrrbcwePBgnDlzplleZ4YbolZq9uzZOHPmDA4cOGDpUqxSx44dceLECRQUFGDDhg2YMmUK9u7da+myrEpqairmzp2LnTt3QqlUWrocqzZq1CjxdY8ePRAREYHAwED8/PPPsLOzs2Bl9WO31B1wd3eHTCarMyI8KysL3t7eFqrK+lRfy5tdZ29vb2RnZxvsr6ysRF5eHv9Z1GPOnDn4/fffsXv3brRt21bc7u3tDY1Gg/z8fIPjb7zW9f2zqN5HenK5HCEhIejduzcWLVqEsLAwLF26lNfYhOLi4pCdnY1evXrBxsYGNjY22Lt3Lz755BPY2NjAy8uL19pMXFxc0KFDByQlJTXLf6cZbu6AXC5H7969ERMTI27T6XSIiYlBZGSkBSuzLsHBwfD29ja4zmq1GkeOHBGvc2RkJPLz8xEXFyces2vXLuh0OkRERDR5zc2VIAiYM2cONm/ejF27diE4ONhgf+/evWFra2twrc+fP4+UlBSDa3369GmDMLlz5044OTmhS5cuTfNDWiCdTofy8nJeYxMaPnw4Tp8+jRMnToh/ffr0waRJk8TXvNbmUVRUhIsXL8LHx6d5/jtt8iHKrcy6desEhUIhrFmzRjh37pwwY8YMwcXFxWBEON1aYWGhcPz4ceH48eMCAGHx4sXC8ePHhStXrgiCoJ8K7uLiIvz666/CqVOnhDFjxtQ7Fbxnz57CkSNHhAMHDgihoaGcCn6DWbNmCc7OzsKePXsMpnSWlJSIx8ycOVMICAgQdu3aJRw9elSIjIwUIiMjxf3VUzpHjBghnDhxQtixY4fg4eHBqbO1vPbaa8LevXuF5ORk4dSpU8Jrr70mSCQS4c8//xQEgdfYnGrPlhIEXmtTeemll4Q9e/YIycnJwsGDB4WoqCjB3d1dyM7OFgSh+V1nhhsT+PTTT4WAgABBLpcL/fr1Ew4fPmzpklqc3bt3CwDq/E2ZMkUQBP108Pnz5wteXl6CQqEQhg8fLpw/f97gHNeuXRMmTpwoODg4CE5OTsLUqVOFwsJCC/ya5qu+awxAWL16tXhMaWmp8Nxzzwmurq6CSqUSHnroISEjI8PgPJcvXxZGjRol2NnZCe7u7sJLL70kVFRUNPGvab6mTZsmBAYGCnK5XPDw8BCGDx8uBhtB4DU2pxvDDa+1aUyYMEHw8fER5HK54OfnJ0yYMEFISkoS9ze36ywRBEEwfXsQERERkWVwzA0RERFZFYYbIiIisioMN0RERGRVGG6IiIjIqjDcEBERkVVhuCEiIiKrwnBDREREVoXhhoiIiKwKww0RtUoSiQS//PKLpcsgIjNguCGiJvfUU09BIpHU+bv33nstXRoRWQEbSxdARK3Tvffei9WrVxtsUygUFqqGiKwJW26IyCIUCgW8vb0N/lxdXQHou4yWL1+OUaNGwc7ODu3atcOGDRsMPn/69GncfffdsLOzQ5s2bTBjxgwUFRUZHLNq1Sp07doVCoUCPj4+mDNnjsH+3NxcPPTQQ1CpVAgNDcWWLVvEfdevX8ekSZPg4eEBOzs7hIaG1gljRNQ8MdwQUbM0f/58jBs3DidPnsSkSZPw2GOPIT4+HgBQXFyMkSNHwtXVFf/88w/Wr1+Pv/76yyC8LF++HLNnz8aMGTNw+vRpbNmyBSEhIQbf8dZbb2H8+PE4deoURo8ejUmTJiEvL0/8/nPnzmH79u2Ij4/H8uXL4e7u3nQXgIhun1nWGiciuokpU6YIMplMsLe3N/h79913BUEQBADCzJkzDT4TEREhzJo1SxAEQfjyyy8FV1dXoaioSNy/detWQSqVCpmZmYIgCIKvr6/w+uuvN1gDAOGNN94Q3xcVFQkAhO3btwuCIAgPPPCAMHXqVNP8YCJqUhxzQ0QWcdddd2H58uUG29zc3MTXkZGRBvsiIyNx4sQJAEB8fDzCwsJgb28v7h84cCB0Oh3Onz8PiUSC9PR0DB8+/KY19OjRQ3xtb28PJycnZGdnAwBmzZqFcePG4dixYxgxYgTGjh2LAQMG3NZvJaKmxXBDRBZhb29fp5vIVOzs7Bp1nK2trcF7iUQCnU4HABg1ahSuXLmCbdu2YefOnRg+fDhmz56N//3vfyavl4hMi2NuiKhZOnz4cJ33nTt3BgB07twZJ0+eRHFxsbj/4MGDkEql6NixIxwdHREUFISYmJg7qsHDwwNTpkzB2rVrsWTJEnz55Zd3dD4iahpsuSEiiygvL0dmZqbBNhsbG3HQ7vr169GnTx8MGjQI33//PWJjY7Fy5UoAwKRJk7Bw4UJMmTIFb775JnJycvD888/jySefhJeXFwDgzTffxMyZM+Hp6YlRo0ahsLAQBw8exPPPP9+o+hYsWIDevXuja9euKC8vx++//y6GKyJq3hhuiMgiduzYAR8fH4NtHTt2REJCAgD9TKZ169bhueeeg4+PD3788Ud06dIFAKBSqfDHH39g7ty56Nu3L1QqFcaNG4fFixeL55oyZQrKysrw8ccf4+WXX4a7uzseeeSRRtcnl8sxb948XL58GXZ2dhg8eDDWrVtngl9OROYmEQRBsHQRRES1SSQSbN68GWPHjrV0KUTUAnHMDREREVkVhhsiIiKyKhxzQ0TNDnvLiehOsOWGiIiIrArDDREREVkVhhsiIiKyKgw3REREZFUYboiIiMiqMNwQERGRVWG4ISIiIqvCcENERERW5f8Bz1oOVcotM2sAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/lung_cancer_model/lungs_cancer_classification86%.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcMGfGTCRjWN",
        "outputId": "4b5d7f72-1bbc-4d0f-b68f-2f0971e4b719"
      },
      "id": "YcMGfGTCRjWN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "test_features,test_labels = feature_extraction(test_dir,114)\n",
        "# Assuming you have your test data and labels loaded as test_features and test_labels\n",
        "\n",
        "# Get predictions on the test data using the trained model\n",
        "predictions = model.predict(test_features)\n",
        "\n",
        "# Convert predictions from probabilities to class labels (assuming binary classification)\n",
        "predicted_labels = (predictions > 0.6).astype(int)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(test_labels, predicted_labels)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVQ8rYtgR4mh",
        "outputId": "a88b3777-2991-4967-9fb8-33cb32607344"
      },
      "id": "XVQ8rYtgR4mh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 114 images belonging to 2 classes.\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "F1 Score: 0.810126582278481\n",
            "Confusion Matrix:\n",
            "[[67  3]\n",
            " [12 32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=[\"F1 Score\"], y=[f1])\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"F1 Score\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.savefig('/content/drive/MyDrive/lung_cancer_model/F1_score.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "zR8q-U-JTXSs",
        "outputId": "27c90fe5-e893-426c-9395-fbf4e520e850"
      },
      "id": "zR8q-U-JTXSs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.savefig('/content/drive/MyDrive/lung_cancer_model/confucious_martix.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "N3Im3_rKTY4K",
        "outputId": "8b7af7ab-7817-4b93-d425-c4e8bcf8a4f0"
      },
      "id": "N3Im3_rKTY4K",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a98f184",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4a98f184"
      },
      "outputs": [],
      "source": [
        "model.save('lungs_cancer_classification.sav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7bd6c0c",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e7bd6c0c"
      },
      "outputs": [],
      "source": [
        "network = models.load_model('lungs_cancer_classification.sav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "981a8282",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "981a8282"
      },
      "outputs": [],
      "source": [
        "network.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "473f943a",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "473f943a"
      },
      "outputs": [],
      "source": [
        "print(test_feature[0].shape)\n",
        "print(test_label[50])\n",
        "\n",
        "network.predict(test_feature)[50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e053176",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3e053176"
      },
      "outputs": [],
      "source": [
        "network.evaluate(test_feature,test_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e20de4a5",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e20de4a5"
      },
      "outputs": [],
      "source": [
        "!apt-get install unrar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ly2TURE_xeFB",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ly2TURE_xeFB"
      },
      "outputs": [],
      "source": [
        "model = models.load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dXUhflUd6Spc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXUhflUd6Spc",
        "outputId": "3ffc0b93-9e80-4ba4-a7b5-618a8057dd35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.10/dist-packages (4.19.0)\n",
            "Requirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.8.3)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (6.4.0)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.16.1)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.16.0)\n",
            "Requirement already satisfied: tensorflow-decision-forests>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.9.0)\n",
            "Requirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (23.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.25.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.0.8)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (13.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (4.11.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (1.11.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.3.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.37.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.0.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.43.0)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.2)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.86)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2024.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (2.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2023.6.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.18.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflowjs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9k3v2U3e7I0F",
      "metadata": {
        "id": "9k3v2U3e7I0F"
      },
      "outputs": [],
      "source": [
        "import tensorflowjs as tfjs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S1xqd1Fb7pvX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1xqd1Fb7pvX",
        "outputId": "c7914321-43fe-4c72-dbb7-f4557cb505d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        }
      ],
      "source": [
        "model = models.load_model(\"/content/drive/MyDrive/lungs_cancer_classification.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rCdI1KGD6Xuu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCdI1KGD6Xuu",
        "outputId": "8f808c52-6d9c-4b29-abeb-010695741af0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "failed to lookup keras version from the file,\n",
            "    this is likely a weight only file\n"
          ]
        }
      ],
      "source": [
        "tfjs.converters.save_keras_model(model,\"/content/drive/MyDrive/json_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S7fqfQlD6b-f",
      "metadata": {
        "id": "S7fqfQlD6b-f"
      },
      "outputs": [],
      "source": [
        "model = models.load_model(\"/content/drive/MyDrive/lung_cancer_model/lungs_cancer_classification85%.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkPO-egFXCIN",
        "outputId": "5a1baf75-9056-41e7-e2b6-aea9304ea367"
      },
      "id": "UkPO-egFXCIN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_14 (Flatten)        (None, 8192)              0         \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 32)                262176    \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 262209 (1.00 MB)\n",
            "Trainable params: 262209 (1.00 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extraction(directory,sample_count,vertical_flip=False,horizontal_flip=False,zoom_range=0.0):\n",
        "    features = np.zeros(shape=(sample_count,4,4,512))\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    gen = ImageDataGenerator(rescale=1./255,vertical_flip=vertical_flip,horizontal_flip=horizontal_flip,zoom_range=zoom_range)\n",
        "    data_gen = gen.flow_from_directory(directory,target_size=(150,150),class_mode='binary',batch_size=30)\n",
        "    i=0\n",
        "    for img_batch,label_batch in data_gen:\n",
        "        prediction = conv.predict(img_batch)\n",
        "        features[i*30 : (i+1)*30] = prediction\n",
        "        labels[i*30:(i+1)*30] = label_batch\n",
        "        i+=1\n",
        "        if i*30 >= sample_count:\n",
        "            return features,labels"
      ],
      "metadata": {
        "id": "YM6PoyBvM2aF"
      },
      "id": "YM6PoyBvM2aF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "model = models.load_model(\"/content/drive/MyDrive/lung_cancer_model/lungs_cancer_classification86%.h5\")\n",
        "\n",
        "# Open the image with error handling\n",
        "try:\n",
        "    image = Image.open(\"/content/lung cancer Detection/test/negative/person137_bacteria_655.jpeg\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Image file not found.\")\n",
        "    exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"Error opening image: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Convert to RGB if necessary\n",
        "if image.mode != \"RGB\":\n",
        "    image = image.convert(\"RGB\")\n",
        "\n",
        "# Resize the image to 128x128 pixels\n",
        "resized_image = image.resize((150, 150))\n",
        "\n",
        "# Convert the resized image to a numpy array and normalize pixel values\n",
        "resized_image_array = np.array(resized_image) / 255.0\n",
        "resized_image_array = resized_image_array.reshape(1, 150, 150, 3)\n",
        "# resized_image_array = resized_image_array.flatten()\n",
        "conv = VGG16(include_top=False,input_shape=(150,150,3))\n",
        "prediction = conv.predict(resized_image_array)\n",
        "prediction = model.predict(prediction)\n",
        "\n",
        "if(prediction > 0.6):\n",
        "  confidence = prediction[0][0] * 100\n",
        "  print(f\"Lung Cancer Detected Confidence: {confidence:.2f}%\")\n",
        "else:\n",
        "  confidence = (1 - prediction[0][0]) * 100\n",
        "  print(f\"Lung Cancer Not Detected Confidence: {confidence:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_0rh3JkNLg-",
        "outputId": "336e86f5-b229-473e-c9a4-751c2e79e311"
      },
      "id": "6_0rh3JkNLg-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 210ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "Lung Cancer Not Detected Confidence: 97.48%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.predict(\"\")"
      ],
      "metadata": {
        "id": "gBhCXiDvXFa7"
      },
      "id": "gBhCXiDvXFa7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}